{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9e6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d09f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('frankenstein_with_splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52785cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data:  (90698, 3)\n",
      "------------------------------------------------------------\n",
      "                                  context        target  split\n",
      "0                                , or the  frankenstein  train\n",
      "1              frankenstein or the modern             ,  train\n",
      "2    frankenstein , the modern prometheus            or  train\n",
      "3  frankenstein , or modern prometheus by           the  train\n",
      "4             , or the prometheus by mary        modern  train\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the data: \", df_all.shape)\n",
    "print('-'*60)\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afde8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n,
    "    def __init__(self, token_to_idx=None, \n",
    "                 mask_token=\"<MASK>\", add_unk=True, \n",
    "                 unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            mask_token (str): the MASK token to add into the Vocabulary; indicates\n",
    "                a position that will not be used in updating the model's parameters\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk   = add_unk\n",
    "        self._unk_token = unk_token    \n",
    "        self._mask_token = mask_token\n",
    "        \n",
    "        ### the mask_token, i.e, \"<MASK>\" is the first added token\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        \n",
    "        self.unk_index  = -999\n",
    "        ### the unk_token, i.e, \"<UNK>\" is the second added token if add_unk=True\n",
    "        ### self.unk_index is changed from -999 to 1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            ### add a new element to _token_to_idx\n",
    "            self._token_to_idx[token] = index\n",
    "            ### add a new element to _idx_to_token\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "   \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            ### .get(): return self.unk_index if the key \"token\" does not exist. \n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbdb2b7",
   "metadata": {},
   "source": [
    "# 1. CBOWVectorizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2aadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, cbow_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_vocab (Vocabulary): maps words to integers\n",
    "        \"\"\"\n",
    "        self.cbow_vocab = cbow_vocab\n",
    "         \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the target dataset\n",
    "        Returns:\n",
    "            an instance of the CBOWVectorizer\n",
    "        \"\"\"\n",
    "        cbow_vocab = Vocabulary()\n",
    "\n",
    "        ########## Add tokens to cbow_vocab\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            for token in row.context.split(' '):\n",
    "                cbow_vocab.add_token(token)\n",
    "            cbow_vocab.add_token(row.target)\n",
    "            \n",
    "        return cls(cbow_vocab)\n",
    "\n",
    "    ### This is the key functionality of the Vectorizer.\n",
    "    ### It takes as an argument a string representing a review,\n",
    "    ### and returns a vectorized representation of the review.\n",
    "    def vectorize(self, context, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        \"\"\"\n",
    "\n",
    "        indices = [self.cbow_vocab.lookup_token(token) for token in context.split(' ')]\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        \n",
    "        ### if vector_length = len(indices), out_vector = indices\n",
    "        ### if vector_length != len(indices), the out_vector is defined in the following lines\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.cbow_vocab.mask_index\n",
    "\n",
    "        return out_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268a8b6",
   "metadata": {},
   "source": [
    "# 2. Instantiate a ReviewVectorizer from the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558ed85",
   "metadata": {},
   "source": [
    "### First draw a (static, fixed random seed) from the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667e8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a4d269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, or the prometheus by mary</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  context        target  split\n",
       "0                                , or the  frankenstein  train\n",
       "1              frankenstein or the modern             ,  train\n",
       "2    frankenstein , the modern prometheus            or  train\n",
       "3  frankenstein , or modern prometheus by           the  train\n",
       "4             , or the prometheus by mary        modern  train"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d526fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instantiate a vectorizer\n",
    "vectorizer_sample = CBOWVectorizer.from_dataframe(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378100ce",
   "metadata": {},
   "source": [
    "### A vectorizer has two vocabularies(attributes), one for review, one for rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3621bd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cbow_vocab': <__main__.Vocabulary at 0x7fab0b121a60>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(vectorizer_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad80b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow_vocab includes 7270 tokens\n",
      "First ten _token_to_idx:\n",
      "[('<MASK>', 0), ('<UNK>', 1), (',', 2), ('or', 3), ('the', 4), ('frankenstein', 5), ('modern', 6), ('prometheus', 7), ('by', 8), ('mary', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"cbow_vocab includes {len(vectorizer_sample.cbow_vocab)} tokens\")\n",
    "print(\"First ten _token_to_idx:\")\n",
    "print(list(vectorizer_sample.cbow_vocab._token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2486c",
   "metadata": {},
   "source": [
    "# 3. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f78a7",
   "metadata": {},
   "source": [
    "### (classmethod) from_dataframe(review_df, cutoff): Instantiate the vectorizer from the dataset dataframe.\n",
    "1. First instantiate a Vocabulariy based on the input data \"frankenstein_with_splits.csv\". [See a walkthrough of Vocabulary class here](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/Frankenstein/class_Vocabulary.ipynb).\n",
    "2. Use the cbow_vocab as the input to instantiate a vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b5097",
   "metadata": {},
   "source": [
    "### vectorize(review): It takes as an argument a string of words separated by a space, and returns a vectorized representation of the string. This is the key functionality of the Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aecb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"the sun is shining and it is a beautiful day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edbea8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens in the example_text is 10\n",
      "The indeces of these tokens in cbow_vocab:[4, 124, 53, 4442, 49, 115, 53, 72, 1995, 328]\n"
     ]
    }
   ],
   "source": [
    "##### Initializing CBOWVectorizer\n",
    "vectorizer = CBOWVectorizer.from_dataframe(df_sample)\n",
    "indices = [vectorizer.cbow_vocab.lookup_token(token) for token in example_text.split(' ')]\n",
    "print(f'The number of tokens in the example_text is {len(indices)}')\n",
    "print('The indeces of these tokens in cbow_vocab:' + str(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa9ad2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   4,  124,   53, 4442,   49,  115,   53,   72, 1995,  328])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Use CBOWVectorizer.vectorize() with vector_length=-1\n",
    "##### i.e., no pre-specified length of index vector \n",
    "vector_1 = vectorizer.vectorize(example_text,vector_length=-1)\n",
    "vector_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17def95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first token: the\n",
      "The index of the first token in vectorizer.cbow_vocab: 4\n"
     ]
    }
   ],
   "source": [
    "token = 'the'\n",
    "index = vectorizer.cbow_vocab.lookup_token(token) \n",
    "print(f\"The first token: {token}\")\n",
    "print(f\"The index of the first token in vectorizer.cbow_vocab: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bb45978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fourth token: shining\n",
      "The index of the fourth token in vectorizer.cbow_vocab: 4442\n"
     ]
    }
   ],
   "source": [
    "token = 'shining'\n",
    "index = vectorizer.cbow_vocab.lookup_token(token) \n",
    "print(f\"The fourth token: {token}\")\n",
    "print(f\"The index of the fourth token in vectorizer.cbow_vocab: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bce663a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   4,  124,   53, 4442,   49,  115,   53,   72, 1995,  328,    0,\n",
       "          0,    0,    0,    0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Use CBOWVectorizer.vectorize() with vector_length>len(indices)\n",
    "##### out_vector[len(indices):] are assigned as CBOWVectorizer.cbow_vocab.mask_index\n",
    "vector_2 = vectorizer.vectorize(example_text,vector_length=15)\n",
    "vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65bb6645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not broadcast input array from shape (10,) into shape (5,)\n"
     ]
    }
   ],
   "source": [
    "##### Use CBOWVectorizer.vectorize() with vector_length<len(indices)\n",
    "try:\n",
    "    vector_3 = vectorizer.vectorize(example_text,vector_length=5)\n",
    "    vector_3\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
