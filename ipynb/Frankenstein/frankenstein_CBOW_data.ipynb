{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d8130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e98e099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_dataset_txt   = \"frankenstein.txt\",\n",
    "    window_size       = 3,\n",
    "    train_proportion  = 0.7,\n",
    "    val_proportion    = 0.15,\n",
    "    test_proportion   = 0.15,\n",
    "    output_csv        = \"frankenstein_with_splits.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a807aac",
   "metadata": {},
   "source": [
    "# 1. Utilizes the sentence tokenizer from NLTK  to segment English text into sentences.\n",
    "### - [nltk.data.load](https://www.nltk.org/api/nltk.data.html): Load a given resource from the NLTK data package. Use the NLTK Downloader to obtain the resource before loading:  nltk.download('punkt').\n",
    "### - [punkt.PunktSentenceTokenizer](https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html): A sentence tokenizer which uses an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences; and then uses that model to find sentence boundaries. This approach has been shown to work well for many European languages.\n",
    "### - PunktSentenceTokenizer.tokenize(): Given a text, returns a list of the sentences in that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4d0e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f6194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_example = \"Pluto was discovered in 1930 by Clyde W. Tombaugh, making it by far the first known object in the Kuiper belt. It was immediately hailed as the ninth planet, but it was always the odd object out, and its planetary status was questioned when it was found to be much smaller than expected. These doubts increased following the discovery of additional objects in the Kuiper belt starting in the 1990s, and particularly the more massive scattered disk object Eris in 2005. In 2006, the International Astronomical Union (IAU) formally redefined the term planet to exclude dwarf planets such as Pluto. Many planetary astronomers, however, continue to consider Pluto and other dwarf planets to be planets.\"\n",
    "sentences_example = tokenizer.tokenize(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ad72c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pluto was discovered in 1930 by Clyde W. Tombaugh, making it by far the first known object in the Kuiper belt. It was immediately hailed as the ninth planet, but it was always the odd object out, and its planetary status was questioned when it was found to be much smaller than expected. These doubts increased following the discovery of additional objects in the Kuiper belt starting in the 1990s, and particularly the more massive scattered disk object Eris in 2005. In 2006, the International Astronomical Union (IAU) formally redefined the term planet to exclude dwarf planets such as Pluto. Many planetary astronomers, however, continue to consider Pluto and other dwarf planets to be planets.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f085b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Pluto was discovered in 1930 by Clyde W. Tombaugh, making it by far the first known object in the Kuiper belt.\n",
      "------------------------------------------------------------\n",
      "1\n",
      "It was immediately hailed as the ninth planet, but it was always the odd object out, and its planetary status was questioned when it was found to be much smaller than expected.\n",
      "------------------------------------------------------------\n",
      "2\n",
      "These doubts increased following the discovery of additional objects in the Kuiper belt starting in the 1990s, and particularly the more massive scattered disk object Eris in 2005.\n",
      "------------------------------------------------------------\n",
      "3\n",
      "In 2006, the International Astronomical Union (IAU) formally redefined the term planet to exclude dwarf planets such as Pluto.\n",
      "------------------------------------------------------------\n",
      "4\n",
      "Many planetary astronomers, however, continue to consider Pluto and other dwarf planets to be planets.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for s in sentences_example:\n",
    "    print(i)\n",
    "    i+=1\n",
    "    print(s)\n",
    "    print('-'*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7545df1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3427 sentences\n",
      "Sample (1378):\n",
      "Cursed be the day, abhorred devil, in which you first saw\n",
      "light!\n"
     ]
    }
   ],
   "source": [
    "with open(args.raw_dataset_txt) as fp:\n",
    "    book = fp.read()\n",
    "sentences = tokenizer.tokenize(book)\n",
    "print (len(sentences), \"sentences\")\n",
    "\n",
    "a = random.randint(0, len(sentences))\n",
    "print(\"Sample ({}):\".format(a))\n",
    "print(sentences[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4888044",
   "metadata": {},
   "source": [
    "# 2. Utilizes the sentence tokenizer from NLTK to segment English text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b56b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    ### converts all words in the text to lowercase\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    ### substitute [.,!?] with spaces before and after matched punctuation marks\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    ### replace [^a-zA-Z.,!?] with a single space\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b2c5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample (3273):\n",
      "Before cleaning\n",
      "Nay, these are virtuous\n",
      "and immaculate beings!\n",
      "After cleaning\n",
      "nay , these are virtuous and immaculate beings ! \n"
     ]
    }
   ],
   "source": [
    "cleaned_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "\n",
    "a = random.randint(0, len(sentences))\n",
    "print(\"Sample ({}):\".format(a))\n",
    "print(\"Before cleaning\")\n",
    "print(sentences[a])\n",
    "print(\"After cleaning\")\n",
    "print(cleaned_sentences[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2414a60e",
   "metadata": {},
   "source": [
    "# 3. Create windows\n",
    "### - The window size used is a hyperparameter, and one that is fairly critical to CBOW. Too large of a window, and the model might fail to capture regularities; too small of a window, and the window might miss out on interesting dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a067b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_TOKEN = \"<MASK>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a47b1",
   "metadata": {},
   "source": [
    "### - [nltk.ngrams(sequence, n)](https://tedboy.github.io/nlps/generated/generated/nltk.ngrams.html): Return the ngrams generated from a sequence of items, as an iterator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "541cb172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f8ba459ed00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### return an iterator\n",
    "nltk.ngrams([1,2,3,4,5],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb179c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (2, 3, 4), (3, 4, 5)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### return a list\n",
    "list(nltk.ngrams([1,2,3,4,5], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e7ac51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a ball'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_now = 'this is a ball'\n",
    "sentence_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fd56d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<MASK>',\n",
       " '<MASK>',\n",
       " '<MASK>',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'ball',\n",
       " '<MASK>',\n",
       " '<MASK>',\n",
       " '<MASK>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size_now = 3\n",
    "sequence_now = [MASK_TOKEN] * window_size_now + sentence_now.split(' ')\\\n",
    "               + [MASK_TOKEN] * window_size_now\n",
    "sequence_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2acfb57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<MASK>', '<MASK>', '<MASK>', 'this', 'is', 'a', 'ball'),\n",
       " ('<MASK>', '<MASK>', 'this', 'is', 'a', 'ball', '<MASK>'),\n",
       " ('<MASK>', 'this', 'is', 'a', 'ball', '<MASK>', '<MASK>'),\n",
       " ('this', 'is', 'a', 'ball', '<MASK>', '<MASK>', '<MASK>')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_now = list(nltk.ngrams(sequence_now, window_size_now * 2 + 1))\n",
    "ngrams_now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be1e72",
   "metadata": {},
   "source": [
    "### - flatten(list): A lambda function that flattens the nested list outer_list into a single-layered list and return this resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23930cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda outer_list: [item for inner_list in outer_list for item in inner_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f3e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'int' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "### The input needs to be a nested list\n",
    "try: \n",
    "    flatten([1,2,3])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a5385fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested list\n",
      "[[1, 2, 3], ['a', 'b', 'c', 'd']]\n",
      "------------------------------------------------------------\n",
      "flattened nested list\n",
      "[1, 2, 3, 'a', 'b', 'c', 'd']\n"
     ]
    }
   ],
   "source": [
    "nested_list = [[1,2,3],['a','b','c','d']]\n",
    "print(\"nested list\")\n",
    "print(nested_list)\n",
    "print('-'*60)\n",
    "print(\"flattened nested list\")\n",
    "print(flatten(nested_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a09c00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested list\n",
      "[(1, 2, 3), ('a', 'b', 'c', 'd')]\n",
      "------------------------------------------------------------\n",
      "flattened nested list\n",
      "[1, 2, 3, 'a', 'b', 'c', 'd']\n"
     ]
    }
   ],
   "source": [
    "nested_list = [(1,2,3),('a','b','c','d')]\n",
    "print(\"nested list\")\n",
    "print(nested_list)\n",
    "print('-'*60)\n",
    "print(\"flattened nested list\")\n",
    "print(flatten(nested_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac554322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngrams_now\n",
      "Shape: 4\n",
      "[('<MASK>', '<MASK>', '<MASK>', 'this', 'is', 'a', 'ball'), ('<MASK>', '<MASK>', 'this', 'is', 'a', 'ball', '<MASK>'), ('<MASK>', 'this', 'is', 'a', 'ball', '<MASK>', '<MASK>'), ('this', 'is', 'a', 'ball', '<MASK>', '<MASK>', '<MASK>')]\n",
      "------------------------------------------------------------\n",
      "flattened ngrams_now\n",
      "Shape: 28\n",
      "['<MASK>', '<MASK>', '<MASK>', 'this', 'is', 'a', 'ball', '<MASK>', '<MASK>', 'this', 'is', 'a', 'ball', '<MASK>', '<MASK>', 'this', 'is', 'a', 'ball', '<MASK>', '<MASK>', 'this', 'is', 'a', 'ball', '<MASK>', '<MASK>', '<MASK>']\n"
     ]
    }
   ],
   "source": [
    "print(\"ngrams_now\")\n",
    "print('Shape: '+str(len(ngrams_now)))\n",
    "print(ngrams_now)\n",
    "print('-'*60)\n",
    "print(\"flattened ngrams_now\")\n",
    "print('Shape: '+str(len(flatten(ngrams_now))))\n",
    "print(flatten(ngrams_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96959fef",
   "metadata": {},
   "source": [
    "### Create the windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b7c04b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed70774f74ec4ad8a5a49859171c8e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create windows\n",
    "flatten  = lambda outer_list: [item for inner_list in outer_list for item in inner_list]\n",
    "_windows = [list(\n",
    "                nltk.ngrams(\n",
    "                    [MASK_TOKEN] * args.window_size + \\\n",
    "                    sentence.split(' ') + \\\n",
    "                    [MASK_TOKEN] * args.window_size, \n",
    "                    args.window_size * 2 + 1)) \\\n",
    "            for sentence in tqdm_notebook(cleaned_sentences)]\n",
    "windows = flatten(_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81fe6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of windows (before flattening): 3427\n",
      "Number of windows (after flattening): 90698\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of windows (before flattening): {}\".format(len(_windows)))\n",
    "print(\"Number of windows (after flattening): {}\".format(len(windows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d51e863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\n",
      "window:\n",
      "('<MASK>', '<MASK>', '<MASK>', 'frankenstein', ',', 'or', 'the')\n",
      "target_token: frankenstein\n"
     ]
    }
   ],
   "source": [
    "print('Sample')\n",
    "window_now = windows[0]\n",
    "target_token_now = window_now[args.window_size]\n",
    "print(\"window:\")\n",
    "print(window_now)\n",
    "print(\"target_token: \"+ target_token_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3fb8091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\n",
      "window:\n",
      "('<MASK>', '<MASK>', '<MASK>', 'frankenstein', ',', 'or', 'the')\n",
      "target_token: frankenstein\n"
     ]
    }
   ],
   "source": [
    "print('Sample')\n",
    "window_now = windows[0]\n",
    "target_token_now = window_now[args.window_size]\n",
    "print(\"window:\")\n",
    "print(window_now)\n",
    "print(\"target_token: \"+ target_token_now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11594c8",
   "metadata": {},
   "source": [
    "### Create the cbow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c260c7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271c64cba91c45a7ac481be922d17ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = []\n",
    "for window in tqdm_notebook(windows):\n",
    "    target_token = window[args.window_size]\n",
    "    context = []\n",
    "    for i, token in enumerate(window):\n",
    "        if token == MASK_TOKEN or i == args.window_size:\n",
    "            continue\n",
    "        else:\n",
    "            context.append(token)\n",
    "    data.append([' '.join(token for token in context), target_token])\n",
    "cbow_data = pd.DataFrame(data, columns=[\"context\", \"target\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52cb7cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\n",
      "\n",
      "window:\n",
      "('<MASK>', '<MASK>', '<MASK>', 'frankenstein', ',', 'or', 'the')\n",
      "\n",
      "target:\n",
      "frankenstein\n",
      "\n",
      "data: \n",
      "    context        target\n",
      "0  , or the  frankenstein\n"
     ]
    }
   ],
   "source": [
    "print('Sample\\n')\n",
    "i = 0\n",
    "window_now = windows[i]\n",
    "print(\"window:\")\n",
    "print(window_now)\n",
    "print(\"\\ntarget:\")\n",
    "print(window_now[args.window_size])\n",
    "print(\"\\ndata: \")\n",
    "print(cbow_data.loc[[i],])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f626270",
   "metadata": {},
   "source": [
    "### Create the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ef22d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create split data\n",
    "n = len(cbow_data)\n",
    "def get_split(row_num):\n",
    "    if row_num <= n*args.train_proportion:\n",
    "        return 'train'\n",
    "    elif (row_num > n*args.train_proportion) and (row_num <= n*args.train_proportion + n*args.val_proportion):\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'test'\n",
    "cbow_data['split']= cbow_data.apply(lambda row: get_split(row.name), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffd7b58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, or the prometheus by mary</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  context        target  split\n",
       "0                                , or the  frankenstein  train\n",
       "1              frankenstein or the modern             ,  train\n",
       "2    frankenstein , the modern prometheus            or  train\n",
       "3  frankenstein , or modern prometheus by           the  train\n",
       "4             , or the prometheus by mary        modern  train"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e09468",
   "metadata": {},
   "source": [
    "# 4. Class frankenstein_munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a58de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class frankenstein_munging():\n",
    "    def __init__(self, txt_file, MASK_TOKEN):\n",
    "        self.tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        \n",
    "        with open(txt_file) as fp:\n",
    "            self.book  = fp.read()\n",
    "        \n",
    "        self.sentences         = self.tokenizer.tokenize(self.book)\n",
    "\n",
    "        self.MASK_TOKEN        = MASK_TOKEN\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def create_windows(self):\n",
    "        def preprocess_text(text):\n",
    "            text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "            text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "            text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "        return text\n",
    "        \n",
    "        flatten = lambda outer_list: [item for inner_list in outer_list for item in inner_list]\n",
    "        \n",
    "        _windows = [list(\n",
    "                        nltk.ngrams(\n",
    "                            [MASK_TOKEN] * args.window_size + \\\n",
    "                            sentence.split(' ') + \\\n",
    "                            [MASK_TOKEN] * args.window_size, \n",
    "                            args.window_size * 2 + 1)\n",
    "                        ) \\\n",
    "                    for sentence in tqdm_notebook(cleaned_sentences)]\n",
    "        \n",
    "        windows = flatten(_windows)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c01c6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cbow_data(raw_dataset_txt,window_size):\n",
    "    def preprocess_text(text):\n",
    "        text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "        text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "        text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "        return text\n",
    "    \n",
    "    train_proportion  = 0.7\n",
    "    val_proportion    = 0.15\n",
    "    test_proportion   = 0.15\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    with open(raw_dataset_txt) as fp:\n",
    "        book = fp.read()\n",
    "    \n",
    "    sentences = tokenizer.tokenize(book)\n",
    "    \n",
    "    cleaned_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "    \n",
    "    MASK_TOKEN = \"<MASK>\"\n",
    "    \n",
    "    ##### Create windows\n",
    "    flatten  = lambda outer_list: [item for inner_list in outer_list for item in inner_list]\n",
    "    _windows = [list(\n",
    "                    nltk.ngrams(\n",
    "                        [MASK_TOKEN] * window_size + \\\n",
    "                        sentence.split(' ') + \\\n",
    "                        [MASK_TOKEN] * window_size, \n",
    "                        window_size * 2 + 1)) \\\n",
    "                for sentence in tqdm_notebook(cleaned_sentences)]\n",
    "    windows = flatten(_windows)\n",
    "    \n",
    "    ##### Create cbow data\n",
    "    data = []\n",
    "    for window in tqdm_notebook(windows):\n",
    "        target_token = window[window_size]\n",
    "        context = []\n",
    "        for i, token in enumerate(window):\n",
    "            if token == MASK_TOKEN or i == window_size:\n",
    "                continue\n",
    "            else:\n",
    "                context.append(token)\n",
    "        data.append([' '.join(token for token in context), target_token])\n",
    "    cbow_data = pd.DataFrame(data, columns=[\"context\", \"target\"])  \n",
    "    \n",
    "    ##### Create split data\n",
    "    n = len(cbow_data)\n",
    "    def get_split(row_num):\n",
    "        if row_num <= n*train_proportion:\n",
    "            return 'train'\n",
    "        elif (row_num > n*train_proportion) and (row_num <= n*train_proportion + n*val_proportion):\n",
    "            return 'val'\n",
    "        else:\n",
    "            return 'test'\n",
    "    cbow_data['split']= cbow_data.apply(lambda row: get_split(row.name), axis=1)\n",
    "    \n",
    "    return cbow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ddf935e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ba52357746480da192b691ce639cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f158dba1ff4018b31b24635c68cd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frankenstein_cbow_3 = create_cbow_data(raw_dataset_txt = raw_dataset_txt,\n",
    "                                       window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab8f0cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6018da27c80a4115b1bb11f1dc58f9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94731eff0bee40fe861ffe1b3309abb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frankenstein_cbow_5 = create_cbow_data(raw_dataset_txt = raw_dataset_txt,\n",
    "                                       window_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b35001fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, or the prometheus by mary</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  context        target  split\n",
       "0                                , or the  frankenstein  train\n",
       "1              frankenstein or the modern             ,  train\n",
       "2    frankenstein , the modern prometheus            or  train\n",
       "3  frankenstein , or modern prometheus by           the  train\n",
       "4             , or the prometheus by mary        modern  train"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frankenstein_cbow_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b31645df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the modern prometheus</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern prometheus by</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus by mary</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by mary wo...</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frankenstein , or the prometheus by mary wolls...</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context        target  split\n",
       "0                         , or the modern prometheus  frankenstein  train\n",
       "1           frankenstein or the modern prometheus by             ,  train\n",
       "2       frankenstein , the modern prometheus by mary            or  train\n",
       "3  frankenstein , or modern prometheus by mary wo...           the  train\n",
       "4  frankenstein , or the prometheus by mary wolls...        modern  train"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frankenstein_cbow_5.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
