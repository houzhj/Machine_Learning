{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f5f7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131f15e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data:  (90698, 3)\n",
      "------------------------------------------------------------\n",
      "                                  context        target  split\n",
      "0                                , or the  frankenstein  train\n",
      "1              frankenstein or the modern             ,  train\n",
      "2    frankenstein , the modern prometheus            or  train\n",
      "3  frankenstein , or modern prometheus by           the  train\n",
      "4             , or the prometheus by mary        modern  train\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv('frankenstein_with_splits.csv')\n",
    "print(\"shape of the data: \", df_all.shape)\n",
    "print('-'*60)\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efaabca",
   "metadata": {},
   "source": [
    "# Define two relevent classes\n",
    "### - Vocabulary ([see a walkthrough here](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/Frankenstein/class_Vocabulary.ipynb))\n",
    "### - CBOWVectorizer ([see a walkthrough here](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/Frankenstein/class_Vectorizer.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f2753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx=None, \n",
    "                 mask_token=\"<MASK>\", add_unk=True, \n",
    "                 unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            mask_token (str): the MASK token to add into the Vocabulary; indicates\n",
    "                a position that will not be used in updating the model's parameters\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk   = add_unk\n",
    "        self._unk_token = unk_token    \n",
    "        self._mask_token = mask_token\n",
    "        \n",
    "        ### the mask_token, i.e, \"<MASK>\" is the first added token\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        \n",
    "        self.unk_index  = -999\n",
    "        ### the unk_token, i.e, \"<UNK>\" is the second added token if add_unk=True\n",
    "        ### self.unk_index is changed from -999 to 1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            ### add a new element to _token_to_idx\n",
    "            self._token_to_idx[token] = index\n",
    "            ### add a new element to _idx_to_token\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "   \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            ### .get(): return self.unk_index if the key \"token\" does not exist. \n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "    \n",
    "class CBOWVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, cbow_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_vocab (Vocabulary): maps words to integers\n",
    "        \"\"\"\n",
    "        self.cbow_vocab = cbow_vocab\n",
    "         \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the target dataset\n",
    "        Returns:\n",
    "            an instance of the CBOWVectorizer\n",
    "        \"\"\"\n",
    "        cbow_vocab = Vocabulary()\n",
    "\n",
    "        ########## Add tokens to cbow_vocab\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            for token in row.context.split(' '):\n",
    "                cbow_vocab.add_token(token)\n",
    "            cbow_vocab.add_token(row.target)\n",
    "            \n",
    "        return cls(cbow_vocab)\n",
    "\n",
    "    ### This is the key functionality of the Vectorizer.\n",
    "    ### It takes as an argument a string representing a text,\n",
    "    ### and returns a vectorized representation of the text.\n",
    "    def vectorize(self, context, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        \"\"\"\n",
    "\n",
    "        indices = [self.cbow_vocab.lookup_token(token) for token in context.split(' ')]\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        \n",
    "        ### if vector_length = len(indices), out_vector = indices\n",
    "        ### if vector_length != len(indices), the out_vector is defined in the following lines\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.cbow_vocab.mask_index\n",
    "\n",
    "        return out_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d61628",
   "metadata": {},
   "source": [
    "# 1. CBOWDataset class\n",
    "### - The Dataset class will characterize the key features of the dataset.\n",
    "### - In the initialization function of the class, make the class inherit the properties of torch.utils.data.Dataset so that we can later leverage its functionalities.\n",
    "### - In the \\_\\_init\\_\\_() function and the set_split() function, store important information such as labels and the features that we wish to generate at each pass.\n",
    "### - Each call requests a sample index for which the upperbound is specified in the \\_\\_len\\_\\_() method.\n",
    "### - When the sample corresponding to a given index is called, the generator executes the \\_\\_getitem\\_\\_() method to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336ed30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self,cbow_df,vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (CBOWVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.cbow_df     = cbow_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        ### CBOWVectorizer.vectorize() with be used with the parameter \n",
    "        ### vector_length = self._max_seq_length (the max length among all comments),\n",
    "        ### so that the vectors for different rows will have the same length.\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "        \n",
    "        self.train_df    = self.cbow_df[self.cbow_df.split=='train']\n",
    "        self.train_size  = len(self.train_df)\n",
    "\n",
    "        self.val_df      = self.cbow_df[self.cbow_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df     = self.cbow_df[self.cbow_df.split=='test']\n",
    "        self.test_size   = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val'  : (self.val_df, self.validation_size),\n",
    "                             'test' : (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_csv_and_make_vectorizer(cls,cbow_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "            cbow_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        cbow_csv = pd.read_csv(cbow_csv)\n",
    "        ### make vectorizer using training dataset\n",
    "        train_cbow_df   = cbow_df[cbow_df.split=='train']\n",
    "        new_vectorizer  = CBOWVectorizer.from_dataframe(train_cbow_df)\n",
    "        return cls(cbow_df,new_vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_df_and_make_vectorizer(cls,cbow_df):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "            cbow_df: dataset\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        ### make vectorizer using training dataset\n",
    "        train_cbow_df  = cbow_df[cbow_df.split=='train']\n",
    "        new_vectorizer = CBOWVectorizer.from_dataframe(train_cbow_df)\n",
    "        return cls(cbow_df,new_vectorizer)\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        ### when split = 'train', _target_df means the training set\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        ### _target_size is defined in set_split() \n",
    "        return self._target_size        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        \n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = \\\n",
    "            self._vectorizer.vectorize(row.context, self._max_seq_length)\n",
    "\n",
    "        target_index   = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eeb7c0",
   "metadata": {},
   "source": [
    "# 2. Instantiate a CBOWDataset from the training data\n",
    "### There are two classmethods can be used to instantiate a CBOWDataset: load_csv_and_make_vectorizer() and load_df_and_make_vectorizer(). The difference is whether the input data is from a csv file or a pd.DataFrame file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851359a",
   "metadata": {},
   "source": [
    "### First draw a (static, fixed random seed) from the entire datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6784708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_all.sample(100,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "064517be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5877</th>\n",
       "      <td>, as mine been .</td>\n",
       "      <td>has</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65522</th>\n",
       "      <td>, this sudden of life rushed</td>\n",
       "      <td>certainty</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49249</th>\n",
       "      <td>to be is indeed to</td>\n",
       "      <td>friendless</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42861</th>\n",
       "      <td>at once drew of sorrow and</td>\n",
       "      <td>tears</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24701</th>\n",
       "      <td>i soon felt rain coming slowly</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              context      target  split\n",
       "5877                , as mine been .          has  train\n",
       "65522    , this sudden of life rushed   certainty    val\n",
       "49249              to be is indeed to  friendless  train\n",
       "42861      at once drew of sorrow and       tears  train\n",
       "24701  i soon felt rain coming slowly         the  train"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c98b6",
   "metadata": {},
   "source": [
    "### Create a CBOWDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c111ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = CBOWDataset.load_df_and_make_vectorizer(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0225ae",
   "metadata": {},
   "source": [
    "## 2.1 - Attributes of a CBOWDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc92dc7",
   "metadata": {},
   "source": [
    "### .cbow_df: the input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ecf3088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5877</th>\n",
       "      <td>, as mine been .</td>\n",
       "      <td>has</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65522</th>\n",
       "      <td>, this sudden of life rushed</td>\n",
       "      <td>certainty</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49249</th>\n",
       "      <td>to be is indeed to</td>\n",
       "      <td>friendless</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42861</th>\n",
       "      <td>at once drew of sorrow and</td>\n",
       "      <td>tears</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24701</th>\n",
       "      <td>i soon felt rain coming slowly</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59428</th>\n",
       "      <td>soothed me , i could thus</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>of the common of men ,</td>\n",
       "      <td>pathways</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59364</th>\n",
       "      <td>but blight had come</td>\n",
       "      <td>a</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56103</th>\n",
       "      <td>from taking the step in an</td>\n",
       "      <td>first</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14410</th>\n",
       "      <td>great proficiency in study and i</td>\n",
       "      <td>that</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                context      target  split\n",
       "5877                  , as mine been .          has  train\n",
       "65522      , this sudden of life rushed   certainty    val\n",
       "49249                to be is indeed to  friendless  train\n",
       "42861        at once drew of sorrow and       tears  train\n",
       "24701    i soon felt rain coming slowly         the  train\n",
       "...                                 ...         ...    ...\n",
       "59428         soothed me , i could thus         and  train\n",
       "2764             of the common of men ,    pathways  train\n",
       "59364               but blight had come           a  train\n",
       "56103        from taking the step in an       first  train\n",
       "14410  great proficiency in study and i        that  train\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.cbow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "177ca416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.cbow_df.equals(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8801876",
   "metadata": {},
   "source": [
    "### ._max_seq_length: max number of tokens in a context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64438dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample._max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7ee85",
   "metadata": {},
   "source": [
    "### ._vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3387bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note that the vectorizer is derived from the training split. \n",
    "v = dataset_sample._vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea070b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"the sun is shining and it is a beautiful day\"\n",
    "vector       = v.vectorize(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34c23dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Vocabulary\n",
      "----------------------------------------------------------------------------------------------------\n",
      "_idx_to_token:  {0: '<MASK>', 1: '<UNK>', 2: ',', 3: 'as', 4: 'mine', 5: 'been', 6: '.', 7: '', 8: 'has', 9: 'to', 10: 'be', 11: 'is', 12: 'indeed', 13: 'friendless', 14: 'at', 15: 'once', 16: 'drew', 17: 'of', 18: 'sorrow', 19: 'and', 20: 'tears', 21: 'i', 22: 'soon', 23: 'felt', 24: 'rain', 25: 'coming', 26: 'slowly', 27: 'the', 28: 'my', 29: 'aunt', 30: nan, 31: 'contrast', 32: 'perpetually', 33: 'presented', 34: 'eyes', 35: 'had', 36: 'are', 37: 'canvassed', 38: 'many', 39: 'lights', 40: 'thrown', 41: 'so', 42: 'on', 43: 'you', 44: 'only', 45: 'any', 46: 'claim', 47: 'prospect', 48: 'such', 49: 'shells', 50: 'beside', 51: 'unexplored', 52: 'ocean', 53: 'great', 54: 'said', 55: '!', 56: 'calm', 57: 'joyous', 58: 'faces', 59: 'back', 60: 'despair', 61: 'brought', 62: 'for', 63: 'figure', 64: 'saw', 65: 'shores', 66: 'como', 67: 'lake', 68: 'an', 69: 'insurmountable', 70: 'placed', 71: 'between', 72: 'me', 73: 'barrier', 74: 'was', 75: 'ardour', 76: 'in', 77: 'habits', 78: 'daily', 79: 'kindness', 80: 'towards', 81: 'incline', 82: 'judges', 83: 'a', 84: 'favourable', 85: 'interpretation', 86: 'her', 87: 'marriage', 88: 'with', 89: 'lover', 90: 'heard', 91: 'harsh', 92: 'unfeeling', 93: 'reasoning', 94: 'pride', 95: 'delight', 96: 'its', 97: 'rage', 98: 'undertaking', 99: 'whose', 100: 'immediate', 101: 'began', 102: 'appear', 103: 'necessity', 104: 'have', 105: 'already', 106: 'engaged', 107: 'were', 108: 'rest', 109: 'or', 110: 'one', 111: 'ever', 112: 'which', 113: 'united', 114: 'by', 115: 'no', 116: 'apparently', 117: 'society', 118: 'who', 119: 'exhibited', 120: 'arabian', 121: 'power', 122: 'waves', 123: 'silent', 124: 'listless', 125: 'then', 126: 'very', 127: 'wretched', 128: 'not', 129: 'event', 130: 'caroline', 131: 'became', 132: 'wife', 133: 'his', 134: 'earliest', 135: 'sensations', 136: 'remember', 137: 'can', 138: 'entered', 139: 'it', 140: 'chinks', 141: 'innumerable', 142: 'sincerely', 143: 'grateful', 144: 'what', 145: 'occasion', 146: 'justine', 147: 'our', 148: 'family', 149: 'moritz', 150: 'servox', 151: 'through', 152: 'just', 153: 'him', 154: 'inexhaustible', 155: 'source', 156: 'personal', 157: 'deformity', 158: 'unrestrained', 159: 'laughter', 160: 'frightened', 161: 'heartless', 162: 'night', 163: 'quickly', 164: 'shut', 165: 'but', 166: 'sank', 167: 'into', 168: 'profound', 169: 'sleep', 170: 'own', 171: 'approve', 172: 'amend', 173: 'sails', 174: 'set', 175: 'carried', 176: 'scion', 177: 'principle', 178: 'evil', 179: 'nearer', 180: 'recent', 181: 'drawn', 182: 'us', 183: 'events', 184: 'find', 185: 'works', 186: 'volume', 187: 'tortures', 188: 'hell', 189: 'morning', 190: 'dismal', 191: 'wet', 192: 'replied', 193: 'decided', 194: 'blooming', 195: 'active', 196: 'health', 197: 'stretched', 198: 'he', 199: 'invented', 200: 'tales', 201: 'wonderful', 202: 'fancy', 203: 'former', 204: 'deceit', 205: 'aided', 206: 'surely', 207: 'oh', 208: 'moulded', 209: 'features', 210: 'sun', 211: 'visible', 212: 'forever', 213: 'spring', 214: 'all', 215: 'joy', 216: 'mockery', 217: 'might', 218: 'hereafter', 219: 'think', 220: 'right', 221: 'pursue', 222: 'generally', 223: 'melancholy', 224: 'shortly', 225: 'after', 226: 'deeply', 227: 'affected', 228: 'soothed', 229: 'could', 230: 'thus', 231: 'common', 232: 'men', 233: 'pathways', 234: 'blight', 235: 'come', 236: 'from', 237: 'taking', 238: 'step', 239: 'first', 240: 'proficiency', 241: 'study', 242: 'that'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "the sun is shining and it is a beautiful day\n",
      "vector representation: [ 27 210  11   1  19 139  11  83   1   1]\n"
     ]
    }
   ],
   "source": [
    "print(f'CBOW Vocabulary')\n",
    "print('-'*100)\n",
    "print(\"_idx_to_token: \", v.cbow_vocab._idx_to_token)\n",
    "print('-'*100)\n",
    "print(example_text)\n",
    "print('vector representation:', vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f76a9b",
   "metadata": {},
   "source": [
    "### ._target_df, _target_size\n",
    "**Defined by method set_split()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c526883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5877</th>\n",
       "      <td>, as mine been .</td>\n",
       "      <td>has</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49249</th>\n",
       "      <td>to be is indeed to</td>\n",
       "      <td>friendless</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42861</th>\n",
       "      <td>at once drew of sorrow and</td>\n",
       "      <td>tears</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24701</th>\n",
       "      <td>i soon felt rain coming slowly</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20535</th>\n",
       "      <td>my aunt .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59428</th>\n",
       "      <td>soothed me , i could thus</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>of the common of men ,</td>\n",
       "      <td>pathways</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59364</th>\n",
       "      <td>but blight had come</td>\n",
       "      <td>a</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56103</th>\n",
       "      <td>from taking the step in an</td>\n",
       "      <td>first</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14410</th>\n",
       "      <td>great proficiency in study and i</td>\n",
       "      <td>that</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                context      target  split\n",
       "5877                  , as mine been .          has  train\n",
       "49249                to be is indeed to  friendless  train\n",
       "42861        at once drew of sorrow and       tears  train\n",
       "24701    i soon felt rain coming slowly         the  train\n",
       "20535                         my aunt .         NaN  train\n",
       "...                                 ...         ...    ...\n",
       "59428         soothed me , i could thus         and  train\n",
       "2764             of the common of men ,    pathways  train\n",
       "59364               but blight had come           a  train\n",
       "56103        from taking the step in an       first  train\n",
       "14410  great proficiency in study and i        that  train\n",
       "\n",
       "[67 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample._target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30bb66b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample._target_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952364c",
   "metadata": {},
   "source": [
    "### ._lookup_dict - will be used in the method set_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec3860e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (                                context      target  split\n",
       "  5877                  , as mine been .          has  train\n",
       "  49249                to be is indeed to  friendless  train\n",
       "  42861        at once drew of sorrow and       tears  train\n",
       "  24701    i soon felt rain coming slowly         the  train\n",
       "  20535                         my aunt .         NaN  train\n",
       "  ...                                 ...         ...    ...\n",
       "  59428         soothed me , i could thus         and  train\n",
       "  2764             of the common of men ,    pathways  train\n",
       "  59364               but blight had come           a  train\n",
       "  56103        from taking the step in an       first  train\n",
       "  14410  great proficiency in study and i        that  train\n",
       "  \n",
       "  [67 rows x 3 columns],\n",
       "  67),\n",
       " 'val': (                                  context      target split\n",
       "  65522        , this sudden of life rushed   certainty   val\n",
       "  65585           my course towards land .          the   val\n",
       "  75275    head upon her and a handkerchief         arm   val\n",
       "  74545                      to me forever            .   val\n",
       "  69493  the season of assizes approached .         the   val\n",
       "  68934                               my !       father   val\n",
       "  67606           i remember , i thus awoke        when   val\n",
       "  72247    the daemon employ art to destroy       every   val\n",
       "  75157                where it is hated .         most   val\n",
       "  65437              some hours thus but by      passed   val\n",
       "  66835         , about an before she heard        hour   val\n",
       "  66203              , but being by a crowd  surrounded   val\n",
       "  67041     , on hearing evidence , desired        this   val\n",
       "  69597             on its being that i was      proved   val\n",
       "  65741   , several people towards the spot     crowded   val\n",
       "  64088                         across me .         NaN   val,\n",
       "  16),\n",
       " 'test': (                                         context        target split\n",
       "  85208                          ask thee to me ?         pardon  test\n",
       "  79515          it was only be distinguished from            to  test\n",
       "  77233                       if it is my power to            in  test\n",
       "  81504              comfort , the of solitude and     offspring  test\n",
       "  89786       , promotion and of project gutenberg  distribution  test\n",
       "  81858                             my was vivid ,   imagination  test\n",
       "  89018              provide , in with paragraph .    accordance  test\n",
       "  83549                                 to reply .           NaN  test\n",
       "  89569             second copy is defective , you          also  test\n",
       "  78277                         i towards the spot        darted  test\n",
       "  85349               of the tempest his passion .            of  test\n",
       "  90286              including outdated equipment              .  test\n",
       "  85332                     and incoherent self .     reproaches  test\n",
       "  89950  ensuring that the gutenberg tm collection       project  test\n",
       "  80226             uttered a wild of ecstasy when           cry  test\n",
       "  82445                       whom i have to be my     persuaded  test\n",
       "  79514               , it was to be distinguished          only  test,\n",
       "  17)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample._lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57c0d32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                context      target  split\n",
       " 5877                  , as mine been .          has  train\n",
       " 49249                to be is indeed to  friendless  train\n",
       " 42861        at once drew of sorrow and       tears  train\n",
       " 24701    i soon felt rain coming slowly         the  train\n",
       " 20535                         my aunt .         NaN  train\n",
       " ...                                 ...         ...    ...\n",
       " 59428         soothed me , i could thus         and  train\n",
       " 2764             of the common of men ,    pathways  train\n",
       " 59364               but blight had come           a  train\n",
       " 56103        from taking the step in an       first  train\n",
       " 14410  great proficiency in study and i        that  train\n",
       " \n",
       " [67 rows x 3 columns],\n",
       " 67)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### A dictionary which contains a df and a scalar\n",
    "dataset_sample._lookup_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fdc9532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5877</th>\n",
       "      <td>, as mine been .</td>\n",
       "      <td>has</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49249</th>\n",
       "      <td>to be is indeed to</td>\n",
       "      <td>friendless</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42861</th>\n",
       "      <td>at once drew of sorrow and</td>\n",
       "      <td>tears</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24701</th>\n",
       "      <td>i soon felt rain coming slowly</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20535</th>\n",
       "      <td>my aunt .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59428</th>\n",
       "      <td>soothed me , i could thus</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>of the common of men ,</td>\n",
       "      <td>pathways</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59364</th>\n",
       "      <td>but blight had come</td>\n",
       "      <td>a</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56103</th>\n",
       "      <td>from taking the step in an</td>\n",
       "      <td>first</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14410</th>\n",
       "      <td>great proficiency in study and i</td>\n",
       "      <td>that</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                context      target  split\n",
       "5877                  , as mine been .          has  train\n",
       "49249                to be is indeed to  friendless  train\n",
       "42861        at once drew of sorrow and       tears  train\n",
       "24701    i soon felt rain coming slowly         the  train\n",
       "20535                         my aunt .         NaN  train\n",
       "...                                 ...         ...    ...\n",
       "59428         soothed me , i could thus         and  train\n",
       "2764             of the common of men ,    pathways  train\n",
       "59364               but blight had come           a  train\n",
       "56103        from taking the step in an       first  train\n",
       "14410  great proficiency in study and i        that  train\n",
       "\n",
       "[67 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### the dataframe\n",
    "dataset_sample._lookup_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ea4c33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### the sample size\n",
    "dataset_sample._lookup_dict['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b974f",
   "metadata": {},
   "source": [
    "## 2.2 - Methods of a CBOWDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014c440",
   "metadata": {},
   "source": [
    "### \\_\\_len()\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2a9ee4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd9617",
   "metadata": {},
   "source": [
    "### \\_\\_getitem()\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c81d7520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([21, 22, 23, 24, 25, 26]), 'y_target': 27}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The 4th element in the \"train\" split\n",
    "### In the __init__ function, self.set_split('train') defines ._target_df\n",
    "dataset_sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3f8e011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5877</th>\n",
       "      <td>, as mine been .</td>\n",
       "      <td>has</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65522</th>\n",
       "      <td>, this sudden of life rushed</td>\n",
       "      <td>certainty</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49249</th>\n",
       "      <td>to be is indeed to</td>\n",
       "      <td>friendless</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42861</th>\n",
       "      <td>at once drew of sorrow and</td>\n",
       "      <td>tears</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24701</th>\n",
       "      <td>i soon felt rain coming slowly</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              context      target  split\n",
       "5877                , as mine been .          has  train\n",
       "65522    , this sudden of life rushed   certainty    val\n",
       "49249              to be is indeed to  friendless  train\n",
       "42861      at once drew of sorrow and       tears  train\n",
       "24701  i soon felt rain coming slowly         the  train"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "922a4596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "soon\n",
      "felt\n",
      "rain\n",
      "coming\n",
      "slowly\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "for i in range(21,28):\n",
    "    print(dataset_sample._vectorizer.cbow_vocab._idx_to_token[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b3eb72",
   "metadata": {},
   "source": [
    "### set_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd13cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = CBOWDataset.load_df_and_make_vectorizer(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f0c92fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5877</th>\n",
       "      <td>, as mine been .</td>\n",
       "      <td>has</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49249</th>\n",
       "      <td>to be is indeed to</td>\n",
       "      <td>friendless</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42861</th>\n",
       "      <td>at once drew of sorrow and</td>\n",
       "      <td>tears</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24701</th>\n",
       "      <td>i soon felt rain coming slowly</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20535</th>\n",
       "      <td>my aunt .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59428</th>\n",
       "      <td>soothed me , i could thus</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>of the common of men ,</td>\n",
       "      <td>pathways</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59364</th>\n",
       "      <td>but blight had come</td>\n",
       "      <td>a</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56103</th>\n",
       "      <td>from taking the step in an</td>\n",
       "      <td>first</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14410</th>\n",
       "      <td>great proficiency in study and i</td>\n",
       "      <td>that</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                context      target  split\n",
       "5877                  , as mine been .          has  train\n",
       "49249                to be is indeed to  friendless  train\n",
       "42861        at once drew of sorrow and       tears  train\n",
       "24701    i soon felt rain coming slowly         the  train\n",
       "20535                         my aunt .         NaN  train\n",
       "...                                 ...         ...    ...\n",
       "59428         soothed me , i could thus         and  train\n",
       "2764             of the common of men ,    pathways  train\n",
       "59364               but blight had come           a  train\n",
       "56103        from taking the step in an       first  train\n",
       "14410  great proficiency in study and i        that  train\n",
       "\n",
       "[67 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now the split for ._target_df and _target_size is 'train'\n",
    "dataset_sample._target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27626144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14848e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([21, 22, 23, 24, 25, 26]), 'y_target': 27}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The 4th element in the \"train\" split\n",
    "dataset_sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a931c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "### run set_split, switch the split to 'val'\n",
    "dataset_sample.set_split('val')\n",
    "# or \n",
    "# CBOWDataset.set_split(dataset_sample,'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f847e938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65522</th>\n",
       "      <td>, this sudden of life rushed</td>\n",
       "      <td>certainty</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65585</th>\n",
       "      <td>my course towards land .</td>\n",
       "      <td>the</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75275</th>\n",
       "      <td>head upon her and a handkerchief</td>\n",
       "      <td>arm</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74545</th>\n",
       "      <td>to me forever</td>\n",
       "      <td>.</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69493</th>\n",
       "      <td>the season of assizes approached .</td>\n",
       "      <td>the</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68934</th>\n",
       "      <td>my !</td>\n",
       "      <td>father</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67606</th>\n",
       "      <td>i remember , i thus awoke</td>\n",
       "      <td>when</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72247</th>\n",
       "      <td>the daemon employ art to destroy</td>\n",
       "      <td>every</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75157</th>\n",
       "      <td>where it is hated .</td>\n",
       "      <td>most</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65437</th>\n",
       "      <td>some hours thus but by</td>\n",
       "      <td>passed</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66835</th>\n",
       "      <td>, about an before she heard</td>\n",
       "      <td>hour</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66203</th>\n",
       "      <td>, but being by a crowd</td>\n",
       "      <td>surrounded</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67041</th>\n",
       "      <td>, on hearing evidence , desired</td>\n",
       "      <td>this</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69597</th>\n",
       "      <td>on its being that i was</td>\n",
       "      <td>proved</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65741</th>\n",
       "      <td>, several people towards the spot</td>\n",
       "      <td>crowded</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64088</th>\n",
       "      <td>across me .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  context      target split\n",
       "65522        , this sudden of life rushed   certainty   val\n",
       "65585           my course towards land .          the   val\n",
       "75275    head upon her and a handkerchief         arm   val\n",
       "74545                      to me forever            .   val\n",
       "69493  the season of assizes approached .         the   val\n",
       "68934                               my !       father   val\n",
       "67606           i remember , i thus awoke        when   val\n",
       "72247    the daemon employ art to destroy       every   val\n",
       "75157                where it is hated .         most   val\n",
       "65437              some hours thus but by      passed   val\n",
       "66835         , about an before she heard        hour   val\n",
       "66203              , but being by a crowd  surrounded   val\n",
       "67041     , on hearing evidence , desired        this   val\n",
       "69597             on its being that i was      proved   val\n",
       "65741   , several people towards the spot     crowded   val\n",
       "64088                         across me .         NaN   val"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now the split for ._target_df and _target_size is 'val'\n",
    "dataset_sample._target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fef7f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04955d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([  9,  72, 212,   7,   0,   0]), 'y_target': 6}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The 4th element in the \"val\" split\n",
    "dataset_sample[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f2c3f",
   "metadata": {},
   "source": [
    "### get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7d3c839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CBOWVectorizer at 0x7ff8afd2f4f0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e427ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CBOWVectorizer at 0x7ff8afd2f4f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Equivalently\n",
    "dataset_sample._vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750f609",
   "metadata": {},
   "source": [
    "### get_num_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "278b79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = CBOWDataset.load_df_and_make_vectorizer(df_sample)\n",
    "### Switch the split to 'train'\n",
    "dataset_sample.set_split('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d697ed6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.get_num_batches(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62c67788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample._target_df)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7464e8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample._target_df)//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e0f7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Switch the split to 'val'\n",
    "dataset_sample.set_split('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f2dacf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.get_num_batches(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5bff7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample._target_df)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f88d7ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedca03f",
   "metadata": {},
   "source": [
    "# 3. Define a batch generator\n",
    "### - Wrap the DataLoader\n",
    "### - Switch the data between the CPU and the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "255860b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device='cpu'):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d4140",
   "metadata": {},
   "source": [
    "## 3.1 Dataset Class\n",
    "### - The Dataset class characterizes the key features of the dataset you want to generate.\n",
    "### - The class uses \\_\\_init\\_\\_(), \\_\\_len\\_\\_(), and \\_\\_getitem\\_\\_() to store important information, and generate samples. \n",
    "### - The Dataset class is an important argument of the DataLoader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9cb9f686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: {'x1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'x2': [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], 'y': [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0]}\n",
      "------------------------------------------------------------\n",
      "df:     x1  x2  y\n",
      "0    1  13  0\n",
      "1    2  14  1\n",
      "2    3  15  0\n",
      "3    4  16  1\n",
      "4    5  17  1\n",
      "5    6  18  0\n",
      "6    7  19  0\n",
      "7    8  20  1\n",
      "8    9  21  1\n",
      "9   10  22  0\n",
      "10  11  23  1\n",
      "11  12  24  0\n"
     ]
    }
   ],
   "source": [
    "data = {'x1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "        'x2': [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
    "        'y': [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0]}\n",
    "data\n",
    "df = pd.DataFrame(data)\n",
    "print(\"data:\" ,data)\n",
    "print(\"-\"*60)\n",
    "print(\"df:\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffe2a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "[tensor([[ 1., 13.],\n",
      "        [ 2., 14.],\n",
      "        [ 3., 15.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "[tensor([[ 4., 16.],\n",
      "        [ 5., 17.],\n",
      "        [ 6., 18.]]), tensor([1., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "[tensor([[ 7., 19.],\n",
      "        [ 8., 20.],\n",
      "        [ 9., 21.]]), tensor([0., 1., 1.])]\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "[tensor([[10., 22.],\n",
      "        [11., 23.],\n",
      "        [12., 24.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##### Define Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = torch.tensor(self.data.iloc[index, :-1].values, dtype=torch.float32)\n",
    "        target = torch.tensor(self.data.iloc[index, -1], dtype=torch.float32)\n",
    "        return sample, target\n",
    "\n",
    "##### Instantiate the Dataset class\n",
    "custom_dataset = CustomDataset(df)\n",
    "\n",
    "##### Instantiate the DataLoader class\n",
    "batch_size  = 3\n",
    "data_loader = DataLoader(dataset=custom_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##### Obtain the batch\n",
    "i = 0\n",
    "for batch in data_loader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(batch)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c885a",
   "metadata": {},
   "source": [
    "### An alternative is to use TensorDataset() directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bbc1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3828382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n",
      "x2: tensor([13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24.])\n",
      "y: tensor([0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.from_numpy(df['x1'].values).float()\n",
    "x2 = torch.from_numpy(df['x2'].values).float()\n",
    "y  = torch.from_numpy(df['y'].values).float()\n",
    "print(\"x1:\", x1)\n",
    "print(\"x2:\", x2)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0989dcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 13.],\n",
       "        [ 2., 14.],\n",
       "        [ 3., 15.],\n",
       "        [ 4., 16.],\n",
       "        [ 5., 17.],\n",
       "        [ 6., 18.],\n",
       "        [ 7., 19.],\n",
       "        [ 8., 20.],\n",
       "        [ 9., 21.],\n",
       "        [10., 22.],\n",
       "        [11., 23.],\n",
       "        [12., 24.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.stack([x1, x2], dim=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb029abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "[tensor([[ 1., 13.],\n",
      "        [ 2., 14.],\n",
      "        [ 3., 15.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "[tensor([[ 4., 16.],\n",
      "        [ 5., 17.],\n",
      "        [ 6., 18.]]), tensor([1., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "[tensor([[ 7., 19.],\n",
      "        [ 8., 20.],\n",
      "        [ 9., 21.]]), tensor([0., 1., 1.])]\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "[tensor([[10., 22.],\n",
      "        [11., 23.],\n",
      "        [12., 24.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##### Create Tensor dataset\n",
    "dataset     = TensorDataset(features, y)\n",
    "batch_size  = 3\n",
    "\n",
    "##### Instantiate the DataLoader class\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##### Obtain the batch\n",
    "i = 0\n",
    "for batch in data_loader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(batch)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace0dd4",
   "metadata": {},
   "source": [
    "### The two methods below are equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d431af5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 13.],\n",
       "        [ 2., 14.],\n",
       "        [ 3., 15.],\n",
       "        [ 4., 16.],\n",
       "        [ 5., 17.],\n",
       "        [ 6., 18.],\n",
       "        [ 7., 19.],\n",
       "        [ 8., 20.],\n",
       "        [ 9., 21.],\n",
       "        [10., 22.],\n",
       "        [11., 23.],\n",
       "        [12., 24.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.from_numpy(df['x1'].values).float()\n",
    "x2 = torch.from_numpy(df['x2'].values).float()\n",
    "torch.stack([x1, x2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "724eebb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 13],\n",
       "        [ 2, 14],\n",
       "        [ 3, 15],\n",
       "        [ 4, 16],\n",
       "        [ 5, 17],\n",
       "        [ 6, 18],\n",
       "        [ 7, 19],\n",
       "        [ 8, 20],\n",
       "        [ 9, 21],\n",
       "        [10, 22],\n",
       "        [11, 23],\n",
       "        [12, 24]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_array = df[['x1', 'x2']].to_numpy()\n",
    "torch.from_numpy(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d5cc0",
   "metadata": {},
   "source": [
    "## 3.2 DataLoader\n",
    "### - batch_size: denotes the number of samples contained in each generated batch.\n",
    "### - shuffle: if set to True, we will get a new order of exploration at each pass (or just keep a linear exploration scheme otherwise). Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.\n",
    "### - drop_last: set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47cf3dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_sample[0]['x_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2fda99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = CBOWDataset.load_df_and_make_vectorizer(df_sample)\n",
    "batch_size     = 10\n",
    "shuffle        = True\n",
    "drop_last      = True\n",
    "dataloader     = DataLoader(dataset=dataset_sample, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ac53c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x in one batch\n",
      "tensor([[  2,  19, 110,   8, 111,   5],\n",
      "        [ 17,  96,  97,   7,   0,   0],\n",
      "        [139,  16, 179,   0,   0,   0],\n",
      "        [162, 163, 164,   2, 165,   9],\n",
      "        [125, 126, 127,  21,  74, 128],\n",
      "        [ 21, 166, 167, 168, 169, 165],\n",
      "        [  9, 184,  83,  17,  27, 185],\n",
      "        [ 53, 240,  76, 241,  19,  21],\n",
      "        [138, 139, 114, 140,   2,  21],\n",
      "        [  2,   3,   4,   5,   6,   7]])\n",
      "size of x_data: torch.Size([10, 6])\n",
      "------------------------------------------------------------\n",
      "y in one batch\n",
      "tensor([112,   6,   3,  76,   2,  83, 186, 242, 141,   8])\n",
      "size of y_data: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "one_batch = next(iter(dataloader))\n",
    "print('x in one batch')\n",
    "print(one_batch['x_data'])\n",
    "print('size of x_data:', one_batch['x_data'].shape)\n",
    "print('-' * 60)\n",
    "print('y in one batch')\n",
    "print(one_batch['y_target'])\n",
    "print('size of y_data:', one_batch['y_target'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7622b",
   "metadata": {},
   "source": [
    "### In this example, dataloader utilizes the return from the \\_\\_getitem\\_\\_() method, which extracts related rows from the _target_df of dataset, with _target_size=65. Also, batch_size=10, and drop_last=True so there are 6 batches created (the last 5 rows are dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37215098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in the target_df:  67\n",
      "number of rows in the target_df:  67\n",
      "The number of batches is: 6\n"
     ]
    }
   ],
   "source": [
    "print('number of rows in the target_df: ', len(dataset_sample._target_df))\n",
    "print('number of rows in the target_df: ', dataset_sample._target_size)\n",
    "print(\"The number of batches is:\",dataset_sample.get_num_batches(batch_size = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bcf58a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "{'x_data': tensor([[ 28,  74,  12,  27,   0,   0],\n",
      "        [ 21, 166, 167, 168, 169, 165],\n",
      "        [ 49,  50,  27,  19,  51,  52],\n",
      "        [194,  19, 195, 196,   2, 197],\n",
      "        [ 53, 240,  76, 241,  19,  21],\n",
      "        [ 36,  37,  19,  38,  39,  40],\n",
      "        [165, 234,  35, 235,   0,   0],\n",
      "        [156, 157,   6,   0,   0,   0],\n",
      "        [138, 224, 225,   7,   0,   0],\n",
      "        [203,   2, 118,  27, 204, 114]]), 'y_target': tensor([ 75,  83,  53,  76, 242,  41,  83,  30,   6, 205])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "{'x_data': tensor([[ 64,  68,  69,  70,  71,  72],\n",
      "        [ 42,  43,  44,  21,  45,  46],\n",
      "        [  2,  19, 110,   8, 111,   5],\n",
      "        [  2,  21,  74, 113, 114, 115],\n",
      "        [142, 143,   6,   0,   0,   0],\n",
      "        [ 74,   9, 153, 154, 155,  17],\n",
      "        [ 17,  96,  97,   7,   0,   0],\n",
      "        [  2,   3,   4,   5,   6,   7],\n",
      "        [ 76,  27,  77,  78,  79,  80],\n",
      "        [139,  16, 179,   0,   0,   0]]), 'y_target': tensor([ 73,  35, 112, 116,  30,  68,   6,   8,  17,   3])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "{'x_data': tensor([[150,   2, 151,  21,  35, 152],\n",
      "        [  2,  57,  58,  59,  60,   9],\n",
      "        [198, 199, 200, 201, 202,  19],\n",
      "        [  2, 192,  21, 139,  11, 193],\n",
      "        [117,  17,  27,   2, 118, 119],\n",
      "        [ 14,  15,  16,  17,  18,  19],\n",
      "        [176,  17,  27, 177,  19,  14],\n",
      "        [ 81,  28,  82,  83,  84,  85],\n",
      "        [ 27, 210,  11, 211,   2,  96],\n",
      "        [217, 218, 219, 220,   9, 221]]), 'y_target': tensor([112,  61,  17,   2, 120,  20, 178,   9, 212, 139])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "{'x_data': tensor([[ 21, 104, 105, 102,   9,  10],\n",
      "        [ 17,  27, 231,  17, 232,   2],\n",
      "        [134, 135,  21, 136,   6,   7],\n",
      "        [  9,  10,  11,  12,   9,   0],\n",
      "        [189, 190,  19, 191,   0,   0],\n",
      "        [  2, 158,   2, 159, 160,  19],\n",
      "        [ 21,  22,  23,  24,  25,  26],\n",
      "        [ 28, 170,   2, 171, 109, 172],\n",
      "        [ 31,  32,  33,  28,  34,  35],\n",
      "        [ 28,  29,   6,   0,   0,   0]]), 'y_target': tensor([106, 233, 137,  13,   2, 161,  27,   9,   9,  30])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n",
      "Batch 4\n",
      "{'x_data': tensor([[ 90,  27,  91,  92,  93,  17],\n",
      "        [213, 214, 215, 165,  83, 216],\n",
      "        [129, 130, 131, 132,   6,   7],\n",
      "        [ 65,  17,  27,  17,  66,   6],\n",
      "        [ 47,  17,  48,   0,   0,   0],\n",
      "        [162, 163, 164,   2, 165,   9],\n",
      "        [228,  72,   2,  21, 229, 230],\n",
      "        [144, 145, 146, 138, 147, 148],\n",
      "        [ 21,  74, 226, 114, 139,   6],\n",
      "        [  2,  19, 133, 107, 208,  88]]), 'y_target': tensor([  2,  74, 133,  67,  27,  76,  19, 149, 227, 209])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n",
      "Batch 5\n",
      "{'x_data': tensor([[ 28,  94,  19,  95,   6,   7],\n",
      "        [  2,  19, 180, 104, 181, 182],\n",
      "        [  2,  62,  21,  27,  63,  17],\n",
      "        [ 17,  27, 122, 123,  19, 124],\n",
      "        [ 21,   2, 107, 108, 109,  76],\n",
      "        [  9, 184,  83,  17,  27, 185],\n",
      "        [ 98,  99, 100, 101,   9, 102],\n",
      "        [125, 126, 127,  21,  74, 128],\n",
      "        [138, 139, 114, 140,   2,  21],\n",
      "        [236, 237,  27, 238,  76,  68]]), 'y_target': tensor([ 28, 183,  64,   2,  14, 186, 103,   2, 141, 239])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for data_dict in dataloader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(data_dict)\n",
    "    print(data_dict['x_data'].shape)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2433c26",
   "metadata": {},
   "source": [
    "### This is equvalent to defining and using the generator function generate_batches()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b396fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "{'x_data': tensor([[213, 214, 215, 165,  83, 216],\n",
      "        [ 14,  15,  16,  17,  18,  19],\n",
      "        [ 98,  99, 100, 101,   9, 102],\n",
      "        [156, 157,   6,   0,   0,   0],\n",
      "        [  2,  21,  74, 113, 114, 115],\n",
      "        [ 21,   2, 107, 108, 109,  76],\n",
      "        [  2,  62,  21,  27,  63,  17],\n",
      "        [ 28,  74,  12,  27,   0,   0],\n",
      "        [  2, 206,  27,   0,   0,   0],\n",
      "        [ 21, 104, 105, 102,   9,  10]]), 'y_target': tensor([ 74,  20, 103,  30, 116,  14,  64,  75, 207, 106])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "{'x_data': tensor([[  2,  19, 180, 104, 181, 182],\n",
      "        [125, 126, 127,  21,  74, 128],\n",
      "        [ 54,   2,  10,  55,   7,   0],\n",
      "        [  2,  19, 110,   8, 111,   5],\n",
      "        [176,  17,  27, 177,  19,  14],\n",
      "        [139,  16, 179,   0,   0,   0],\n",
      "        [162, 163, 164,   2, 165,   9],\n",
      "        [ 21,  74, 226, 114, 139,   6],\n",
      "        [ 74,   9, 153, 154, 155,  17],\n",
      "        [ 28, 121,   6,   0,   0,   0]]), 'y_target': tensor([183,   2,  56, 112, 178,   3,  76, 227,  68,  30])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "{'x_data': tensor([[ 65,  17,  27,  17,  66,   6],\n",
      "        [  2,   3,   4,   5,   6,   7],\n",
      "        [ 47,  17,  48,   0,   0,   0],\n",
      "        [ 28, 170,   2, 171, 109, 172],\n",
      "        [  2, 158,   2, 159, 160,  19],\n",
      "        [  2,  57,  58,  59,  60,   9],\n",
      "        [187,  17, 188,   0,   0,   0],\n",
      "        [129, 130, 131, 132,   6,   7],\n",
      "        [150,   2, 151,  21,  35, 152],\n",
      "        [ 17,  27, 122, 123,  19, 124]]), 'y_target': tensor([ 67,   8,  27,   9, 161,  61,  27, 133, 112,   2])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "{'x_data': tensor([[117,  17,  27,   2, 118, 119],\n",
      "        [ 81,  28,  82,  83,  84,  85],\n",
      "        [ 28,  94,  19,  95,   6,   7],\n",
      "        [ 21,  22,  23,  24,  25,  26],\n",
      "        [ 21, 166, 167, 168, 169, 165],\n",
      "        [ 36,  37,  19,  38,  39,  40],\n",
      "        [ 64,  68,  69,  70,  71,  72],\n",
      "        [138, 224, 225,   7,   0,   0],\n",
      "        [228,  72,   2,  21, 229, 230],\n",
      "        [ 28,  29,   6,   0,   0,   0]]), 'y_target': tensor([120,   9,  28,  27,  83,  41,  73,   6,  19,  30])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n",
      "Batch 4\n",
      "{'x_data': tensor([[  2, 192,  21, 139,  11, 193],\n",
      "        [  9, 184,  83,  17,  27, 185],\n",
      "        [  9,  10,  11,  12,   9,   0],\n",
      "        [ 90,  27,  91,  92,  93,  17],\n",
      "        [142, 143,   6,   0,   0,   0],\n",
      "        [194,  19, 195, 196,   2, 197],\n",
      "        [ 17,  96,  97,   7,   0,   0],\n",
      "        [165, 234,  35, 235,   0,   0],\n",
      "        [144, 145, 146, 138, 147, 148],\n",
      "        [217, 218, 219, 220,   9, 221]]), 'y_target': tensor([  2, 186,  13,   2,  30,  76,   6,  83, 149, 139])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n",
      "Batch 5\n",
      "{'x_data': tensor([[173, 174,   2,  74, 175, 114],\n",
      "        [189, 190,  19, 191,   0,   0],\n",
      "        [138, 139, 114, 140,   2,  21],\n",
      "        [236, 237,  27, 238,  76,  68],\n",
      "        [  2,  19, 133, 107, 208,  88],\n",
      "        [ 31,  32,  33,  28,  34,  35],\n",
      "        [ 76,  27,  77,  78,  79,  80],\n",
      "        [134, 135,  21, 136,   6,   7],\n",
      "        [ 42,  43,  44,  21,  45,  46],\n",
      "        [ 17,  27, 231,  17, 232,   2]]), 'y_target': tensor([ 21,   2, 141, 239, 209,   9,  17, 137,  35, 233])}\n",
      "torch.Size([10, 6])\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for data_dict in dataloader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(data_dict)\n",
    "    print(data_dict['x_data'].shape)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0743de",
   "metadata": {},
   "source": [
    "## 3.3 Generator\n",
    "### - Generator functions declare a function that behaves like an iterator, i.e. it can be used in a for loop.\n",
    "### - A generator function is defined just like a normal function, but whenever it needs to generate a value, it does so with the yield keyword rather than return. \n",
    "### - Yield is used in Python generators. If the body of a def contains yield, the function automatically becomes a generator function. \n",
    "### - *return* sends a specified value back to its caller whereas *yield* can produce a sequence of values. We should use *yield* when we want to iterate over a sequence, but don’t want to store the entire sequence in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a01ec",
   "metadata": {},
   "source": [
    "### Consider a task to calculate the sum of the first n integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c313379f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### The function below builds the full list in memory\n",
    "def first_n(n):\n",
    "    num, nums = 0, []\n",
    "    while num < n:\n",
    "        nums.append(num)\n",
    "        num += 1\n",
    "    return nums\n",
    "sum(first_n(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac676abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vars(a): {'n': 10, 'num': 0}\n",
      "sum(a): 45\n"
     ]
    }
   ],
   "source": [
    "##### The following implements generator as an iterable object.\n",
    "class first_n(object):\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.num = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    # Python 3 compatibility\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        if self.num < self.n:\n",
    "            cur, self.num = self.num, self.num+1\n",
    "            return cur\n",
    "        raise StopIteration\n",
    "        \n",
    "a = first_n(10)\n",
    "print('vars(a):', vars(a))\n",
    "print('sum(a):', sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55b34e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next(a): 0\n",
      "sum(a): 45\n",
      "next(a): None\n"
     ]
    }
   ],
   "source": [
    "##### a generator that yields items instead of returning a list\n",
    "\n",
    "def first_n(n):\n",
    "    num = 0\n",
    "    while num < n:\n",
    "        yield num\n",
    "        num += 1\n",
    "\n",
    "a = first_n(10)\n",
    "\n",
    "print('next(a):', next(a))\n",
    "print('sum(a):', sum(a))\n",
    "##### In Python, some built-in functions like sum(a), max(a), list(a) iterates through each element\n",
    "##### in 'a' and calculate the sum/max/list. This means sum(a) traverses all elements in the iterator\n",
    "##### 'a' until the iteration is completed. If the generator has already produced all its values,\n",
    "##### calling next() again will raise a StopIteration exception, indicating that the generator has\n",
    "##### been exhausted. use next(generator, default) to provide a default value, avoiding the occurrence\n",
    "##### of an exception. \n",
    "\n",
    "print('next(a):', next(a,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8999326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Now next(a) = None so the code will not print anything \n",
    "for i in a:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c06edf5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "##### using a new generator\n",
    "a = first_n(10)\n",
    "for i in a:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb003e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'StopIteration'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### The next() will raise StopIteration Exception\n",
    "##### since all items are iterated in the max()\n",
    "a = first_n(10)\n",
    "print(max(a))\n",
    "next(a,'StopIteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f8d9024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'StopIteration'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### The next() will raise StopIteration Exception\n",
    "##### since all items are iterated in the list()\n",
    "a = first_n(10)\n",
    "print(list(a))\n",
    "next(a,'StopIteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd8def66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'StopIteration'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### The next() will raise StopIteration Exception\n",
    "##### since all items are iterated in the sorted()\n",
    "a = first_n(10)\n",
    "print(sorted(a))\n",
    "next(a,'StopIteration')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
