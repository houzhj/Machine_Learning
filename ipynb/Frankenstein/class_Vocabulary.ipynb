{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddef8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2f0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('frankenstein_with_splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e82b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data:  (90698, 3)\n",
      "------------------------------------------------------------\n",
      "                                  context        target  split\n",
      "0                                , or the  frankenstein  train\n",
      "1              frankenstein or the modern             ,  train\n",
      "2    frankenstein , the modern prometheus            or  train\n",
      "3  frankenstein , or modern prometheus by           the  train\n",
      "4             , or the prometheus by mary        modern  train\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the data: \", df_all.shape)\n",
    "print('-'*60)\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804fc58",
   "metadata": {},
   "source": [
    "# 1. Vocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebd1f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n",
    "    def __init__(self, token_to_idx=None, \n",
    "                 mask_token=\"<MASK>\", add_unk=True, \n",
    "                 unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            mask_token (str): the MASK token to add into the Vocabulary; indicates\n",
    "                a position that will not be used in updating the model's parameters\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk   = add_unk\n",
    "        self._unk_token = unk_token    \n",
    "        self._mask_token = mask_token\n",
    "        \n",
    "        ### the mask_token, i.e, \"<MASK>\" is the first added token\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        \n",
    "        self.unk_index  = -999\n",
    "        ### the unk_token, i.e, \"<UNK>\" is the second added token if add_unk=True\n",
    "        ### self.unk_index is changed from -999 to 1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            ### add a new element to _token_to_idx\n",
    "            self._token_to_idx[token] = index\n",
    "            ### add a new element to _idx_to_token\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "   \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            ### .get(): return self.unk_index if the key \"token\" does not exist. \n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba2ede",
   "metadata": {},
   "source": [
    "# 2. Instantiate the Vocabulary from the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1945c91",
   "metadata": {},
   "source": [
    "### Initializing cbow_vocab.\n",
    "### The unk_token, i.e,  \"UNK\" (unknown word), is the first added token if add_unk=True. \n",
    "### After the initialization, there are only two tokens stored in the object -MASK and UNK, and the index of these tokens in cbow_vocab is 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4def5db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0, '<UNK>': 1},\n",
       " '_idx_to_token': {0: '<MASK>', 1: '<UNK>'},\n",
       " '_add_unk': True,\n",
       " '_unk_token': '<UNK>',\n",
       " '_mask_token': '<MASK>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_vocab = Vocabulary(add_unk=True)\n",
    "vars(cbow_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab270e01",
   "metadata": {},
   "source": [
    "### Add tokens appear in the ratings to cbow_vocab. \n",
    "\n",
    "### (There is one additional optional step for creating the vocabulary - couting the tokens appeared in the \"context\" and \"target\" columns , and ONLY add frequent tokens that apprear more than a pre-specified number to the Vocabulary, while treat infrequent tokens as UNK.  See an example with cutoff = 25 [here](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/Yelp_Reviews/yelp_perceptron.ipynb).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eabc55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_vocab = Vocabulary()\n",
    "for index, row in df_all.iterrows():\n",
    "    for token in row.context.split(' '):\n",
    "        cbow_vocab.add_token(token)\n",
    "    cbow_vocab.add_token(row.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222602df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7270 tokesn added into cbow_vocab\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(cbow_vocab._token_to_idx)} tokesn added into cbow_vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833828a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, or the prometheus by mary</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>or the modern by mary wollstonecraft</td>\n",
       "      <td>prometheus</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  context        target  split\n",
       "0                                , or the  frankenstein  train\n",
       "1              frankenstein or the modern             ,  train\n",
       "2    frankenstein , the modern prometheus            or  train\n",
       "3  frankenstein , or modern prometheus by           the  train\n",
       "4             , or the prometheus by mary        modern  train\n",
       "5    or the modern by mary wollstonecraft    prometheus  train"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6cc8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens added to cbow_vocab based on the first 6 rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<UNK>', 0),\n",
       " ('<MASK>', 1),\n",
       " (',', 2),\n",
       " ('or', 3),\n",
       " ('the', 4),\n",
       " ('frankenstein', 5),\n",
       " ('modern', 6),\n",
       " ('prometheus', 7),\n",
       " ('by', 8),\n",
       " ('mary', 9)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Tokens added to cbow_vocab based on the first 6 rows:')\n",
    "list(cbow_vocab._token_to_idx.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93527692",
   "metadata": {},
   "source": [
    "# 3. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0537983",
   "metadata": {},
   "source": [
    "### ._token_to_idx: a mapping of index and token added to the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39830ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print out 20 tokens in the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<UNK>', 0),\n",
       " ('<MASK>', 1),\n",
       " (',', 2),\n",
       " ('or', 3),\n",
       " ('the', 4),\n",
       " ('frankenstein', 5),\n",
       " ('modern', 6),\n",
       " ('prometheus', 7),\n",
       " ('by', 8),\n",
       " ('mary', 9),\n",
       " ('wollstonecraft', 10),\n",
       " ('godwin', 11),\n",
       " ('shelley', 12),\n",
       " ('letter', 13),\n",
       " ('st', 14),\n",
       " ('.', 15),\n",
       " ('petersburgh', 16),\n",
       " ('dec', 17),\n",
       " ('th', 18),\n",
       " ('to', 19)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Print out 20 tokens in the vocabulary\")\n",
    "list(cbow_vocab._token_to_idx.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7417ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print a few elements in cbow_vocab._token_to_idx\n",
      "The index for \"one\" is 358\n",
      "The index for \"ten\" is 1710\n",
      "The index for \"hundred\" is 1130\n",
      "The index for \"thousand\" is 185\n",
      "The index for \"million\" is 0\n"
     ]
    }
   ],
   "source": [
    "tokens  = ['one','ten','hundred','thousand','million']\n",
    "mapping = cbow_vocab._token_to_idx\n",
    "print(\"Print a few elements in cbow_vocab._token_to_idx\")\n",
    "for i in tokens:\n",
    "    print(f'The index for \"{i}\" is {mapping.get(i,0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ad486",
   "metadata": {},
   "source": [
    "### ._idx_to_token: a mapping of index and token added to the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64b0569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print out 20 tokens in the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, '<UNK>'),\n",
       " (1, '<MASK>'),\n",
       " (2, ','),\n",
       " (3, 'or'),\n",
       " (4, 'the'),\n",
       " (5, 'frankenstein'),\n",
       " (6, 'modern'),\n",
       " (7, 'prometheus'),\n",
       " (8, 'by'),\n",
       " (9, 'mary'),\n",
       " (10, 'wollstonecraft'),\n",
       " (11, 'godwin'),\n",
       " (12, 'shelley'),\n",
       " (13, 'letter'),\n",
       " (14, 'st'),\n",
       " (15, '.'),\n",
       " (16, 'petersburgh'),\n",
       " (17, 'dec'),\n",
       " (18, 'th'),\n",
       " (19, 'to')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Print out 20 tokens in the vocabulary\")\n",
    "list(cbow_vocab._idx_to_token.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c97b0252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print a few elements cbow_vocab._idx_to_token\n",
      "The token for index=0 is <MASK>\n",
      "The token for index=2 is 0\n",
      "The token for index=6 is 0\n",
      "The token for index=100 is 0\n"
     ]
    }
   ],
   "source": [
    "indices  = [0,2,6,100]\n",
    "mapping = cbow_vocab._idx_to_token\n",
    "print(\"Print a few elements cbow_vocab._idx_to_token\")\n",
    "for i in indices:\n",
    "    print(f'The token for index={i} is {mapping.get(i,0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66cef5",
   "metadata": {},
   "source": [
    "# 4. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cdc72",
   "metadata": {},
   "source": [
    "### add_token(token): Update mapping dicts based on the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40e60fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<UNK>': 0, '<MASK>': 1},\n",
       " '_idx_to_token': {0: '<UNK>', 1: '<MASK>'},\n",
       " '_add_unk': True,\n",
       " '_unk_token': '<UNK>',\n",
       " 'unk_index': 0,\n",
       " '_mask_token': '<MASK>',\n",
       " 'mask_index': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_vocab = Vocabulary(add_unk=True)\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "176db67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add one token apple\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<UNK>': 0, '<MASK>': 1, 'apple': 2},\n",
       " '_idx_to_token': {0: '<UNK>', 1: '<MASK>', 2: 'apple'},\n",
       " '_add_unk': True,\n",
       " '_unk_token': '<UNK>',\n",
       " 'unk_index': 0,\n",
       " '_mask_token': '<MASK>',\n",
       " 'mask_index': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token = 'apple'\n",
    "example_vocab.add_token(new_token)\n",
    "print(f\"Add one token {new_token}\")\n",
    "print('-'*60)\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ce9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add one token banana\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<UNK>': 0, '<MASK>': 1, 'apple': 2, 'banana': 3},\n",
       " '_idx_to_token': {0: '<UNK>', 1: '<MASK>', 2: 'apple', 3: 'banana'},\n",
       " '_add_unk': True,\n",
       " '_unk_token': '<UNK>',\n",
       " 'unk_index': 0,\n",
       " '_mask_token': '<MASK>',\n",
       " 'mask_index': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token = 'banana'\n",
    "example_vocab.add_token(new_token)\n",
    "print(f\"Add one token {new_token}\")\n",
    "print('-'*60)\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec5871",
   "metadata": {},
   "source": [
    "### lookup_token(token): Retrieve the index associated with the token or the UNK index if token isn't present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1b5cb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<UNK>': 0, '<MASK>': 1},\n",
       " '_idx_to_token': {0: '<UNK>', 1: '<MASK>'},\n",
       " '_add_unk': True,\n",
       " '_unk_token': '<UNK>',\n",
       " 'unk_index': 0,\n",
       " '_mask_token': '<MASK>',\n",
       " 'mask_index': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_vocab = Vocabulary(add_unk=True)\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fe5e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple added\n",
      "banana added\n",
      "peach added\n",
      "orange added\n",
      "coconut added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<UNK>': 0,\n",
       "  '<MASK>': 1,\n",
       "  'apple': 2,\n",
       "  'banana': 3,\n",
       "  'peach': 4,\n",
       "  'orange': 5,\n",
       "  'coconut': 6},\n",
       " '_idx_to_token': {0: '<UNK>',\n",
       "  1: '<MASK>',\n",
       "  2: 'apple',\n",
       "  3: 'banana',\n",
       "  4: 'peach',\n",
       "  5: 'orange',\n",
       "  6: 'coconut'},\n",
       " '_add_unk': True,\n",
       " '_unk_token': '<UNK>',\n",
       " 'unk_index': 0,\n",
       " '_mask_token': '<MASK>',\n",
       " 'mask_index': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_add = ['apple','banana','peach','orange','coconut']\n",
    "for i in tokens_to_add:\n",
    "    example_vocab.add_token(i)\n",
    "    print(i + ' added')\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7634d002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index for orange is 5\n",
      "The index for rice is 0\n"
     ]
    }
   ],
   "source": [
    "tokens_list = ['orange','rice']\n",
    "for i in tokens_list:\n",
    "    print(f\"The index for {i} is {example_vocab.lookup_token(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2166ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index for orange is 5\n",
      "The index for rice is 0\n"
     ]
    }
   ],
   "source": [
    "### Equivalent codes\n",
    "for i in tokens_list:\n",
    "    print(f\"The index for {i} is {Vocabulary.lookup_token(example_vocab,i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5e2f7",
   "metadata": {},
   "source": [
    "### lookup_index(index): Return the token associated with the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc06f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token with index=1 is <MASK>\n",
      "The token with index=4 is peach\n"
     ]
    }
   ],
   "source": [
    "indices_list = [1,4]\n",
    "for i in indices_list:\n",
    "    print(f\"The token with index={i} is {example_vocab.lookup_index(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b8800d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token with index=1 is <MASK>\n",
      "The token with index=4 is peach\n"
     ]
    }
   ],
   "source": [
    "### Equivalent codes\n",
    "for i in indices_list:\n",
    "    print(f\"The token with index={i} is {Vocabulary.lookup_index(example_vocab,i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1a705",
   "metadata": {},
   "source": [
    "### \\_\\_len\\_\\_(): Return the length of _token_to_idx (i.e, the number of tokens in the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84349006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<UNK>', 1: '<MASK>', 2: 'token1', 3: 'token2', 4: 'token3', 5: 'token4'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_vocab = Vocabulary(add_unk=True)\n",
    "tokens_to_add = ['token1','token2','token3','token4']\n",
    "for i in tokens_to_add:\n",
    "    example_vocab.add_token(i)\n",
    "example_vocab._idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af5eee36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
