{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc05f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ef5276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    print(f\"seed: {seed}\")\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdbd1659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_from_file(glove_filepath):\n",
    "    \"\"\"\n",
    "    Load the GloVe embeddings \n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): path to the glove embeddings file \n",
    "    Returns:\n",
    "        word_to_index (dict), embeddings (numpy.ndarary)\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, \"r\") as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a81a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a specific set of words.\n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): file path to the glove embeddigns\n",
    "        words (list): list of words in the dataset\n",
    "    \"\"\"\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c897e3a",
   "metadata": {},
   "source": [
    "# Function load_glove_from_file(glove_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d8251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_filepath = \"glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd89412",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index = 0\n",
      "Word: the\n",
      "Updated word_to_index:{'the': 0}\n",
      "Appended embedding vector: the vector for the\n",
      "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Index = 1\n",
      "Word: ,\n",
      "Updated word_to_index:{'the': 0, ',': 1}\n",
      "Appended embedding vector: the vector for ,\n",
      "[-0.10767    0.11053    0.59812   -0.54361    0.67396    0.10663\n",
      "  0.038867   0.35481    0.06351   -0.094189   0.15786   -0.81665\n",
      "  0.14172    0.21939    0.58505   -0.52158    0.22783   -0.16642\n",
      " -0.68228    0.3587     0.42568    0.19021    0.91963    0.57555\n",
      "  0.46185    0.42363   -0.095399  -0.42749   -0.16567   -0.056842\n",
      " -0.29595    0.26037   -0.26606   -0.070404  -0.27662    0.15821\n",
      "  0.69825    0.43081    0.27952   -0.45437   -0.33801   -0.58184\n",
      "  0.22364   -0.5778    -0.26862   -0.20425    0.56394   -0.58524\n",
      " -0.14365   -0.64218    0.0054697 -0.35248    0.16162    1.1796\n",
      " -0.47674   -2.7553    -0.1321    -0.047729   1.0655     1.1034\n",
      " -0.2208     0.18669    0.13177    0.15117    0.7131    -0.35215\n",
      "  0.91348    0.61783    0.70992    0.23955   -0.14571   -0.37859\n",
      " -0.045959  -0.47368    0.2385     0.20536   -0.18996    0.32507\n",
      " -1.1112    -0.36341    0.98679   -0.084776  -0.54008    0.11726\n",
      " -1.0194    -0.24424    0.12771    0.013884   0.080374  -0.35414\n",
      "  0.34951   -0.7226     0.37549    0.4441    -0.99059    0.61214\n",
      " -0.35111   -0.83155    0.45293    0.082577 ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Index = 2\n",
      "Word: .\n",
      "Updated word_to_index:{'the': 0, ',': 1, '.': 2}\n",
      "Appended embedding vector: the vector for .\n",
      "[-0.33979    0.20941    0.46348   -0.64792   -0.38377    0.038034\n",
      "  0.17127    0.15978    0.46619   -0.019169   0.41479   -0.34349\n",
      "  0.26872    0.04464    0.42131   -0.41032    0.15459    0.022239\n",
      " -0.64653    0.25256    0.043136  -0.19445    0.46516    0.45651\n",
      "  0.68588    0.091295   0.21875   -0.70351    0.16785   -0.35079\n",
      " -0.12634    0.66384   -0.2582     0.036542  -0.13605    0.40253\n",
      "  0.14289    0.38132   -0.12283   -0.45886   -0.25282   -0.30432\n",
      " -0.11215   -0.26182   -0.22482   -0.44554    0.2991    -0.85612\n",
      " -0.14503   -0.49086    0.0082973 -0.17491    0.27524    1.4401\n",
      " -0.21239   -2.8435    -0.27958   -0.45722    1.6386     0.78808\n",
      " -0.55262    0.65       0.086426   0.39012    1.0632    -0.35379\n",
      "  0.48328    0.346      0.84174    0.098707  -0.24213   -0.27053\n",
      "  0.045287  -0.40147    0.11395    0.0062226  0.036673   0.018518\n",
      " -1.0213    -0.20806    0.64072   -0.068763  -0.58635    0.33476\n",
      " -1.1432    -0.1148    -0.25091   -0.45907   -0.096819  -0.17946\n",
      " -0.063351  -0.67412   -0.068895   0.53604   -0.87773    0.31802\n",
      " -0.39242   -0.23394    0.47298   -0.028803 ]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##### For demonstration purposes, we'll only use the first 3 lines of glove.6B.100d.txt, \n",
    "word_to_index = {}\n",
    "embeddings = []\n",
    "with open(glove_filepath, \"r\") as fp:\n",
    "    for index, line in enumerate(fp):\n",
    "        if index>=3:\n",
    "            break\n",
    "        print(f\"Index = {index}\")\n",
    "        line = line.split(\" \") \n",
    "        \n",
    "        ### the first element in the list (i.e., the first word in the original line) is the word \n",
    "        word = line[0]\n",
    "        word_to_index[word] = index\n",
    "        print(f\"Word: {word}\")\n",
    "        print(f\"Updated word_to_index:{word_to_index}\")\n",
    "        \n",
    "        ### the remaining elements in the list are 100-dimension vectors \n",
    "        ### transferring str to float\n",
    "        embedding_i = np.array([float(val) for val in line[1:]])\n",
    "        embeddings.append(embedding_i)\n",
    "        \n",
    "        print(f\"Appended embedding vector: the vector for {word}\")\n",
    "        print(embedding_i)\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a10cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of embedding: 3\n"
     ]
    }
   ],
   "source": [
    "### Now embeddings is a list\n",
    "print(f\"The length of embedding: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f1a3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Join a sequence of arrays along a new axis.\n",
    "np.stack(embeddings).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49b69d",
   "metadata": {},
   "source": [
    "# Function make_embedding_matrix(glove_filepath, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897612b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain the word_to_idx\n",
    "word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "846ccbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in glove_embeddings: 400001\n",
      "The dimention of the embeddings: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of words in glove_embeddings: {len(word_to_idx)}\")\n",
    "print(f\"The dimention of the embeddings: {glove_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65150e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = glove_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8450f04",
   "metadata": {},
   "source": [
    "### Look up the embedding (in glove_embeddings) of a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97f95fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding of the word the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'the'\n",
    "print(f\"The embedding of the word {word}\")\n",
    "glove_embeddings[word_to_idx[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a46873",
   "metadata": {},
   "source": [
    "### If a word is not in GloVe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "268d43a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dfewqw'\n"
     ]
    }
   ],
   "source": [
    "### If a word is not in GloVe:\n",
    "word = 'dfewqw'\n",
    "try:\n",
    "    glove_embeddings[word_to_idx[word]]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19388a6",
   "metadata": {},
   "source": [
    "### One common method for handling this is to use an initialization method from the PyTorch library, such as the Xavier Uniform method: [torch.nn.init.xavier_uniform_](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff690da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400001, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4151a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1863,  0.2023, -0.0571,  0.2239, -0.0534,  0.0492, -0.1187,  0.1431,\n",
       "          0.2149, -0.1788,  0.2119,  0.0456,  0.1801,  0.0330,  0.1175, -0.0344,\n",
       "          0.1879,  0.0360, -0.1138,  0.0621, -0.1123, -0.0286, -0.0990,  0.1617,\n",
       "         -0.1924, -0.1124, -0.0688, -0.1466,  0.0230, -0.2407,  0.2201, -0.2070,\n",
       "          0.1882,  0.0406, -0.0791,  0.1506,  0.0380,  0.1969,  0.0266, -0.0769,\n",
       "          0.0655, -0.0661,  0.1026,  0.2176,  0.1409, -0.1066,  0.1407,  0.0436,\n",
       "          0.1238, -0.1486, -0.2413, -0.0942, -0.1869,  0.2000,  0.0702,  0.1010,\n",
       "          0.0771, -0.0042,  0.1907, -0.1732,  0.0153, -0.1664,  0.0752, -0.0839,\n",
       "          0.0747, -0.0508,  0.2022, -0.1445, -0.1454, -0.1454,  0.2192,  0.0812,\n",
       "          0.2345, -0.2011, -0.2418, -0.1907, -0.1640,  0.0987,  0.0873,  0.2025,\n",
       "         -0.1259, -0.1662,  0.1293, -0.0985,  0.1479, -0.0578,  0.1394, -0.1894,\n",
       "         -0.1230,  0.0743,  0.0515, -0.0621,  0.1453,  0.1657, -0.1767, -0.1301,\n",
       "          0.2232, -0.0822, -0.0864, -0.2358]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_i = torch.ones(1, embedding_size)\n",
    "torch.nn.init.xavier_uniform_(embedding_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b36e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['this', 'is', 'a', 'beautiful', 'day']\n",
    "\n",
    "### Initialize the final_embeddings, shape = [number of word = 5, dimension of the embeddings = 100]\n",
    "final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ### If word is in the word_to_idx, look for its vector in glove_embeddings\n",
    "    if word in word_to_idx:\n",
    "        final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "    ### If word is not in the word_to_idx, generate a vector using a Xavier uniform distribution\n",
    "    else:\n",
    "        embedding_i = torch.ones(1, embedding_size)\n",
    "        torch.nn.init.xavier_uniform_(embedding_i)\n",
    "        final_embeddings[i, :] = embedding_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6e665a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word indeced 1 in words: is\n",
      "The embedding of is in final_embeddings\n",
      "[-0.54264    0.41476    1.0322    -0.40244    0.46691    0.21816\n",
      " -0.074864   0.47332    0.080996  -0.22079   -0.12808   -0.1144\n",
      "  0.50891    0.11568    0.028211  -0.3628     0.43823    0.047511\n",
      "  0.20282    0.49857   -0.10068    0.13269    0.16972    0.11653\n",
      "  0.31355    0.25713    0.092783  -0.56826   -0.52975   -0.051456\n",
      " -0.67326    0.92533    0.2693     0.22734    0.66365    0.26221\n",
      "  0.19719    0.2609     0.18774   -0.3454    -0.42635    0.13975\n",
      "  0.56338   -0.56907    0.12398   -0.12894    0.72484   -0.26105\n",
      " -0.26314   -0.43605    0.078908  -0.84146    0.51595    1.3997\n",
      " -0.7646    -3.1453    -0.29202   -0.31247    1.5129     0.52435\n",
      "  0.21456    0.42452   -0.088411  -0.17805    1.1876     0.10579\n",
      "  0.76571    0.21914    0.35824   -0.11636    0.093261  -0.62483\n",
      " -0.21898    0.21796    0.74056   -0.43735    0.14343    0.14719\n",
      " -1.1605    -0.050508   0.12677   -0.014395  -0.98676   -0.091297\n",
      " -1.2054    -0.11974    0.047847  -0.54001    0.52457   -0.70963\n",
      " -0.32528   -0.1346    -0.41314    0.33435   -0.0072412  0.32253\n",
      " -0.044219  -1.2969     0.76217    0.46349  ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Matching the vector for is in glove_embeddings\n",
      "[-0.54264    0.41476    1.0322    -0.40244    0.46691    0.21816\n",
      " -0.074864   0.47332    0.080996  -0.22079   -0.12808   -0.1144\n",
      "  0.50891    0.11568    0.028211  -0.3628     0.43823    0.047511\n",
      "  0.20282    0.49857   -0.10068    0.13269    0.16972    0.11653\n",
      "  0.31355    0.25713    0.092783  -0.56826   -0.52975   -0.051456\n",
      " -0.67326    0.92533    0.2693     0.22734    0.66365    0.26221\n",
      "  0.19719    0.2609     0.18774   -0.3454    -0.42635    0.13975\n",
      "  0.56338   -0.56907    0.12398   -0.12894    0.72484   -0.26105\n",
      " -0.26314   -0.43605    0.078908  -0.84146    0.51595    1.3997\n",
      " -0.7646    -3.1453    -0.29202   -0.31247    1.5129     0.52435\n",
      "  0.21456    0.42452   -0.088411  -0.17805    1.1876     0.10579\n",
      "  0.76571    0.21914    0.35824   -0.11636    0.093261  -0.62483\n",
      " -0.21898    0.21796    0.74056   -0.43735    0.14343    0.14719\n",
      " -1.1605    -0.050508   0.12677   -0.014395  -0.98676   -0.091297\n",
      " -1.2054    -0.11974    0.047847  -0.54001    0.52457   -0.70963\n",
      " -0.32528   -0.1346    -0.41314    0.33435   -0.0072412  0.32253\n",
      " -0.044219  -1.2969     0.76217    0.46349  ]\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print(f\"The word indeced {i} in words: {words[i]}\")\n",
    "print(f\"The embedding of {words[i]} in final_embeddings\")\n",
    "print(final_embeddings[i])\n",
    "print(\"-\"*100)\n",
    "print(\"\")\n",
    "print(f\"Matching the vector for {words[i]} in glove_embeddings\")\n",
    "print(glove_embeddings[word_to_idx[words[i]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf430f74",
   "metadata": {},
   "source": [
    "# Create input and the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c3f48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embedding(load_file=True, save_file=False):\n",
    "    if load_file:\n",
    "        embeddings = np.load('embeddings_glove100.npy')\n",
    "    else:\n",
    "        embeddings = make_embedding_matrix(glove_filepath=\"glove.6B.100d.txt\",\n",
    "                                           words=words)\n",
    "        if save_file:\n",
    "            np.save('embeddings_glove100.npy', embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c8d66b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3566, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_glove100 = get_glove_embedding()\n",
    "embeddings_glove100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cb064d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  7, 16, 14,  6, 15,  0],\n",
       "        [ 4, 10, 13, 18, 14, 10, 14]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "batch_size     = 2\n",
    "length_of_text = 7\n",
    "my_input = torch.randint(0,20,[batch_size, length_of_text])\n",
    "my_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a4aad",
   "metadata": {},
   "source": [
    "# Define the nn.Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5861fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of embeddings (the number of vocabulary items)\n",
    "n_tokens_in_vocabulary = embeddings_glove100.shape[0]\n",
    "# size of the embeddings (embedding dimension)\n",
    "dimension_embedding    = embeddings_glove100.shape[1]\n",
    "# If one specifies padding_idx=0 every input where the value is equal to padding_idx will \n",
    "# be zero-ed out \n",
    "padding_idx            = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd61aa",
   "metadata": {},
   "source": [
    "### Use randomly initialized embeddings (no pre-trained embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27dc92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The parameters of nn.Embedding do not necessarily need to match those of embeddings_glove100.\n",
    "emb_no_pretrained_1 = nn.Embedding(num_embeddings = n_tokens_in_vocabulary,\n",
    "                                 embedding_dim  = dimension_embedding,\n",
    "                                 padding_idx    = 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae4f2d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained, parameters are matched to those in embeddings_glove100\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.5992,  0.4771,  0.7262,  ..., -0.5923,  0.1543,  0.4408],\n",
      "        [ 0.3125, -0.0335, -0.3980,  ..., -0.7712,  0.1799, -2.1268],\n",
      "        ...,\n",
      "        [-0.6912,  0.2276,  0.1961,  ...,  2.0628, -0.5818,  0.3142],\n",
      "        [ 1.0843,  1.1831,  0.9872,  ...,  1.5555,  0.1373, -0.8254],\n",
      "        [ 0.5877,  0.7533, -0.9052,  ..., -0.2541, -1.3026,  0.3046]],\n",
      "       requires_grad=True)\n",
      "------------------------------------------------------------\n",
      "Shape:\n",
      "torch.Size([3566, 100])\n"
     ]
    }
   ],
   "source": [
    "print(\"No pretrained, parameters are matched to those in embeddings_glove100\")\n",
    "print(emb_no_pretrained_1.weight)\n",
    "print(\"-\"*60)\n",
    "print(\"Shape:\")\n",
    "print(emb_no_pretrained_1.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9413d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_no_pretrained_2 = nn.Embedding(num_embeddings = 20,\n",
    "                                   embedding_dim  = 5,\n",
    "                                   padding_idx    = 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b17ea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained, parameters are NOT matched to those in embeddings_glove100\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1411,  0.6992,  0.1621,  2.1184, -0.6479],\n",
      "        [ 0.2340, -0.5506, -0.0992,  0.6625,  0.0203],\n",
      "        [-1.9562, -0.6154,  1.5287, -0.7422,  0.1131],\n",
      "        [-0.0563, -0.3043, -0.8458,  0.2183, -2.1090],\n",
      "        [ 0.4854,  1.3382, -0.0947,  0.9836, -0.3024],\n",
      "        [ 0.9934, -0.6586,  2.0798, -0.5245, -0.5633],\n",
      "        [-0.1805,  2.8780, -1.1575, -0.2114,  0.0456],\n",
      "        [ 0.3959,  0.7365,  0.9696,  0.1698,  0.8996],\n",
      "        [ 1.8139,  1.0857, -0.7120, -0.4977, -1.3081],\n",
      "        [-1.7504,  1.6461, -0.4334,  0.7102, -1.3929],\n",
      "        [ 0.8666,  0.9977,  0.4087, -0.8471, -0.8180],\n",
      "        [-0.3010, -1.1261, -1.3120,  0.5677,  0.0400],\n",
      "        [-0.5751, -1.2112, -2.3304,  0.5915, -0.4762],\n",
      "        [ 1.0297, -0.7221, -0.2239,  0.6002,  0.1559],\n",
      "        [ 0.2766, -0.5412, -0.2827, -1.1301,  0.0931],\n",
      "        [ 1.1534,  0.7729, -1.4189,  0.0849,  1.8991],\n",
      "        [-0.5489, -0.0324, -0.4655, -0.5713, -0.8790],\n",
      "        [ 0.8212,  0.3841,  1.3524, -1.0636, -0.6856],\n",
      "        [-0.2902, -1.0660, -1.2917, -0.7190, -0.3728]], requires_grad=True)\n",
      "------------------------------------------------------------\n",
      "Shape:\n",
      "torch.Size([20, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"No pretrained, parameters are NOT matched to those in embeddings_glove100\")\n",
    "print(emb_no_pretrained_2.weight)\n",
    "print(\"-\"*60)\n",
    "print(\"Shape:\")\n",
    "print(emb_no_pretrained_2.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd88354",
   "metadata": {},
   "source": [
    "### Use pre-trained  GloVe100 embeddings\n",
    "### One can use the _weight attribute of nn.Embedding to set the pretrained word embedding matrix (such as GloVe) as the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a49c3b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weight does not match num_embeddings and embedding_dim\n"
     ]
    }
   ],
   "source": [
    "### The parameters of nn.Embedding need to match those of embeddings_glove100.\n",
    "### Otherwize, it will report an error\n",
    "try:\n",
    "    nn.Embedding(embedding_dim  = 20,\n",
    "                 num_embeddings = 5,\n",
    "                 padding_idx    = 0,\n",
    "                 _weight        = embeddings_glove100)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4f59e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'numpy.ndarray' object has no attribute 'detach'\n"
     ]
    }
   ],
   "source": [
    "### The parameters of nn.Embedding need to be a tensor\n",
    "### Otherwize, it will report an error\n",
    "try:\n",
    "    nn.Embedding(embedding_dim  = dimension_embedding,\n",
    "                 num_embeddings = n_tokens_in_vocabulary,\n",
    "                 padding_idx    = 0,\n",
    "                 _weight        = embeddings_glove100)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "272e14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The parameters of nn.Embedding need to match those of embeddings_glove100.\n",
    "emb_pretrained = nn.Embedding(embedding_dim  = dimension_embedding,\n",
    "                              num_embeddings = n_tokens_in_vocabulary,\n",
    "                              padding_idx    = 0,\n",
    "                              _weight        = torch.from_numpy(embeddings_glove100).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "184e3e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained, parameters are matched to those in embeddings_glove100\n",
      "Parameter containing:\n",
      "tensor([[-0.0382, -0.0121, -0.0462,  ..., -0.0651, -0.0608,  0.1727],\n",
      "        [ 0.2213,  0.0832, -0.0284,  ..., -0.2280,  0.1983, -0.0641],\n",
      "        [-0.0497,  0.0369,  0.2134,  ..., -0.0461,  0.0416, -0.0685],\n",
      "        ...,\n",
      "        [-0.1232,  0.1483, -0.1853,  ...,  0.0279, -0.0490,  0.0812],\n",
      "        [ 0.1091,  0.1739,  0.1431,  ...,  0.0281, -0.2263, -0.0592],\n",
      "        [-0.4105,  0.0906,  0.8362,  ...,  0.1629,  0.9483,  0.2537]],\n",
      "       requires_grad=True)\n",
      "------------------------------------------------------------\n",
      "Shape:\n",
      "torch.Size([3566, 100])\n"
     ]
    }
   ],
   "source": [
    "print(\"Pretrained, parameters are matched to those in embeddings_glove100\")\n",
    "print(emb_pretrained.weight)\n",
    "print(\"-\"*60)\n",
    "print(\"Shape:\")\n",
    "print(emb_pretrained.weight.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
