{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9e6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d09f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('news_with_splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52785cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data:  (120000, 3)\n",
      "------------------------------------------------------------\n",
      "   category  split                                 title\n",
      "0  Business  train    Jobs, tax cuts key issues for Bush\n",
      "1  Business  train  Jarden Buying Mr. Coffee #39;s Maker\n",
      "2  Business  train     Retail sales show festive fervour\n",
      "3  Business  train   Intervoice's Customers Come Calling\n",
      "4  Business  train     Boeing Expects Air Force Contract\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the data: \", df_all.shape)\n",
    "print('-'*60)\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afde8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=False, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk   = add_unk\n",
    "        self._unk_token = unk_token      \n",
    "        self.unk_index  = -999\n",
    "        ### the unk_token, i.e, \"<UNK>\" is the first added token if add_unk=True\n",
    "        ### self.unk_index is changed from -999 to 0\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            ### add a new element to _token_to_idx\n",
    "            self._token_to_idx[token] = index\n",
    "            ### add a new element to _idx_to_token\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "   \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            ### .get(): return self.unk_index if the key \"token\" does not exist. \n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    \n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx = None, \n",
    "                 unk_token          = \"<UNK>\",\n",
    "                 mask_token         = \"<MASK>\", \n",
    "                 begin_seq_token    = \"<BEGIN>\",\n",
    "                 end_seq_token      = \"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        \"\"\"\n",
    "        The follow attributes have been defined in the Vocabulary class:\n",
    "            - ._token_to_idx\n",
    "            - ._idx_to_token\n",
    "            - ._add_unk\n",
    "            - ._unk_token\n",
    "            - .unk_index\n",
    "        \"\"\"\n",
    "        self._mask_token      = mask_token\n",
    "        self._unk_token       = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token   = end_seq_token\n",
    "\n",
    "        self.mask_index       = self.add_token(self._mask_token)\n",
    "        self.unk_index        = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index  = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index    = self.add_token(self._end_seq_token)\n",
    "        \n",
    "        self._add_unk         = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbdb2b7",
   "metadata": {},
   "source": [
    "# 1. NewsVectorizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2aadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    \n",
    "    def __init__(self, title_vocab, category_vocab):\n",
    "        self.title_vocab    = title_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "         \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, news_df, cutoff):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            news_df (pandas.DataFrame): the news dataset\n",
    "            cutoff (int): the parameter for frequency-based filtering\n",
    "        Returns:\n",
    "            an instance of the NewsVectorizer\n",
    "        \"\"\"\n",
    "        category_vocab = Vocabulary()\n",
    "        title_vocab    = SequenceVocabulary()\n",
    "        \n",
    "        ########## Add tokens to category_vocab ('Business','Sci/Tech','Sports','World')\n",
    "        for category in sorted(set(news_df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "            \n",
    "        ########## Add tokens to title_vocab\n",
    "        ### Create a Counter() to count all tokens appears in news_df.title\n",
    "        word_counts = Counter()\n",
    "        for title in news_df.title:\n",
    "            for word in title.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "        ### execute add_token if a word appears more than \"cutoff\" times\n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                title_vocab.add_token(word)\n",
    "                \n",
    "        return cls(title_vocab, category_vocab)\n",
    "    \n",
    "    ### This is the key functionality of the Vectorizer.\n",
    "    ### It takes as an argument a string representing a text,\n",
    "    ### and returns a vectorized representation of the text.\n",
    "    def vectorize(self, title, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        \"\"\"\n",
    "        ### set the first index to be begin_seq_index=2 (defined in SequenceVocabulary)\n",
    "        indices = [self.title_vocab.begin_seq_index]\n",
    "        \n",
    "        ### adding the indeces for the title after the first index\n",
    "        indices.extend(self.title_vocab.lookup_token(token)\n",
    "                       for token in title.split(\" \"))\n",
    "        \n",
    "        ### set the last index to be end_seq_index=3 (defined in SequenceVocabulary)\n",
    "        indices.append(self.title_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.title_vocab.mask_index\n",
    "\n",
    "        return out_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268a8b6",
   "metadata": {},
   "source": [
    "# 2. Instantiate a NewsVectorizer from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667e8d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Jobs, tax cuts key issues for Bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Jarden Buying Mr. Coffee #39;s Maker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Retail sales show festive fervour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Intervoice's Customers Come Calling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Boeing Expects Air Force Contract</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  split                                 title\n",
       "0  Business  train    Jobs, tax cuts key issues for Bush\n",
       "1  Business  train  Jarden Buying Mr. Coffee #39;s Maker\n",
       "2  Business  train     Retail sales show festive fervour\n",
       "3  Business  train   Intervoice's Customers Come Calling\n",
       "4  Business  train     Boeing Expects Air Force Contract"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df_all.copy()\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d526fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instantiate a vectorizer\n",
    "vectorizer = NewsVectorizer.from_dataframe(df_sample,cutoff=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378100ce",
   "metadata": {},
   "source": [
    "### A vectorizer has two vocabularies(attributes), one for title, one for category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3621bd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title_vocab': <__main__.SequenceVocabulary at 0x7fc50aa1e6d0>,\n",
       " 'category_vocab': <__main__.Vocabulary at 0x7fc50aa1e550>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c11e6eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_vocab\n",
      "{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n",
      "{0: 'Business', 1: 'Sci/Tech', 2: 'Sports', 3: 'World'}\n",
      "------------------------------------------------------------\n",
      "title_vocab\n",
      "- Includes 4794 tokens\n",
      "- First ten _token_to_idx:\n",
      "[('<MASK>', 0), ('<UNK>', 1), ('<BEGIN>', 2), ('<END>', 3), ('Jobs,', 4), ('tax', 5), ('cuts', 6), ('key', 7), ('issues', 8), ('for', 9)]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('category_vocab')\n",
    "print(vectorizer.category_vocab._token_to_idx)\n",
    "print(vectorizer.category_vocab._idx_to_token)\n",
    "print('-'*60)\n",
    "print('title_vocab')\n",
    "print(f\"- Includes {len(vectorizer.title_vocab)} tokens\")\n",
    "print(\"- First ten _token_to_idx:\")\n",
    "print(list(vectorizer.title_vocab._token_to_idx.items())[:10])\n",
    "print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2486c",
   "metadata": {},
   "source": [
    "# 3. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f78a7",
   "metadata": {},
   "source": [
    "### (classmethod) from_dataframe(news_df, cutoff): Instantiate the vectorizer from the dataset dataframe.\n",
    "1. First instantiate a Vocabulariy for categories and a SequenceVocabulary for titles, based on the input data \"news_with_splits.csv\".\n",
    "2. Use category_vocab and title_vocab (and a pre-specified cutoff value) as the inputs to instantiate a vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b5097",
   "metadata": {},
   "source": [
    "### vectorize(title): It takes as an argument a string of words separated by a space, and returns a vectorized representation of the string. This is the key functionality of the Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aecb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"the sun is shining and it is a beautiful day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edbea8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens in the example_text is 10\n",
      "The indeces of these tokens in title_vocab:[276, 1, 132, 1, 536, 1860, 132, 101, 1, 72]\n"
     ]
    }
   ],
   "source": [
    "##### Initializing NewsVectorizer\n",
    "vectorizer = NewsVectorizer.from_dataframe(df_sample, 25)\n",
    "indices = [vectorizer.title_vocab.lookup_token(token) for token in example_text.split(' ')]\n",
    "print(f'The number of tokens in the example_text is {len(indices)}')\n",
    "print('The indeces of these tokens in title_vocab:' + str(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa9ad2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_1: [   2  276    1  132    1  536 1860  132  101    1   72    3]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "indeces for <BEGIN> and <END>:\n",
      "[('<MASK>', 0), ('<UNK>', 1), ('<BEGIN>', 2), ('<END>', 3)]\n"
     ]
    }
   ],
   "source": [
    "##### Use NewsVectorizer.vectorize() with vector_length=-1\n",
    "##### i.e., no pre-specified length of index vector \n",
    "##### begin_seq_index:<2> and end_seq_index<3> are added to the front and end of the vector.\n",
    "vector_1 = vectorizer.vectorize(example_text,vector_length=-1)\n",
    "print('vector_1:',vector_1)\n",
    "print('-'*100)\n",
    "print('indeces for <BEGIN> and <END>:')\n",
    "print(list(vectorizer.title_vocab._token_to_idx.items())[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17def95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first token: the\n",
      "The index of the first token in vectorizer.title_vocab: 276\n"
     ]
    }
   ],
   "source": [
    "token = 'the'\n",
    "index = vectorizer.title_vocab.lookup_token(token) \n",
    "print(f\"The first token: {token}\")\n",
    "print(f\"The index of the first token in vectorizer.title_vocab: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bb45978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fourth token: shining\n",
      "The index of the fourth token in vectorizer.title_vocab: 1\n"
     ]
    }
   ],
   "source": [
    "token = 'shining'\n",
    "index = vectorizer.title_vocab.lookup_token(token) \n",
    "print(f\"The fourth token: {token}\")\n",
    "print(f\"The index of the fourth token in vectorizer.title_vocab: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bce663a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,  276,    1,  132,    1,  536, 1860,  132,  101,    1,   72,\n",
       "          3,    0,    0,    0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Use NewsVectorizer.vectorize() with vector_length>len(indices)\n",
    "##### out_vector[len(indices):] are assigned as NewsVectorizer.title_vocab.mask_index\n",
    "##### I.e., if the number of tokens in the context is less than the max length, \n",
    "##### the remaining entries are filled with zeros. \n",
    "vector_2 = vectorizer.vectorize(example_text,vector_length=15)\n",
    "vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65bb6645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not broadcast input array from shape (12,) into shape (5,)\n"
     ]
    }
   ],
   "source": [
    "##### Use NewsVectorizer.vectorize() with vector_length<len(indices)\n",
    "try:\n",
    "    vector_3 = vectorizer.vectorize(example_text,vector_length=5)\n",
    "    vector_3\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb337c",
   "metadata": {},
   "source": [
    "### Use NewsVectorizer.vectorize() with vector_length = max length among all comments, so that the vectors for different rows will have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27a3d5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Earth is the third planet from the Sun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Earth is the only astronomical object known to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Earth has a dynamic atmosphere.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            context\n",
       "0   0            Earth is the third planet from the Sun.\n",
       "1   1  Earth is the only astronomical object known to...\n",
       "2   2                    Earth has a dynamic atmosphere."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_1 = 'Earth is the third planet from the Sun.'\n",
    "context_2 = 'Earth is the only astronomical object known to harbor life.'\n",
    "context_3 = 'Earth has a dynamic atmosphere.'\n",
    "context_df = pd.DataFrame(dict(id=[i for i in range(3)],\n",
    "                               context=[context_1,context_2,context_3]))\n",
    "\n",
    "context_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bba846e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context 0:\n",
      "Earth is the third planet from the Sun.\n",
      "length of context 0: 8\n",
      "------------------------------------------------------------\n",
      "Context 1:\n",
      "Earth is the only astronomical object known to harbor life.\n",
      "length of context 1: 10\n",
      "------------------------------------------------------------\n",
      "Context 2:\n",
      "Earth has a dynamic atmosphere.\n",
      "length of context 2: 5\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### A function returns the length (number of tokens) in a context\n",
    "measure_len = lambda context: len(context.split(\" \"))\n",
    "\n",
    "### calculate the length of each context in context_df\n",
    "for i in range(3):\n",
    "    print(f\"Context {i}:\")\n",
    "    print(context_df.loc[i,'context'])\n",
    "    print(f\"length of context {i}: {measure_len(context_df.loc[i,'context'])}\")\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2da4a387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 10, 5]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Use map() function\n",
    "list(map(measure_len,context_df['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ee68164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(map(measure_len,context_df['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49c35f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth is the third planet from the Sun.\n",
      "[   2 4228  132  276   70    1  386  276    1    3    0    0]\n",
      "Earth is the only astronomical object known to harbor life.\n",
      "[   2 4228  132  276 2260    1    1    1   39    1    1    3]\n",
      "Earth has a dynamic atmosphere.\n",
      "[   2 4228 1850  101    1    1    3    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "### +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "max_length = max(map(measure_len,context_df['context']))+2\n",
    "for i in range(3):\n",
    "    text_now = context_df.loc[i,'context']\n",
    "    print(text_now)\n",
    "    print(vectorizer.vectorize(text_now,vector_length=max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570f045",
   "metadata": {},
   "source": [
    "### Use NewsVectorizer.vectorize() with different cutoff values\n",
    "#### The larger the cutoff, the fewer tokens with appearance counts greater than this value, and the more tokens in the text with an index of 1 in the vector representation (indicating they are recognized as \"unk\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84b37ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: the sun is shining and it is a beautiful day\n",
      "cutoff=10\n",
      "Title Vocabulary: the words appear >10 times\n",
      "The number of tokens: 9530\n",
      "Vector representation: [   2  321 5924  143    1  632 2453  143  110    1   79    3]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "cutoff=50\n",
      "Title Vocabulary: the words appear >50 times\n",
      "The number of tokens: 2642\n",
      "Vector representation: [   2  231    1  117    1  444 1347  117   92    1   66    3]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "cutoff=100\n",
      "Title Vocabulary: the words appear >100 times\n",
      "The number of tokens: 1288\n",
      "Vector representation: [  2 175   1  90   1 326 846  90  69   1  48   3]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "cutoff=1000\n",
      "Title Vocabulary: the words appear >1000 times\n",
      "The number of tokens: 45\n",
      "Vector representation: [ 2 29  1 20  1 39  1 20 17  1  1  3]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Text:', example_text)\n",
    "cut_off_list = [10,50,100,1000]\n",
    "for c in cut_off_list:\n",
    "    vectorizer = vectorizer = NewsVectorizer.from_dataframe(df_sample, c)\n",
    "    vector     = vectorizer.vectorize(example_text)\n",
    "    print(f\"cutoff={c}\")\n",
    "    print(f'Title Vocabulary: the words appear >{c} times')\n",
    "    print(f'The number of tokens: {len(vectorizer.title_vocab)}')\n",
    "    print('Vector representation:', vector)\n",
    "    print('-'*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
