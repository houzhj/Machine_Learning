{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f5f7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131f15e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data:  (120000, 3)\n",
      "------------------------------------------------------------\n",
      "   category  split                                 title\n",
      "0  Business  train    Jobs, tax cuts key issues for Bush\n",
      "1  Business  train  Jarden Buying Mr. Coffee #39;s Maker\n",
      "2  Business  train     Retail sales show festive fervour\n",
      "3  Business  train   Intervoice's Customers Come Calling\n",
      "4  Business  train     Boeing Expects Air Force Contract\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv('news_with_splits.csv')\n",
    "print(\"shape of the data: \", df_all.shape)\n",
    "print('-'*60)\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682ba82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    cutoff = 1,\n",
    "    device = 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efaabca",
   "metadata": {},
   "source": [
    "# Define two relevent classes\n",
    "### - Vocabulary ([see a walkthrough here](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/AGNews/class_Vocabulary.ipynb))\n",
    "### - SequenceVocabulary ([see a walkthrough here](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/AGNews/class_SequenceVocabulary.ipynb))\n",
    "### - NewsVectorizer ([see a walkthrough here](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/AGNews/class_Vectorizer.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f2753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            ### add a new element to _token_to_idx\n",
    "            self._token_to_idx[token] = index\n",
    "            ### add a new element to _idx_to_token\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "   \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    \n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, \n",
    "                 token_to_idx    = None, \n",
    "                 unk_token       = \"<UNK>\",\n",
    "                 mask_token      = \"<MASK>\", \n",
    "                 begin_seq_token = \"<BEGIN>\",\n",
    "                 end_seq_token   = \"<END>\"):\n",
    "        \n",
    "        \n",
    "        super().__init__(token_to_idx)\n",
    "        \"\"\"\n",
    "        The follow attributes have been defined in the Vocabulary class:\n",
    "            - ._token_to_idx\n",
    "            - ._idx_to_token\n",
    "        \"\"\"\n",
    "\n",
    "        self._mask_token      = mask_token      # default: \"<MASK>\"\n",
    "        self._unk_token       = unk_token       # default: \"<UNK>\"\n",
    "        self._begin_seq_token = begin_seq_token # default: \"<BEGIN>\"\n",
    "        self._end_seq_token   = end_seq_token   # default: \"<END>\"\n",
    "\n",
    "        self.mask_index       = self.add_token(self._mask_token)      # return 0\n",
    "        self.unk_index        = self.add_token(self._unk_token)       # return 1\n",
    "        self.begin_seq_index  = self.add_token(self._begin_seq_token) # return 2\n",
    "        self.end_seq_index    = self.add_token(self._end_seq_token)   # return 3\n",
    "        \n",
    "    \n",
    "    ### Overriding the self.lookup_token() method\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "        \n",
    "class NewsVectorizer(object):\n",
    "    \n",
    "    def __init__(self, title_vocab, category_vocab):\n",
    "        self.title_vocab    = title_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "         \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, news_df, cutoff):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            news_df (pandas.DataFrame): the news dataset\n",
    "            cutoff (int): the parameter for frequency-based filtering\n",
    "        Returns:\n",
    "            an instance of the NewsVectorizer\n",
    "        \"\"\"\n",
    "        category_vocab = Vocabulary()\n",
    "        title_vocab    = SequenceVocabulary()\n",
    "        \n",
    "        ########## Add tokens to category_vocab ('Business','Sci/Tech','Sports','World')\n",
    "        for category in sorted(set(news_df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "            \n",
    "        ########## Add tokens to title_vocab\n",
    "        ### Create a Counter() to count all tokens appears in news_df.title\n",
    "        word_counts = Counter()\n",
    "        for title in news_df.title:\n",
    "            for word in title.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "        ### execute add_token if a word appears more than \"cutoff\" times\n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                title_vocab.add_token(word)\n",
    "                \n",
    "        return cls(title_vocab, category_vocab)\n",
    "    \n",
    "    ### This is the key functionality of the Vectorizer.\n",
    "    ### It takes as an argument a string representing a text,\n",
    "    ### and returns a vectorized representation of the text.\n",
    "    def vectorize(self, title, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        \"\"\"\n",
    "        ### set the first index to be begin_seq_index=2 (defined in SequenceVocabulary)\n",
    "        indices = [self.title_vocab.begin_seq_index]\n",
    "        \n",
    "        ### adding the indeces for the title after the first index\n",
    "        indices.extend(self.title_vocab.lookup_token(token)\n",
    "                       for token in title.split(\" \"))\n",
    "        \n",
    "        ### set the last index to be end_seq_index=3 (defined in SequenceVocabulary)\n",
    "        indices.append(self.title_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.title_vocab.mask_index\n",
    "\n",
    "        return out_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d61628",
   "metadata": {},
   "source": [
    "# 1. NewsDataset class\n",
    "### - The Dataset class will characterize the key features of the dataset.\n",
    "### - In the initialization function of the class, make the class inherit the properties of torch.utils.data.Dataset so that we can later leverage its functionalities.\n",
    "### - In the \\_\\_init\\_\\_() function and the set_split() function, store important information such as labels and the features that we wish to generate at each pass.\n",
    "### - Each call requests a sample index for which the upperbound is specified in the \\_\\_len\\_\\_() method.\n",
    "### - When the sample corresponding to a given index is called, the generator executes the \\_\\_getitem\\_\\_() method to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336ed30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self,news_df,vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            news_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (NewsVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.news_df     = news_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        ### NewsVectorizer.vectorize() with be used with the parameter \n",
    "        ### vector_length = self._max_seq_length (the max length among all comments),\n",
    "        ### so that the vectors for different rows will have the same length.\n",
    "        measure_len = lambda text: len(text.split(\" \"))\n",
    "        ### +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        self._max_seq_length = max(map(measure_len, news_df.title)) + 2\n",
    "        \n",
    "        self.train_df    = self.news_df[self.news_df.split=='train']\n",
    "        self.train_size  = len(self.train_df)\n",
    "\n",
    "        self.val_df      = self.news_df[self.news_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df     = self.news_df[self.news_df.split=='test']\n",
    "        self.test_size   = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val'  : (self.val_df, self.validation_size),\n",
    "                             'test' : (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # Class weights\n",
    "        class_counts = news_df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "    @classmethod\n",
    "    def load_csv_and_make_vectorizer(cls,news_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "            news_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of NewsDataset\n",
    "        \"\"\"\n",
    "        news_csv = pd.read_csv(news_csv)\n",
    "        ### make vectorizer using training dataset\n",
    "        train_news_df   = news_df[news_df.split=='train']\n",
    "        vectorizer  = NewsVectorizer.from_dataframe(train_news_df,args.cutoff)\n",
    "        return cls(news_df,vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_df_and_make_vectorizer(cls,news_df):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "            news_df: dataset\n",
    "        Returns:\n",
    "            an instance of NewsDataset\n",
    "        \"\"\"\n",
    "        ### make vectorizer using training dataset\n",
    "        train_news_df  = news_df[news_df.split=='train']\n",
    "        vectorizer = NewsVectorizer.from_dataframe(train_news_df,args.cutoff)\n",
    "        return cls(news_df,vectorizer)\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        ### when split = 'train', _target_df means the training set\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        ### _target_size is defined in set_split() \n",
    "        return self._target_size        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        \n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        title_vector = \\\n",
    "            self._vectorizer.vectorize(row.title, self._max_seq_length)\n",
    "\n",
    "        category_index   = self._vectorizer.category_vocab.lookup_token(row.category)\n",
    "\n",
    "        return {'x_data': title_vector,\n",
    "                'y_target': category_index}\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eeb7c0",
   "metadata": {},
   "source": [
    "# 2. Instantiate a NewsDataset from the training data\n",
    "### There are two classmethods can be used to instantiate a NewsDataset: load_csv_and_make_vectorizer() and load_df_and_make_vectorizer(). The difference is whether the input data is from a csv file or a pd.DataFrame file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6784708b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76467</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Jets remain unbeaten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27991</th>\n",
       "      <td>Business</td>\n",
       "      <td>test</td>\n",
       "      <td>AstraZeneca in drive to slash costs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62646</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>R. Williams Could Be Eligible to Start Next Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28374</th>\n",
       "      <td>Business</td>\n",
       "      <td>test</td>\n",
       "      <td>So much for the  #39;soft patch #39;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98305</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>Bush urges N. Ireland leaders to accept Anglo-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category  split                                              title\n",
       "76467    Sports  train                               Jets remain unbeaten\n",
       "27991  Business   test                AstraZeneca in drive to slash costs\n",
       "62646    Sports  train  R. Williams Could Be Eligible to Start Next Se...\n",
       "28374  Business   test               So much for the  #39;soft patch #39;\n",
       "98305     World  train  Bush urges N. Ireland leaders to accept Anglo-..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df_all.sample(200, random_state=1)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c98b6",
   "metadata": {},
   "source": [
    "### Create a CBOWDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c111ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = NewsDataset.load_df_and_make_vectorizer(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0225ae",
   "metadata": {},
   "source": [
    "## 2.1 - Attributes of a CBOWDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc92dc7",
   "metadata": {},
   "source": [
    "### .cbow_df: the input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ecf3088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76467</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Jets remain unbeaten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27991</th>\n",
       "      <td>Business</td>\n",
       "      <td>test</td>\n",
       "      <td>AstraZeneca in drive to slash costs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62646</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>R. Williams Could Be Eligible to Start Next Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28374</th>\n",
       "      <td>Business</td>\n",
       "      <td>test</td>\n",
       "      <td>So much for the  #39;soft patch #39;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98305</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>Bush urges N. Ireland leaders to accept Anglo-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36523</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>train</td>\n",
       "      <td>Apple iMac G5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66196</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Petacchi sprints to third victory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51981</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>val</td>\n",
       "      <td>Bare cupboards force space station crew to diet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85956</th>\n",
       "      <td>Sports</td>\n",
       "      <td>test</td>\n",
       "      <td>Strong to Coach Gators in the Peach Bowl (AP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18176</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Nokia plans 40 new models in  #39;05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category  split                                              title\n",
       "76467    Sports  train                               Jets remain unbeaten\n",
       "27991  Business   test                AstraZeneca in drive to slash costs\n",
       "62646    Sports  train  R. Williams Could Be Eligible to Start Next Se...\n",
       "28374  Business   test               So much for the  #39;soft patch #39;\n",
       "98305     World  train  Bush urges N. Ireland leaders to accept Anglo-...\n",
       "...         ...    ...                                                ...\n",
       "36523  Sci/Tech  train                                      Apple iMac G5\n",
       "66196    Sports  train                  Petacchi sprints to third victory\n",
       "51981  Sci/Tech    val    Bare cupboards force space station crew to diet\n",
       "85956    Sports   test      Strong to Coach Gators in the Peach Bowl (AP)\n",
       "18176  Business  train               Nokia plans 40 new models in  #39;05\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "177ca416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.news_df.equals(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8801876",
   "metadata": {},
   "source": [
    "### ._max_seq_length: max number of tokens in a context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64438dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample._max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7ee85",
   "metadata": {},
   "source": [
    "### ._vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3387bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note that the vectorizer is derived from the training split. \n",
    "v = dataset_sample._vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea070b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"the sun is shining and it is a beautiful day\"\n",
    "vector       = v.vectorize(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34c23dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Vocabulary: number of tokens: 100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The 100 items in '_idx_to_token':  [('<MASK>', 0), ('<UNK>', 1), ('<BEGIN>', 2), ('<END>', 3), ('to', 4), ('Start', 5), ('N.', 6), ('(AFP)', 7), ('German', 8), ('nuclear', 9), ('be', 10), ('a', 11), ('#39;', 12), ('Mac', 13), ('start', 14), ('Out', 15), ('New', 16), ('of', 17), ('#39;s', 18), ('fall', 19), ('Deal', 20), ('Open', 21), ('in', 22), ('Up', 23), ('as', 24), ('beat', 25), ('Say', 26), ('Star', 27), ('(Reuters)', 28), ('Since', 29), ('up', 30), ('with', 31), ('the', 32), ('door', 33), ('saves', 34), ('by', 35), ('Oil', 36), ('EU', 37), ('on', 38), ('Stocks', 39), ('Data', 40), ('Is', 41), ('for', 42), ('IBM', 43), ('China', 44), ('over', 45), ('Microsoft', 46), ('first', 47), ('day', 48), ('Down', 49), ('lead', 50), ('Six', 51), ('vow', 52), ('Boeing', 53), ('Percent', 54), ('Powell', 55), ('no', 56), ('U.S.', 57), ('supporters', 58), ('(Canadian', 59), ('Press)', 60), ('Cambodia', 61), ('new', 62), ('crisis', 63), ('Treasuries', 64), ('Study', 65), ('Off', 66), ('Against', 67), ('(AP)', 68), ('100', 69), ('Nokia', 70), ('back', 71), ('cutting', 72), ('sell', 73), ('executive', 74), ('case', 75), ('says', 76), ('Unemployment', 77), ('Web', 78), ('Move', 79), ('Carter', 80), ('after', 81), ('Net', 82), ('Iran', 83), ('and', 84), ('Australian', 85), ('Palestinian', 86), ('&lt;b&gt;...&lt;/b&gt;', 87), ('change', 88), ('US', 89), ('Cup', 90), ('victory', 91), ('Google', 92), ('at', 93), ('Has', 94), ('Iraq', 95), ('seeks', 96), ('Pakistan', 97), ('Apple', 98), ('British', 99)]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Example text: the sun is shining and it is a beautiful day\n",
      "vector representation: [ 2 32  1  1  1 84  1  1 11  1 48  3]\n"
     ]
    }
   ],
   "source": [
    "print(f'Title Vocabulary: number of tokens: {len(v.title_vocab)}')\n",
    "print('-'*100)\n",
    "print(f\"The {len(v.title_vocab)} items in '_idx_to_token': \", list(v.title_vocab._token_to_idx.items()))\n",
    "print('-'*100)\n",
    "print(\"Example text:\", example_text)\n",
    "print('vector representation:', vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a359072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first index: ('<BEGIN>', 2)\n",
      "The last index: ('<END>', 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"The first index: ('<BEGIN>', 2)\")\n",
    "print(\"The last index: ('<END>', 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c0bdac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The indeces in the middle corresponed to the mapping stored in title_vocab.\n",
      "Example - the second last index in the vector representation:\n",
      "The index of 'day': 48\n",
      "The token of '48': day\n"
     ]
    }
   ],
   "source": [
    "print(\"The indeces in the middle corresponed to the mapping stored in title_vocab.\")\n",
    "print(\"Example - the second last index in the vector representation:\")\n",
    "print(f\"The index of 'day': {v.title_vocab._token_to_idx['day']}\")\n",
    "print(f\"The token of '48': {v.title_vocab._idx_to_token[48]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f76a9b",
   "metadata": {},
   "source": [
    "### ._target_df, _target_size\n",
    "**Defined by method set_split()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c526883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76467</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Jets remain unbeaten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62646</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>R. Williams Could Be Eligible to Start Next Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98305</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>Bush urges N. Ireland leaders to accept Anglo-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97960</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>German FM: Iranian nuclear arms buildup would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12114</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>UPDATE 1-Freddie Mac to start payments to form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67695</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Spring change slows Cup cars a bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48961</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>train</td>\n",
       "      <td>BlackBerry in sync with Mac OS X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36523</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>train</td>\n",
       "      <td>Apple iMac G5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66196</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Petacchi sprints to third victory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18176</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Nokia plans 40 new models in  #39;05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category  split                                              title\n",
       "76467    Sports  train                               Jets remain unbeaten\n",
       "62646    Sports  train  R. Williams Could Be Eligible to Start Next Se...\n",
       "98305     World  train  Bush urges N. Ireland leaders to accept Anglo-...\n",
       "97960     World  train  German FM: Iranian nuclear arms buildup would ...\n",
       "12114  Business  train  UPDATE 1-Freddie Mac to start payments to form...\n",
       "...         ...    ...                                                ...\n",
       "67695    Sports  train                 Spring change slows Cup cars a bit\n",
       "48961  Sci/Tech  train                   BlackBerry in sync with Mac OS X\n",
       "36523  Sci/Tech  train                                      Apple iMac G5\n",
       "66196    Sports  train                  Petacchi sprints to third victory\n",
       "18176  Business  train               Nokia plans 40 new models in  #39;05\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample._target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30bb66b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample._target_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952364c",
   "metadata": {},
   "source": [
    "### ._lookup_dict - will be used in the method set_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec3860e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (       category  split                                              title\n",
       "  76467    Sports  train                               Jets remain unbeaten\n",
       "  62646    Sports  train  R. Williams Could Be Eligible to Start Next Se...\n",
       "  98305     World  train  Bush urges N. Ireland leaders to accept Anglo-...\n",
       "  97960     World  train  German FM: Iranian nuclear arms buildup would ...\n",
       "  12114  Business  train  UPDATE 1-Freddie Mac to start payments to form...\n",
       "  ...         ...    ...                                                ...\n",
       "  67695    Sports  train                 Spring change slows Cup cars a bit\n",
       "  48961  Sci/Tech  train                   BlackBerry in sync with Mac OS X\n",
       "  36523  Sci/Tech  train                                      Apple iMac G5\n",
       "  66196    Sports  train                  Petacchi sprints to third victory\n",
       "  18176  Business  train               Nokia plans 40 new models in  #39;05\n",
       "  \n",
       "  [150 rows x 3 columns],\n",
       "  150),\n",
       " 'val': (        category split                                              title\n",
       "  23632   Business   val            Marsh to Make Payments More Transparent\n",
       "  83309     Sports   val      Rams QB Leaves Game With Shoulder Injury (AP)\n",
       "  114704     World   val  Farooqi key link between Pakistan and al Qaeda...\n",
       "  115317     World   val         China's Land Grabs Raise Specter of Unrest\n",
       "  24902   Business   val     Select Comfort down; lowers sales expectations\n",
       "  54089   Sci/Tech   val  Australia Says It's Starting to Win Its Locust...\n",
       "  81400     Sports   val                             MRFIXIT: EASTER PARADE\n",
       "  112105     World   val         Mortars hit Baghdad safe zone, killing one\n",
       "  114698     World   val         Japanese hostage in Iraq reportedly killed\n",
       "  51546   Sci/Tech   val              Tiny memory card for mobiles launched\n",
       "  112472     World   val  Zimbabwe Convicts Accused  #39;Mercenary #39; ...\n",
       "  82867     Sports   val            Henman Battles Into Basel Quarterfinals\n",
       "  23142   Business   val  AMICUS CALLS ON ADAIR TURNER TO RECOMMEND COMP...\n",
       "  24367   Business   val          Costco Disappoints on Profit, Stock Falls\n",
       "  52673   Sci/Tech   val             John, Paul, George, Ringo...and Steve?\n",
       "  24936   Business   val  Mortgage Applications Ease in Aug 27 Week (Reu...\n",
       "  114255     World   val  About 100 Dead in Floods in Philippine Town --...\n",
       "  84615     Sports   val              Mickelson out of WGC event in Ireland\n",
       "  23933   Business   val              Icelandair buys small part of easyJet\n",
       "  23888   Business   val         Mars consolidates advertising with Omnicom\n",
       "  24626   Business   val                         Wal-Mart spooks US traders\n",
       "  53021   Sci/Tech   val         Firefox surpasses 10 million download mark\n",
       "  21360   Business   val      Airbus raises projection of deliveries to 320\n",
       "  51789   Sci/Tech   val                         Wi-Fi Adapters Turn Inward\n",
       "  112288     World   val  Bush says truth must come out regarding milita...\n",
       "  83657     Sports   val                   Sosa responds to Baker criticism\n",
       "  85362     Sports   val    Kobe Bryant arrives in court for jury selection\n",
       "  51981   Sci/Tech   val    Bare cupboards force space station crew to diet,\n",
       "  28),\n",
       " 'test': (        category split                                              title\n",
       "  27991   Business  test                AstraZeneca in drive to slash costs\n",
       "  28374   Business  test               So much for the  #39;soft patch #39;\n",
       "  86331     Sports  test  Chelsea impresses in opening Champions League ...\n",
       "  59555   Sci/Tech  test             Security alert identifies Oracle holes\n",
       "  119159     World  test                 Malaysia finds more bird flu cases\n",
       "  27036   Business  test                         PeopleSoft's Prevarication\n",
       "  59312   Sci/Tech  test           Thousands Sign Up for Space Flights (AP)\n",
       "  119421     World  test       Polls: Coors Gains Ground in Colo. Race (AP)\n",
       "  118674     World  test        Militants Release Video of Japanese Hostage\n",
       "  57519   Sci/Tech  test                              Apple AirPort Express\n",
       "  29351   Business  test           NWA #39;s Anderson moves to UnitedHealth\n",
       "  118504     World  test             SBS journo released by captors in Iraq\n",
       "  57131   Sci/Tech  test        NASA Sets Swift Launch for Saturday Nov. 20\n",
       "  29133   Business  test  Saudi Arabia Is Sued by Firm That Lost 658 in ...\n",
       "  86843     Sports  test                     Watson comes up a winner twice\n",
       "  86809     Sports  test                              Dodgers keep it going\n",
       "  89644     Sports  test  Armstrong short on summits in quest for new peaks\n",
       "  118937     World  test                  Huge haul of ecstasy in Australia\n",
       "  27641   Business  test  Yukos Cuts China National Petroleum #39;s Oil ...\n",
       "  59715   Sci/Tech  test     Rising Fuel Prices Boost Renewable Energy (AP)\n",
       "  87695     Sports  test                          Kansas City vs. Tennessee\n",
       "  85956     Sports  test      Strong to Coach Gators in the Peach Bowl (AP),\n",
       "  22)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample._lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57c0d32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       category  split                                              title\n",
       " 76467    Sports  train                               Jets remain unbeaten\n",
       " 62646    Sports  train  R. Williams Could Be Eligible to Start Next Se...\n",
       " 98305     World  train  Bush urges N. Ireland leaders to accept Anglo-...\n",
       " 97960     World  train  German FM: Iranian nuclear arms buildup would ...\n",
       " 12114  Business  train  UPDATE 1-Freddie Mac to start payments to form...\n",
       " ...         ...    ...                                                ...\n",
       " 67695    Sports  train                 Spring change slows Cup cars a bit\n",
       " 48961  Sci/Tech  train                   BlackBerry in sync with Mac OS X\n",
       " 36523  Sci/Tech  train                                      Apple iMac G5\n",
       " 66196    Sports  train                  Petacchi sprints to third victory\n",
       " 18176  Business  train               Nokia plans 40 new models in  #39;05\n",
       " \n",
       " [150 rows x 3 columns],\n",
       " 150)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### A dictionary which contains a df and a scalar\n",
    "dataset_sample._lookup_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fdc9532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76467</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Jets remain unbeaten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62646</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>R. Williams Could Be Eligible to Start Next Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98305</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>Bush urges N. Ireland leaders to accept Anglo-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97960</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>German FM: Iranian nuclear arms buildup would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12114</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>UPDATE 1-Freddie Mac to start payments to form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67695</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Spring change slows Cup cars a bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48961</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>train</td>\n",
       "      <td>BlackBerry in sync with Mac OS X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36523</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>train</td>\n",
       "      <td>Apple iMac G5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66196</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Petacchi sprints to third victory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18176</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Nokia plans 40 new models in  #39;05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category  split                                              title\n",
       "76467    Sports  train                               Jets remain unbeaten\n",
       "62646    Sports  train  R. Williams Could Be Eligible to Start Next Se...\n",
       "98305     World  train  Bush urges N. Ireland leaders to accept Anglo-...\n",
       "97960     World  train  German FM: Iranian nuclear arms buildup would ...\n",
       "12114  Business  train  UPDATE 1-Freddie Mac to start payments to form...\n",
       "...         ...    ...                                                ...\n",
       "67695    Sports  train                 Spring change slows Cup cars a bit\n",
       "48961  Sci/Tech  train                   BlackBerry in sync with Mac OS X\n",
       "36523  Sci/Tech  train                                      Apple iMac G5\n",
       "66196    Sports  train                  Petacchi sprints to third victory\n",
       "18176  Business  train               Nokia plans 40 new models in  #39;05\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### the dataframe\n",
    "dataset_sample._lookup_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ea4c33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### the sample size\n",
    "dataset_sample._lookup_dict['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b974f",
   "metadata": {},
   "source": [
    "## 2.2 - Methods of a CBOWDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014c440",
   "metadata": {},
   "source": [
    "### \\_\\_len()\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2a9ee4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd9617",
   "metadata": {},
   "source": [
    "### \\_\\_getitem()\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c81d7520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([ 2,  8,  1,  1,  9,  1,  1,  1, 10, 11,  1,  1, 12,  3,  0,  0,  0,\n",
       "         0]),\n",
       " 'y_target': 3}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The 4th element in the \"train\" split\n",
    "### In the __init__ function, self.set_split('train') defines ._target_df\n",
    "dataset_sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3f8e011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76467</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Jets remain unbeaten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62646</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>R. Williams Could Be Eligible to Start Next Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98305</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>Bush urges N. Ireland leaders to accept Anglo-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97960</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>German FM: Iranian nuclear arms buildup would ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category  split                                              title\n",
       "76467   Sports  train                               Jets remain unbeaten\n",
       "62646   Sports  train  R. Williams Could Be Eligible to Start Next Se...\n",
       "98305    World  train  Bush urges N. Ireland leaders to accept Anglo-...\n",
       "97960    World  train  German FM: Iranian nuclear arms buildup would ..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.loc[df_sample['split']=='train',].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "935f943f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'German FM: Iranian nuclear arms buildup would be a  #39;nightmare #39;'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.loc[df_sample['split']=='train',].iloc[3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "922a4596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AFP)\n",
      "German\n",
      "nuclear\n",
      "be\n",
      "a\n",
      "#39;\n",
      "Mac\n"
     ]
    }
   ],
   "source": [
    "for i in range(7,14):\n",
    "    print(dataset_sample._vectorizer.title_vocab._idx_to_token[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b3eb72",
   "metadata": {},
   "source": [
    "### set_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd13cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = NewsDataset.load_df_and_make_vectorizer(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f0c92fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76467</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Jets remain unbeaten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62646</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>R. Williams Could Be Eligible to Start Next Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98305</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>Bush urges N. Ireland leaders to accept Anglo-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97960</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>German FM: Iranian nuclear arms buildup would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12114</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>UPDATE 1-Freddie Mac to start payments to form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67695</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Spring change slows Cup cars a bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48961</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>train</td>\n",
       "      <td>BlackBerry in sync with Mac OS X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36523</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>train</td>\n",
       "      <td>Apple iMac G5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66196</th>\n",
       "      <td>Sports</td>\n",
       "      <td>train</td>\n",
       "      <td>Petacchi sprints to third victory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18176</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Nokia plans 40 new models in  #39;05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category  split                                              title\n",
       "76467    Sports  train                               Jets remain unbeaten\n",
       "62646    Sports  train  R. Williams Could Be Eligible to Start Next Se...\n",
       "98305     World  train  Bush urges N. Ireland leaders to accept Anglo-...\n",
       "97960     World  train  German FM: Iranian nuclear arms buildup would ...\n",
       "12114  Business  train  UPDATE 1-Freddie Mac to start payments to form...\n",
       "...         ...    ...                                                ...\n",
       "67695    Sports  train                 Spring change slows Cup cars a bit\n",
       "48961  Sci/Tech  train                   BlackBerry in sync with Mac OS X\n",
       "36523  Sci/Tech  train                                      Apple iMac G5\n",
       "66196    Sports  train                  Petacchi sprints to third victory\n",
       "18176  Business  train               Nokia plans 40 new models in  #39;05\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now the split for ._target_df and _target_size is 'train'\n",
    "dataset_sample._target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27626144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14848e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([ 2,  8,  1,  1,  9,  1,  1,  1, 10, 11,  1,  1, 12,  3,  0,  0,  0,\n",
       "         0]),\n",
       " 'y_target': 3}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The 4th element in the \"train\" split\n",
    "dataset_sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a931c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "### run set_split, switch the split to 'val'\n",
    "dataset_sample.set_split('val')\n",
    "# or \n",
    "# CBOWDataset.set_split(dataset_sample,'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f847e938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23632</th>\n",
       "      <td>Business</td>\n",
       "      <td>val</td>\n",
       "      <td>Marsh to Make Payments More Transparent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83309</th>\n",
       "      <td>Sports</td>\n",
       "      <td>val</td>\n",
       "      <td>Rams QB Leaves Game With Shoulder Injury (AP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114704</th>\n",
       "      <td>World</td>\n",
       "      <td>val</td>\n",
       "      <td>Farooqi key link between Pakistan and al Qaeda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115317</th>\n",
       "      <td>World</td>\n",
       "      <td>val</td>\n",
       "      <td>China's Land Grabs Raise Specter of Unrest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24902</th>\n",
       "      <td>Business</td>\n",
       "      <td>val</td>\n",
       "      <td>Select Comfort down; lowers sales expectations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54089</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>val</td>\n",
       "      <td>Australia Says It's Starting to Win Its Locust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81400</th>\n",
       "      <td>Sports</td>\n",
       "      <td>val</td>\n",
       "      <td>MRFIXIT: EASTER PARADE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112105</th>\n",
       "      <td>World</td>\n",
       "      <td>val</td>\n",
       "      <td>Mortars hit Baghdad safe zone, killing one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114698</th>\n",
       "      <td>World</td>\n",
       "      <td>val</td>\n",
       "      <td>Japanese hostage in Iraq reportedly killed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51546</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>val</td>\n",
       "      <td>Tiny memory card for mobiles launched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112472</th>\n",
       "      <td>World</td>\n",
       "      <td>val</td>\n",
       "      <td>Zimbabwe Convicts Accused  #39;Mercenary #39; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82867</th>\n",
       "      <td>Sports</td>\n",
       "      <td>val</td>\n",
       "      <td>Henman Battles Into Basel Quarterfinals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23142</th>\n",
       "      <td>Business</td>\n",
       "      <td>val</td>\n",
       "      <td>AMICUS CALLS ON ADAIR TURNER TO RECOMMEND COMP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24367</th>\n",
       "      <td>Business</td>\n",
       "      <td>val</td>\n",
       "      <td>Costco Disappoints on Profit, Stock Falls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52673</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>val</td>\n",
       "      <td>John, Paul, George, Ringo...and Steve?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24936</th>\n",
       "      <td>Business</td>\n",
       "      <td>val</td>\n",
       "      <td>Mortgage Applications Ease in Aug 27 Week (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114255</th>\n",
       "      <td>World</td>\n",
       "      <td>val</td>\n",
       "      <td>About 100 Dead in Floods in Philippine Town --...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84615</th>\n",
       "      <td>Sports</td>\n",
       "      <td>val</td>\n",
       "      <td>Mickelson out of WGC event in Ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23933</th>\n",
       "      <td>Business</td>\n",
       "      <td>val</td>\n",
       "      <td>Icelandair buys small part of easyJet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23888</th>\n",
       "      <td>Business</td>\n",
       "      <td>val</td>\n",
       "      <td>Mars consolidates advertising with Omnicom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24626</th>\n",
       "      <td>Business</td>\n",
       "      <td>val</td>\n",
       "      <td>Wal-Mart spooks US traders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53021</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>val</td>\n",
       "      <td>Firefox surpasses 10 million download mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21360</th>\n",
       "      <td>Business</td>\n",
       "      <td>val</td>\n",
       "      <td>Airbus raises projection of deliveries to 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51789</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>val</td>\n",
       "      <td>Wi-Fi Adapters Turn Inward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112288</th>\n",
       "      <td>World</td>\n",
       "      <td>val</td>\n",
       "      <td>Bush says truth must come out regarding milita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83657</th>\n",
       "      <td>Sports</td>\n",
       "      <td>val</td>\n",
       "      <td>Sosa responds to Baker criticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85362</th>\n",
       "      <td>Sports</td>\n",
       "      <td>val</td>\n",
       "      <td>Kobe Bryant arrives in court for jury selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51981</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>val</td>\n",
       "      <td>Bare cupboards force space station crew to diet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category split                                              title\n",
       "23632   Business   val            Marsh to Make Payments More Transparent\n",
       "83309     Sports   val      Rams QB Leaves Game With Shoulder Injury (AP)\n",
       "114704     World   val  Farooqi key link between Pakistan and al Qaeda...\n",
       "115317     World   val         China's Land Grabs Raise Specter of Unrest\n",
       "24902   Business   val     Select Comfort down; lowers sales expectations\n",
       "54089   Sci/Tech   val  Australia Says It's Starting to Win Its Locust...\n",
       "81400     Sports   val                             MRFIXIT: EASTER PARADE\n",
       "112105     World   val         Mortars hit Baghdad safe zone, killing one\n",
       "114698     World   val         Japanese hostage in Iraq reportedly killed\n",
       "51546   Sci/Tech   val              Tiny memory card for mobiles launched\n",
       "112472     World   val  Zimbabwe Convicts Accused  #39;Mercenary #39; ...\n",
       "82867     Sports   val            Henman Battles Into Basel Quarterfinals\n",
       "23142   Business   val  AMICUS CALLS ON ADAIR TURNER TO RECOMMEND COMP...\n",
       "24367   Business   val          Costco Disappoints on Profit, Stock Falls\n",
       "52673   Sci/Tech   val             John, Paul, George, Ringo...and Steve?\n",
       "24936   Business   val  Mortgage Applications Ease in Aug 27 Week (Reu...\n",
       "114255     World   val  About 100 Dead in Floods in Philippine Town --...\n",
       "84615     Sports   val              Mickelson out of WGC event in Ireland\n",
       "23933   Business   val              Icelandair buys small part of easyJet\n",
       "23888   Business   val         Mars consolidates advertising with Omnicom\n",
       "24626   Business   val                         Wal-Mart spooks US traders\n",
       "53021   Sci/Tech   val         Firefox surpasses 10 million download mark\n",
       "21360   Business   val      Airbus raises projection of deliveries to 320\n",
       "51789   Sci/Tech   val                         Wi-Fi Adapters Turn Inward\n",
       "112288     World   val  Bush says truth must come out regarding milita...\n",
       "83657     Sports   val                   Sosa responds to Baker criticism\n",
       "85362     Sports   val    Kobe Bryant arrives in court for jury selection\n",
       "51981   Sci/Tech   val    Bare cupboards force space station crew to diet"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now the split for ._target_df and _target_size is 'val'\n",
    "dataset_sample._target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fef7f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04955d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([ 2,  1,  1,  1,  1,  1, 17,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0]),\n",
       " 'y_target': 3}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The 4th element in the \"val\" split\n",
    "dataset_sample[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f2c3f",
   "metadata": {},
   "source": [
    "### get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7d3c839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NewsVectorizer at 0x7fc708b50190>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e427ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NewsVectorizer at 0x7fc708b50190>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Equivalently\n",
    "dataset_sample._vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750f609",
   "metadata": {},
   "source": [
    "### get_num_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "278b79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = NewsDataset.load_df_and_make_vectorizer(df_sample)\n",
    "### Switch the split to 'train'\n",
    "dataset_sample.set_split('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d697ed6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.get_num_batches(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62c67788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample._target_df)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7464e8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample._target_df)//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e0f7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Switch the split to 'val'\n",
    "dataset_sample.set_split('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f2dacf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.get_num_batches(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5bff7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample._target_df)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f88d7ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedca03f",
   "metadata": {},
   "source": [
    "# 3. Define a batch generator\n",
    "### - Wrap the DataLoader\n",
    "### - Switch the data between the CPU and the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "255860b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device='cpu'):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d4140",
   "metadata": {},
   "source": [
    "## 3.1 Dataset Class\n",
    "### - The Dataset class characterizes the key features of the dataset you want to generate.\n",
    "### - The class uses \\_\\_init\\_\\_(), \\_\\_len\\_\\_(), and \\_\\_getitem\\_\\_() to store important information, and generate samples. \n",
    "### - The Dataset class is an important argument of the DataLoader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cb9f686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: {'x1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'x2': [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], 'y': [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0]}\n",
      "------------------------------------------------------------\n",
      "df:     x1  x2  y\n",
      "0    1  13  0\n",
      "1    2  14  1\n",
      "2    3  15  0\n",
      "3    4  16  1\n",
      "4    5  17  1\n",
      "5    6  18  0\n",
      "6    7  19  0\n",
      "7    8  20  1\n",
      "8    9  21  1\n",
      "9   10  22  0\n",
      "10  11  23  1\n",
      "11  12  24  0\n"
     ]
    }
   ],
   "source": [
    "data = {'x1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "        'x2': [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
    "        'y': [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0]}\n",
    "data\n",
    "df = pd.DataFrame(data)\n",
    "print(\"data:\" ,data)\n",
    "print(\"-\"*60)\n",
    "print(\"df:\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffe2a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "[tensor([[ 1., 13.],\n",
      "        [ 2., 14.],\n",
      "        [ 3., 15.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "[tensor([[ 4., 16.],\n",
      "        [ 5., 17.],\n",
      "        [ 6., 18.]]), tensor([1., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "[tensor([[ 7., 19.],\n",
      "        [ 8., 20.],\n",
      "        [ 9., 21.]]), tensor([0., 1., 1.])]\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "[tensor([[10., 22.],\n",
      "        [11., 23.],\n",
      "        [12., 24.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##### Define Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = torch.tensor(self.data.iloc[index, :-1].values, dtype=torch.float32)\n",
    "        target = torch.tensor(self.data.iloc[index, -1], dtype=torch.float32)\n",
    "        return sample, target\n",
    "\n",
    "##### Instantiate the Dataset class\n",
    "custom_dataset = CustomDataset(df)\n",
    "\n",
    "##### Instantiate the DataLoader class\n",
    "batch_size  = 3\n",
    "data_loader = DataLoader(dataset=custom_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##### Obtain the batch\n",
    "i = 0\n",
    "for batch in data_loader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(batch)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c885a",
   "metadata": {},
   "source": [
    "### An alternative is to use TensorDataset() directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bbc1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3828382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n",
      "x2: tensor([13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24.])\n",
      "y: tensor([0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.from_numpy(df['x1'].values).float()\n",
    "x2 = torch.from_numpy(df['x2'].values).float()\n",
    "y  = torch.from_numpy(df['y'].values).float()\n",
    "print(\"x1:\", x1)\n",
    "print(\"x2:\", x2)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0989dcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 13.],\n",
       "        [ 2., 14.],\n",
       "        [ 3., 15.],\n",
       "        [ 4., 16.],\n",
       "        [ 5., 17.],\n",
       "        [ 6., 18.],\n",
       "        [ 7., 19.],\n",
       "        [ 8., 20.],\n",
       "        [ 9., 21.],\n",
       "        [10., 22.],\n",
       "        [11., 23.],\n",
       "        [12., 24.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.stack([x1, x2], dim=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb029abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "[tensor([[ 1., 13.],\n",
      "        [ 2., 14.],\n",
      "        [ 3., 15.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "[tensor([[ 4., 16.],\n",
      "        [ 5., 17.],\n",
      "        [ 6., 18.]]), tensor([1., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "[tensor([[ 7., 19.],\n",
      "        [ 8., 20.],\n",
      "        [ 9., 21.]]), tensor([0., 1., 1.])]\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "[tensor([[10., 22.],\n",
      "        [11., 23.],\n",
      "        [12., 24.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##### Create Tensor dataset\n",
    "dataset     = TensorDataset(features, y)\n",
    "batch_size  = 3\n",
    "\n",
    "##### Instantiate the DataLoader class\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##### Obtain the batch\n",
    "i = 0\n",
    "for batch in data_loader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(batch)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace0dd4",
   "metadata": {},
   "source": [
    "### The two methods below are equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d431af5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 13.],\n",
       "        [ 2., 14.],\n",
       "        [ 3., 15.],\n",
       "        [ 4., 16.],\n",
       "        [ 5., 17.],\n",
       "        [ 6., 18.],\n",
       "        [ 7., 19.],\n",
       "        [ 8., 20.],\n",
       "        [ 9., 21.],\n",
       "        [10., 22.],\n",
       "        [11., 23.],\n",
       "        [12., 24.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.from_numpy(df['x1'].values).float()\n",
    "x2 = torch.from_numpy(df['x2'].values).float()\n",
    "torch.stack([x1, x2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "724eebb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 13],\n",
       "        [ 2, 14],\n",
       "        [ 3, 15],\n",
       "        [ 4, 16],\n",
       "        [ 5, 17],\n",
       "        [ 6, 18],\n",
       "        [ 7, 19],\n",
       "        [ 8, 20],\n",
       "        [ 9, 21],\n",
       "        [10, 22],\n",
       "        [11, 23],\n",
       "        [12, 24]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_array = df[['x1', 'x2']].to_numpy()\n",
    "torch.from_numpy(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d5cc0",
   "metadata": {},
   "source": [
    "## 3.2 DataLoader\n",
    "### - batch_size: denotes the number of samples contained in each generated batch.\n",
    "### - shuffle: if set to True, we will get a new order of exploration at each pass (or just keep a linear exploration scheme otherwise). Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.\n",
    "### - drop_last: set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47cf3dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_sample[0]['x_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2fda99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = NewsDataset.load_df_and_make_vectorizer(df_sample)\n",
    "batch_size     = 30\n",
    "shuffle        = True\n",
    "drop_last      = True\n",
    "dataloader     = DataLoader(dataset=dataset_sample, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ac53c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x in one batch\n",
      "tensor([[ 2,  1,  1, 34,  1, 35,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 41,  1,  1,  1, 31, 78,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 99,  1, 22,  1,  1,  1,  7,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 38, 11,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 98,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 34, 48, 42,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 50,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 42, 32,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 57,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 58,  1,  1,  1, 22,  1,  1,  1,  1,  4,  1, 59, 60,  3,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 77,  1, 35,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 55,  1, 17,  1, 95,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 38,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 27, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 41,  1,  4,  1,  1, 42,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 22,  1, 31, 13,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  1, 17,  1,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 82,  1, 23,  1, 54,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 56,  1, 42,  1, 57,  1,  1, 58,  1, 59, 60,  3,  0,  0],\n",
      "        [ 2, 85,  1,  1,  1, 24,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 51,  1,  4,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 85, 39,  1,  1, 94,  1,  1, 29,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 23, 84, 49,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "size of x_data: torch.Size([30, 18])\n",
      "------------------------------------------------------------\n",
      "y in one batch\n",
      "tensor([0, 1, 3, 2, 1, 1, 2, 2, 3, 0, 1, 3, 0, 0, 3, 3, 2, 2, 0, 3, 1, 1, 0, 0,\n",
      "        3, 0, 2, 0, 0, 3])\n",
      "size of y_data: torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "one_batch = next(iter(dataloader))\n",
    "print('x in one batch')\n",
    "print(one_batch['x_data'])\n",
    "print('size of x_data:', one_batch['x_data'].shape)\n",
    "print('-' * 60)\n",
    "print('y in one batch')\n",
    "print(one_batch['y_target'])\n",
    "print('size of y_data:', one_batch['y_target'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7622b",
   "metadata": {},
   "source": [
    "### In this example, dataloader utilizes the return from the \\_\\_getitem\\_\\_() method, which extracts related rows from the _target_df of dataset, with _target_size=65. Also, batch_size=10, and drop_last=True so there are 6 batches created (the last 5 rows are dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37215098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in the target_df:  150\n",
      "number of rows in the target_df:  150\n",
      "The number of batches is: 5\n"
     ]
    }
   ],
   "source": [
    "print('number of rows in the target_df: ', len(dataset_sample._target_df))\n",
    "print('number of rows in the target_df: ', dataset_sample._target_size)\n",
    "print(\"The number of batches is:\",dataset_sample.get_num_batches(batch_size = batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcf58a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "{'x_data': tensor([[ 2,  1, 94,  1,  1, 38,  1, 24,  1,  1,  4,  1,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 79, 42,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 23,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 39,  1,  1, 38, 40,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 83,  1,  4,  1,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 57,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 24,  1, 25,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 93,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 61, 18,  1,  1, 24, 62,  1, 63,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  4,  1,  1, 21,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 80,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 16,  1, 17, 32,  1, 17,  1, 36,  4,  1,  1,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 93,  1,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 32, 33,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 80,  1,  1, 81, 11,  1, 14,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1, 49,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 74,  1, 22,  1, 75,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 92, 96,  1,  4, 73,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  4,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 64, 23, 24, 36,  1,  1,  1, 28,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 21, 12, 26,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 88,  1, 90,  1, 11,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 41,  1,  4,  1,  1, 42,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 55,  1, 17,  1, 95,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 97, 38,  1, 81,  1,  1,  1, 22,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 13,  4, 14,  1,  4,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 18, 19,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 38,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]), 'y_target': tensor([2, 2, 2, 0, 0, 3, 1, 2, 1, 3, 2, 2, 0, 2, 1, 2, 3, 1, 0, 0, 3, 0, 3, 2,\n",
      "        0, 3, 3, 0, 0, 3])}\n",
      "torch.Size([30, 18])\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "{'x_data': tensor([[ 2,  1,  1, 92, 18,  1,  1,  1, 93,  1,  1,  1,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 96,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  4,  1, 66, 32,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 84,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  1, 17,  1,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 98,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  4,  1,  1,  1, 47, 22,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 67,  1,  1,  1,  1,  1, 87,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 41,  1, 22,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 65,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 38, 53, 18,  1, 74,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 41,  1,  1,  1, 31, 78,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 63,  1,  1,  1,  1,  1,  1, 12,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 58,  1,  1,  1, 22,  1,  1,  1,  1,  4,  1, 59, 60,  3,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  4,  1,  1,  1,  1,  1, 35,  1,  1, 59, 60,  3],\n",
      "        [ 2,  1,  1,  1, 17,  1, 42,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 85,  1,  1,  1, 24,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 22, 38,  1,  1, 24, 89,  1,  1,  1, 38, 87,  3,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 56,  1, 42,  1, 57,  1,  1, 58,  1, 59, 60,  3,  0,  0],\n",
      "        [ 2,  1,  1, 25,  1,  4,  1, 50,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 99,  1, 22,  1,  1,  1,  7,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  4,  1, 66, 40,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 18,  1,  1, 11,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 42,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  1, 69,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 42, 97,  1, 22, 95,  3,  0,  0,  0,  0,  0,  0,  0,  0]]), 'y_target': tensor([0, 3, 2, 0, 2, 1, 1, 1, 0, 1, 3, 1, 0, 1, 0, 3, 3, 3, 3, 1, 0, 3, 3, 2,\n",
      "        3, 0, 2, 2, 3, 3])}\n",
      "torch.Size([30, 18])\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "{'x_data': tensor([[ 2, 86,  1,  1, 88, 22,  1, 18,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 44, 18,  1,  1, 22,  1, 17,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 55,  1,  1, 22,  6,  1,  9,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 16,  1, 17,  1,  1,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 22,  1, 31, 13,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 45,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 21, 23,  1,  4,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 22,  1, 75,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 38, 11,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 85, 39,  1,  1, 94,  1,  1, 29,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  4,  1, 91,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  8,  1,  1,  9,  1,  1,  1, 10, 11,  1,  1, 12,  3,  0,  0,  0,  0],\n",
      "        [ 2,  1,  4, 73,  1,  1, 72,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1, 79,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 28,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 22,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 34,  1, 35,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 22, 82,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  4,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 46,  1,  1,  1, 38, 47, 48,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  1, 28,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  4,  1, 15, 16,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 84,  1,  1,  1, 22,  1, 38,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 49,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]), 'y_target': tensor([3, 1, 3, 0, 1, 1, 3, 1, 3, 1, 0, 2, 0, 3, 0, 2, 3, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 3, 3, 2, 0])}\n",
      "torch.Size([30, 18])\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "{'x_data': tensor([[ 2, 51,  1,  4,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 24,  1,  1,  1, 76,  1,  1, 10, 56, 86, 87,  3,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  4,  5,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 81, 32, 19,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 22,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 27,  1, 23,  1,  1,  1,  1, 12,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 70,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 78,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 38,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 98,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 42,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 31,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  4,  5,  1, 67,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 77,  1,  1, 35,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 50,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 77,  1, 35,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 37,  1,  4,  1,  1, 38, 46, 22,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 43,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 52,  1,  4,  1,  4,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 42,  1,  1, 35,  1, 76, 33,  1,  1, 42,  1,  1, 59, 60,  3,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 78,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 37,  1,  1,  4,  1, 20, 38,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 11,  1,  1, 42, 95,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 26, 23,  4, 69,  1,  1, 22,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  4,  1, 30,  1,  1,  1,  1,  1,  4,  1, 22,  3,  0,  0,  0,  0],\n",
      "        [ 2, 70,  1, 71,  1,  1, 35, 72, 71,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 70,  1,  1, 62,  1, 22,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0]]), 'y_target': tensor([2, 3, 2, 2, 3, 2, 2, 1, 1, 3, 1, 2, 1, 2, 0, 2, 0, 0, 1, 3, 3, 3, 0, 3,\n",
      "        2, 3, 3, 0, 1, 0])}\n",
      "torch.Size([30, 18])\n",
      "------------------------------------------------------------\n",
      "Batch 4\n",
      "{'x_data': tensor([[ 2, 53,  1,  1, 54,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  6,  1,  1,  4,  1,  1,  1,  7,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 20,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 84,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 32,  1, 17, 11,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 42, 32,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 51,  1, 24,  1,  1, 52,  4,  1, 30,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 23, 84, 49,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  4,  1, 22,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  4, 44,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 43,  1, 15,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 83,  1,  1,  1,  1, 11,  1,  1, 57,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 34, 48, 42,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 82,  1, 23,  1, 54,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 99,  1,  1, 38,  1,  4,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 89, 22,  1,  1, 90, 91,  7,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 29,  1, 28,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 42,  1, 41,  1,  1, 22,  1,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 61,  4,  1, 15,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 35,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  8,  1,  1, 31,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 22,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 64,  1, 22, 40,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 27, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 21,  4,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 84,  1, 42, 47,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 65,  1,  1,  1,  1, 93,  1, 45,  1,  1,  3,  0,  0,  0,  0,  0]]), 'y_target': tensor([0, 3, 2, 1, 0, 2, 0, 3, 2, 0, 3, 0, 3, 1, 3, 2, 0, 1, 3, 0, 3, 3, 3, 1,\n",
      "        2, 0, 2, 0, 2, 1])}\n",
      "torch.Size([30, 18])\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for data_dict in dataloader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(data_dict)\n",
    "    print(data_dict['x_data'].shape)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2433c26",
   "metadata": {},
   "source": [
    "### This is equvalent to defining and using the generator function generate_batches()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b396fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "{'x_data': tensor([[ 2,  1, 23, 84, 49,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 65,  1,  1,  1,  1, 93,  1, 45,  1,  1,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 93,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 17,  1, 42,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 78,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 41,  1, 22,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 27,  1, 23,  1,  1,  1,  1, 12,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 20,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 31,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 50,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 35,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 70,  1,  1, 62,  1, 22,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 98,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 77,  1,  1, 35,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 42,  1, 41,  1,  1, 22,  1,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  4, 73,  1,  1, 72,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 18,  1,  1, 11,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 38,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  1, 28,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 94,  1,  1, 38,  1, 24,  1,  1,  4,  1,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 64,  1, 22, 40,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 46,  1,  1,  1, 38, 47, 48,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 42,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 38, 53, 18,  1, 74,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  8,  1,  1,  9,  1,  1,  1, 10, 11,  1,  1, 12,  3,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 38,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]), 'y_target': tensor([0, 1, 1, 1, 0, 1, 3, 2, 2, 1, 3, 2, 3, 0, 1, 0, 3, 0, 2, 3, 3, 0, 2, 2,\n",
      "        0, 1, 2, 0, 3, 3])}\n",
      "torch.Size([30, 18])\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "{'x_data': tensor([[ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 74,  1, 22,  1, 75,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 25,  1,  4,  1, 50,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 29,  1, 28,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 41,  1,  4,  1,  1, 42,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 22,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 22,  1, 31, 13,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  8,  1,  1, 31,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 22, 38,  1,  1, 24, 89,  1,  1,  1, 38, 87,  3,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  1, 69,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 58,  1,  1,  1, 22,  1,  1,  1,  1,  4,  1, 59, 60,  3,  0,  0],\n",
      "        [ 2, 85, 39,  1,  1, 94,  1,  1, 29,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 21, 23,  1,  4,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 83,  1,  1,  1,  1, 11,  1,  1, 57,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 18, 19,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 98,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 34,  1, 35,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 22,  1, 75,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 99,  1,  1, 38,  1,  4,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 77,  1, 35,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  4,  1, 66, 40,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 82,  1, 23,  1, 54,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 84,  1, 42, 47,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1, 49,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 86,  1,  1, 88, 22,  1, 18,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  4, 44,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]), 'y_target': tensor([0, 0, 2, 0, 0, 1, 0, 3, 1, 1, 2, 3, 3, 3, 0, 1, 3, 0, 1, 0, 3, 1, 0, 2,\n",
      "        0, 0, 2, 1, 3, 0])}\n",
      "torch.Size([30, 18])\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "{'x_data': tensor([[ 2,  1,  1, 24,  1, 25,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 32,  1, 17, 11,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  4,  1, 15, 16,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 45,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 42, 32,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 41,  1,  1,  1, 31, 78,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 64, 23, 24, 36,  1,  1,  1, 28,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 78,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 80,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 27, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  4,  1, 30,  1,  1,  1,  1,  1,  4,  1, 22,  3,  0,  0,  0,  0],\n",
      "        [ 2,  1, 61,  4,  1, 15,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 34, 48, 42,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 52,  1,  4,  1,  4,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 53,  1,  1, 54,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 42,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 37,  1,  1,  4,  1, 20, 38,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 96,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  4,  1, 91,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1, 79,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 42, 97,  1, 22, 95,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  6,  1,  1,  4,  1,  1,  1,  7,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 23,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 70,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  4,  1,  1,  1,  1,  1, 35,  1,  1, 59, 60,  3],\n",
      "        [ 2,  1, 88,  1, 90,  1, 11,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  4,  1, 22,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0]]), 'y_target': tensor([2, 0, 3, 3, 0, 1, 0, 0, 2, 2, 0, 3, 3, 2, 3, 0, 2, 3, 2, 3, 3, 2, 2, 3,\n",
      "        3, 0, 1, 3, 2, 3])}\n",
      "torch.Size([30, 18])\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "{'x_data': tensor([[ 2,  1, 84,  1,  1,  1, 22,  1, 38,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 28,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 81, 32, 19,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 22,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 92, 18,  1,  1,  1, 93,  1,  1,  1,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  4,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 32, 33,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 51,  1,  4,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 16,  1, 17,  1,  1,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 13,  4, 14,  1,  4,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 24,  1,  1,  1, 76,  1,  1, 10, 56, 86, 87,  3,  0,  0,  0],\n",
      "        [ 2,  1, 80,  1,  1, 81, 11,  1, 14,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  4,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 43,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 56,  1, 42,  1, 57,  1,  1, 58,  1, 59, 60,  3,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 79, 42,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 22, 82,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 93,  1,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 92, 96,  1,  4, 73,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 21, 12, 26,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 21,  4,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 42,  1,  1, 35,  1, 76, 33,  1,  1, 42,  1,  1, 59, 60,  3,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 10,  1, 17,  1,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  4,  1,  1, 21,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 97, 38,  1, 81,  1,  1,  1, 22,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  4,  1, 66, 32,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 55,  1,  1, 22,  6,  1,  9,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 89, 22,  1,  1, 90, 91,  7,  3,  0,  0,  0,  0,  0,  0,  0]]), 'y_target': tensor([2, 3, 3, 2, 2, 0, 3, 1, 2, 1, 0, 3, 2, 0, 1, 3, 2, 1, 2, 0, 3, 0, 3, 1,\n",
      "        2, 3, 3, 2, 3, 3])}\n",
      "torch.Size([30, 18])\n",
      "------------------------------------------------------------\n",
      "Batch 4\n",
      "{'x_data': tensor([[ 2,  1,  4,  1,  1,  1, 47, 22,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 61, 18,  1,  1, 24, 62,  1, 63,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 49,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 16,  1, 17, 32,  1, 17,  1, 36,  4,  1,  1,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2, 44, 18,  1,  1, 22,  1, 17,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  4,  5,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 83,  1,  4,  1,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  4,  5,  1, 67,  1, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 11,  1,  1, 42, 95,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 67,  1,  1,  1,  1,  1, 87,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 55,  1, 17,  1, 95,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 43,  1, 15,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 85,  1,  1,  1, 24,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 99,  1, 22,  1,  1,  1,  7,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 65,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 39,  1,  1, 38, 40,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 37,  1,  4,  1,  1, 38, 46, 22,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1, 84,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1, 57,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 70,  1, 71,  1,  1, 35, 72, 71,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 26, 23,  4, 69,  1,  1, 22,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1, 22,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 63,  1,  1,  1,  1,  1,  1, 12,  3,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1, 38, 11,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1,  1,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 51,  1, 24,  1,  1, 52,  4,  1, 30,  1,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  1, 84,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]), 'y_target': tensor([1, 3, 0, 0, 1, 2, 3, 2, 3, 2, 1, 3, 1, 0, 3, 1, 0, 0, 0, 0, 1, 1, 1, 3,\n",
      "        0, 0, 1, 0, 3, 2])}\n",
      "torch.Size([30, 18])\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for data_dict in dataloader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(data_dict)\n",
    "    print(data_dict['x_data'].shape)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0743de",
   "metadata": {},
   "source": [
    "## 3.3 Generator\n",
    "### - Generator functions declare a function that behaves like an iterator, i.e. it can be used in a for loop.\n",
    "### - A generator function is defined just like a normal function, but whenever it needs to generate a value, it does so with the yield keyword rather than return. \n",
    "### - Yield is used in Python generators. If the body of a def contains yield, the function automatically becomes a generator function. \n",
    "### - *return* sends a specified value back to its caller whereas *yield* can produce a sequence of values. We should use *yield* when we want to iterate over a sequence, but don’t want to store the entire sequence in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a01ec",
   "metadata": {},
   "source": [
    "### Consider a task to calculate the sum of the first n integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c313379f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### The function below builds the full list in memory\n",
    "def first_n(n):\n",
    "    num, nums = 0, []\n",
    "    while num < n:\n",
    "        nums.append(num)\n",
    "        num += 1\n",
    "    return nums\n",
    "sum(first_n(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac676abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vars(a): {'n': 10, 'num': 0}\n",
      "sum(a): 45\n"
     ]
    }
   ],
   "source": [
    "##### The following implements generator as an iterable object.\n",
    "class first_n(object):\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.num = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    # Python 3 compatibility\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        if self.num < self.n:\n",
    "            cur, self.num = self.num, self.num+1\n",
    "            return cur\n",
    "        raise StopIteration\n",
    "        \n",
    "a = first_n(10)\n",
    "print('vars(a):', vars(a))\n",
    "print('sum(a):', sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "55b34e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next(a): 0\n",
      "sum(a): 45\n",
      "next(a): None\n"
     ]
    }
   ],
   "source": [
    "##### a generator that yields items instead of returning a list\n",
    "\n",
    "def first_n(n):\n",
    "    num = 0\n",
    "    while num < n:\n",
    "        yield num\n",
    "        num += 1\n",
    "\n",
    "a = first_n(10)\n",
    "\n",
    "print('next(a):', next(a))\n",
    "print('sum(a):', sum(a))\n",
    "##### In Python, some built-in functions like sum(a), max(a), list(a) iterates through each element\n",
    "##### in 'a' and calculate the sum/max/list. This means sum(a) traverses all elements in the iterator\n",
    "##### 'a' until the iteration is completed. If the generator has already produced all its values,\n",
    "##### calling next() again will raise a StopIteration exception, indicating that the generator has\n",
    "##### been exhausted. use next(generator, default) to provide a default value, avoiding the occurrence\n",
    "##### of an exception. \n",
    "\n",
    "print('next(a):', next(a,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8999326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Now next(a) = None so the code will not print anything \n",
    "for i in a:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c06edf5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "##### using a new generator\n",
    "a = first_n(10)\n",
    "for i in a:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb003e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'StopIteration'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### The next() will raise StopIteration Exception\n",
    "##### since all items are iterated in the max()\n",
    "a = first_n(10)\n",
    "print(max(a))\n",
    "next(a,'StopIteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f8d9024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'StopIteration'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### The next() will raise StopIteration Exception\n",
    "##### since all items are iterated in the list()\n",
    "a = first_n(10)\n",
    "print(list(a))\n",
    "next(a,'StopIteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd8def66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'StopIteration'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### The next() will raise StopIteration Exception\n",
    "##### since all items are iterated in the sorted()\n",
    "a = first_n(10)\n",
    "print(sorted(a))\n",
    "next(a,'StopIteration')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
