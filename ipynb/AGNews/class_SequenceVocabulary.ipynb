{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddef8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2f0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"news_with_splits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e82b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data:  (120000, 3)\n",
      "------------------------------------------------------------\n",
      "   category  split                                 title\n",
      "0  Business  train    Jobs, tax cuts key issues for Bush\n",
      "1  Business  train  Jarden Buying Mr. Coffee #39;s Maker\n",
      "2  Business  train     Retail sales show festive fervour\n",
      "3  Business  train   Intervoice's Customers Come Calling\n",
      "4  Business  train     Boeing Expects Air Force Contract\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the data: \", df_all.shape)\n",
    "print('-'*60)\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153524e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Vocabulary class\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            ### add a new element to _token_to_idx\n",
    "            self._token_to_idx[token] = index\n",
    "            ### add a new element to _idx_to_token\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "   \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804fc58",
   "metadata": {},
   "source": [
    "# 1. SequenceVocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd1f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, \n",
    "                 token_to_idx    = None, \n",
    "                 unk_token       = \"<UNK>\",\n",
    "                 mask_token      = \"<MASK>\", \n",
    "                 begin_seq_token = \"<BEGIN>\",\n",
    "                 end_seq_token   = \"<END>\"):\n",
    "        \n",
    "        \n",
    "        super().__init__(token_to_idx)\n",
    "        \"\"\"\n",
    "        The follow attributes have been defined in the Vocabulary class:\n",
    "            - ._token_to_idx\n",
    "            - ._idx_to_token\n",
    "        \"\"\"\n",
    "\n",
    "        self._mask_token      = mask_token      # default: \"<MASK>\"\n",
    "        self._unk_token       = unk_token       # default: \"<UNK>\"\n",
    "        self._begin_seq_token = begin_seq_token # default: \"<BEGIN>\"\n",
    "        self._end_seq_token   = end_seq_token   # default: \"<END>\"\n",
    "\n",
    "        self.mask_index       = self.add_token(self._mask_token)      # return 0\n",
    "        self.unk_index        = self.add_token(self._unk_token)       # return 1\n",
    "        self.begin_seq_index  = self.add_token(self._begin_seq_token) # return 2\n",
    "        self.end_seq_index    = self.add_token(self._end_seq_token)   # return 3\n",
    "        \n",
    "    \n",
    "    ### Overriding the self.lookup_token() method\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba2ede",
   "metadata": {},
   "source": [
    "# 2. Instantiate the SequenceVocabulary from the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40957aa",
   "metadata": {},
   "source": [
    "## (1) The SequenceVocabulary for the titles - title_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af11a3",
   "metadata": {},
   "source": [
    "### There is one additional step for creating the vocabulary for titles - couting the tokens appeared in the titles, and add frequent tokens that apprear more than a pre-specified number to title_vocab, while treat infrequent tokens as UNK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42de8ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        Jobs, tax cuts key issues for Bush\n",
       "1                      Jarden Buying Mr. Coffee #39;s Maker\n",
       "2                         Retail sales show festive fervour\n",
       "3                       Intervoice's Customers Come Calling\n",
       "4                         Boeing Expects Air Force Contract\n",
       "                                ...                        \n",
       "119995            Genesis Space Capsule Crashes Into Desert\n",
       "119996             U.S.: Too Early to Tell Iraq Unit's Fate\n",
       "119997                   AFGHAN OPIUM GROWING UP TWO THIRDS\n",
       "119998    At least one Saudi policeman killed in clashes...\n",
       "119999                   U.S. Forces Claim Most of Fallujah\n",
       "Name: title, Length: 120000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b45019",
   "metadata": {},
   "source": [
    "### Initializing title_vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b6a7413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3},\n",
       " '_idx_to_token': {0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_vocab = SequenceVocabulary()\n",
    "vars(title_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb3e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts=Counter()\n",
    "for title in df_all.title:\n",
    "    for word in title.split(\" \"):\n",
    "        if word not in string.punctuation:\n",
    "            word_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9f2bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 most frequent words (out of 71747 in total)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 22691),\n",
       " ('in', 16690),\n",
       " ('for', 11625),\n",
       " ('on', 8915),\n",
       " ('of', 8716),\n",
       " ('(AP)', 7692),\n",
       " ('#39;s', 6048),\n",
       " ('the', 4950),\n",
       " ('(Reuters)', 4231),\n",
       " ('a', 3728),\n",
       " ('US', 3702),\n",
       " ('at', 3653),\n",
       " ('#39;', 3155),\n",
       " ('with', 3034),\n",
       " ('as', 3006),\n",
       " ('and', 2878),\n",
       " ('New', 2566),\n",
       " ('&lt;b&gt;...&lt;/b&gt;', 2559),\n",
       " ('Microsoft', 2100),\n",
       " ('Iraq', 2057)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"The 20 most frequent words (out of {len(word_counts)} in total)\")\n",
    "word_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f054a",
   "metadata": {},
   "source": [
    "### Only the tokens with more than 100 (a pre-specified number) counts will be added to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94267f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When cut_off = 100, 1288 tokesn added into title_vocab.\n"
     ]
    }
   ],
   "source": [
    "title_vocab = SequenceVocabulary()\n",
    "cut_off = 100\n",
    "for word, count in word_counts.items():\n",
    "    if count > cut_off:\n",
    "        title_vocab.add_token(word)\n",
    "print(f\"When cut_off = {cut_off}, {len(title_vocab._token_to_idx)} tokesn added into title_vocab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f54c1",
   "metadata": {},
   "source": [
    "### If cut_off = 1000, there are fewer tokens added to the title_vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3458b4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When cut_off = 1000, 45 tokesn added into title_vocab.\n"
     ]
    }
   ],
   "source": [
    "title_vocab = SequenceVocabulary()\n",
    "cut_off = 1000\n",
    "for word, count in word_counts.items():\n",
    "    if count > cut_off:\n",
    "        title_vocab.add_token(word)\n",
    "print(f\"When cut_off = {cut_off}, {len(title_vocab._token_to_idx)} tokesn added into title_vocab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaadc1ca",
   "metadata": {},
   "source": [
    "### If cut_off = 10000, only a few tokens are added to the title_vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04c8b9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When cut_off = 10000, 7 tokesn added into title_vocab.\n"
     ]
    }
   ],
   "source": [
    "title_vocab = SequenceVocabulary()\n",
    "cut_off = 10000\n",
    "for word, count in word_counts.items():\n",
    "    if count > cut_off:\n",
    "        title_vocab.add_token(word)\n",
    "print(f\"When cut_off = {cut_off}, {len(title_vocab._token_to_idx)} tokesn added into title_vocab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15808d",
   "metadata": {},
   "source": [
    "### In the following, use cut_off = 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cec7835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When cut_off = 50, 2642 tokesn added into title_vocab.\n"
     ]
    }
   ],
   "source": [
    "title_vocab = SequenceVocabulary()\n",
    "cut_off = 50\n",
    "for word, count in word_counts.items():\n",
    "    if count > cut_off:\n",
    "        title_vocab.add_token(word)\n",
    "print(f\"When cut_off = {cut_off}, {len(title_vocab._token_to_idx)} tokesn added into title_vocab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93527692",
   "metadata": {},
   "source": [
    "# 3. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0537983",
   "metadata": {},
   "source": [
    "### ._token_to_idx: a mapping of index and token added to the SequenceVocabulary (inherited from Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39830ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print out 20 tokens in the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<MASK>', 0),\n",
       " ('<UNK>', 1),\n",
       " ('<BEGIN>', 2),\n",
       " ('<END>', 3),\n",
       " ('tax', 4),\n",
       " ('cuts', 5),\n",
       " ('key', 6),\n",
       " ('issues', 7),\n",
       " ('for', 8),\n",
       " ('Bush', 9),\n",
       " ('Buying', 10),\n",
       " ('#39;s', 11),\n",
       " ('Maker', 12),\n",
       " ('Retail', 13),\n",
       " ('sales', 14),\n",
       " ('show', 15),\n",
       " ('Customers', 16),\n",
       " ('Boeing', 17),\n",
       " ('Expects', 18),\n",
       " ('Air', 19)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Print out 20 tokens in the vocabulary\")\n",
    "list(title_vocab._token_to_idx.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7417ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print a few elements in title_vocab._token_to_idx\n",
      "The index for \"place\" is 1455\n",
      "The index for \"and\" is 444\n",
      "The index for \"follow\" is 0\n",
      "The index for \"good\" is 198\n"
     ]
    }
   ],
   "source": [
    "tokens  = ['place','and','follow','good']\n",
    "mapping = title_vocab._token_to_idx\n",
    "print(\"Print a few elements in title_vocab._token_to_idx\")\n",
    "for i in tokens:\n",
    "    print(f'The index for \"{i}\" is {mapping.get(i,0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ad486",
   "metadata": {},
   "source": [
    "### ._idx_to_token: a mapping of index and token added to the SequenceVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64b0569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print out 20 tokens in the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, '<MASK>'),\n",
       " (1, '<UNK>'),\n",
       " (2, '<BEGIN>'),\n",
       " (3, '<END>'),\n",
       " (4, 'tax'),\n",
       " (5, 'cuts'),\n",
       " (6, 'key'),\n",
       " (7, 'issues'),\n",
       " (8, 'for'),\n",
       " (9, 'Bush'),\n",
       " (10, 'Buying'),\n",
       " (11, '#39;s'),\n",
       " (12, 'Maker'),\n",
       " (13, 'Retail'),\n",
       " (14, 'sales'),\n",
       " (15, 'show'),\n",
       " (16, 'Customers'),\n",
       " (17, 'Boeing'),\n",
       " (18, 'Expects'),\n",
       " (19, 'Air')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Print out 20 tokens in the vocabulary\")\n",
    "list(title_vocab._idx_to_token.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c97b0252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print a few elements in title_vocab._idx_to_token\n",
      "The token for index=0 is <MASK>\n",
      "The token for index=2 is <BEGIN>\n",
      "The token for index=6 is key\n",
      "The token for index=100 is business\n"
     ]
    }
   ],
   "source": [
    "indices  = [0,2,6,100]\n",
    "mapping = title_vocab._idx_to_token\n",
    "print(\"Print a few elements in title_vocab._idx_to_token\")\n",
    "for i in indices:\n",
    "    print(f'The token for index={i} is {mapping.get(i,0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66cef5",
   "metadata": {},
   "source": [
    "# 4. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cdc72",
   "metadata": {},
   "source": [
    "### add_token(token): Update mapping dicts based on the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40e60fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3},\n",
       " '_idx_to_token': {0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_vocab = SequenceVocabulary()\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "176db67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add one token apple\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0,\n",
       "  '<UNK>': 1,\n",
       "  '<BEGIN>': 2,\n",
       "  '<END>': 3,\n",
       "  'apple': 4},\n",
       " '_idx_to_token': {0: '<MASK>',\n",
       "  1: '<UNK>',\n",
       "  2: '<BEGIN>',\n",
       "  3: '<END>',\n",
       "  4: 'apple'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token = 'apple'\n",
    "example_vocab.add_token(new_token)\n",
    "print(f\"Add one token {new_token}\")\n",
    "print('-'*60)\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1ce9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add one token banana\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0,\n",
       "  '<UNK>': 1,\n",
       "  '<BEGIN>': 2,\n",
       "  '<END>': 3,\n",
       "  'apple': 4,\n",
       "  'banana': 5},\n",
       " '_idx_to_token': {0: '<MASK>',\n",
       "  1: '<UNK>',\n",
       "  2: '<BEGIN>',\n",
       "  3: '<END>',\n",
       "  4: 'apple',\n",
       "  5: 'banana'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token = 'banana'\n",
    "example_vocab.add_token(new_token)\n",
    "print(f\"Add one token {new_token}\")\n",
    "print('-'*60)\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec5871",
   "metadata": {},
   "source": [
    "### lookup_token(token): Retrieve the index associated with the token or the UNK index if token isn't present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1b5cb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3},\n",
       " '_idx_to_token': {0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_vocab = SequenceVocabulary()\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fe5e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple added\n",
      "banana added\n",
      "peach added\n",
      "orange added\n",
      "coconut added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0,\n",
       "  '<UNK>': 1,\n",
       "  '<BEGIN>': 2,\n",
       "  '<END>': 3,\n",
       "  'apple': 4,\n",
       "  'banana': 5,\n",
       "  'peach': 6,\n",
       "  'orange': 7,\n",
       "  'coconut': 8},\n",
       " '_idx_to_token': {0: '<MASK>',\n",
       "  1: '<UNK>',\n",
       "  2: '<BEGIN>',\n",
       "  3: '<END>',\n",
       "  4: 'apple',\n",
       "  5: 'banana',\n",
       "  6: 'peach',\n",
       "  7: 'orange',\n",
       "  8: 'coconut'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_add = ['apple','banana','peach','orange','coconut']\n",
    "for i in tokens_to_add:\n",
    "    example_vocab.add_token(i)\n",
    "    print(i + ' added')\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7634d002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index for orange is 7\n",
      "The index for rice is 1\n"
     ]
    }
   ],
   "source": [
    "tokens_list = ['orange','rice']\n",
    "for i in tokens_list:\n",
    "    print(f\"The index for {i} is {example_vocab.lookup_token(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d2166ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index for orange is 7\n",
      "The index for rice is 1\n"
     ]
    }
   ],
   "source": [
    "### Equivalent codes\n",
    "for i in tokens_list:\n",
    "    print(f\"The index for {i} is {SequenceVocabulary.lookup_token(example_vocab,i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5e2f7",
   "metadata": {},
   "source": [
    "### lookup_index(index): Return the token associated with the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc06f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token with index=1 is <UNK>\n",
      "The token with index=4 is apple\n"
     ]
    }
   ],
   "source": [
    "indices_list = [1,4]\n",
    "for i in indices_list:\n",
    "    print(f\"The token with index={i} is {example_vocab.lookup_index(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b8800d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token with index=1 is <UNK>\n",
      "The token with index=4 is apple\n"
     ]
    }
   ],
   "source": [
    "### Equivalent codes\n",
    "for i in indices_list:\n",
    "    print(f\"The token with index={i} is {Vocabulary.lookup_index(example_vocab,i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1a705",
   "metadata": {},
   "source": [
    "### \\_\\_len\\_\\_(): Return the length of _token_to_idx (i.e, the number of tokens in the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84349006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<MASK>',\n",
       " 1: '<UNK>',\n",
       " 2: '<BEGIN>',\n",
       " 3: '<END>',\n",
       " 4: 'token1',\n",
       " 5: 'token2',\n",
       " 6: 'token3',\n",
       " 7: 'token4'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_vocab = SequenceVocabulary()\n",
    "tokens_to_add = ['token1','token2','token3','token4']\n",
    "for i in tokens_to_add:\n",
    "    example_vocab.add_token(i)\n",
    "example_vocab._idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af5eee36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
