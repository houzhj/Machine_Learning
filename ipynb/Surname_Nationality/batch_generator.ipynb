{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f5f7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131f15e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data:  (10980, 4)\n",
      "------------------------------------------------------------\n",
      "  nationality  nationality_index  split   surname\n",
      "0      Arabic                 15  train     Totah\n",
      "1      Arabic                 15  train    Abboud\n",
      "2      Arabic                 15  train  Fakhoury\n",
      "3      Arabic                 15  train     Srour\n",
      "4      Arabic                 15  train    Sayegh\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv('surnames_with_splits.csv')\n",
    "print(\"shape of the data: \", df_all.shape)\n",
    "print('-'*60)\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efaabca",
   "metadata": {},
   "source": [
    "# Define two relevent classes\n",
    "### - Vocabulary ([see a walkthrough here](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/Surname_Nationality/class_Vocabulary.ipynb))\n",
    "### - SurnameVectorizer ([see a walkthrough here](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/Surname_Nationality/class_Vectorizer.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f2753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk   = add_unk\n",
    "        self._unk_token = unk_token      \n",
    "        self.unk_index  = -999\n",
    "        ### the unk_token, i.e, \"<UNK>\" is the first added token if add_unk=True\n",
    "        ### self.unk_index is changed from -999 to 0\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "   \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            ### .get(): return self.unk_index if the key \"token\" does not exist. \n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "    \n",
    "class SurnameVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_vocab (Vocabulary): maps characters to integers\n",
    "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
    "        \"\"\"\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "         \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): the surnames dataset\n",
    "        Returns:\n",
    "            an instance of the SurnameVectorizer\n",
    "        \"\"\"\n",
    "        surname_vocab     = Vocabulary(add_unk=True, unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        ########## Add tokens to surname_vocab and nationality_vocab\n",
    "        for index, row in surname_df.iterrows():\n",
    "            # Add tokens(characters) to surname_vocab\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            # Add tokens(words) to nationality_vocab\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    ### This is the key functionality of the Vectorizer.\n",
    "    ### It takes as an argument a string representing a surname,\n",
    "    ### and returns a vectorized representation of the surname.\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\"\n",
    "        Create a collapsed one-hot representation vector for the surname\n",
    "        Limitations of the one-hot method:\n",
    "        1 - Sparseness, n_unique_characters in a surname << n_unique_characters in a vocabulary\n",
    "        2 - Discarding the order of the words' appearance\n",
    "        \n",
    "        Args:\n",
    "            surname (str): the surname \n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed one-hot encoding \n",
    "        \"\"\"\n",
    "        ### Create an array where each element corresponds to each character in the vocabulary\n",
    "        one_hot = np.zeros(len(self.surname_vocab), dtype=np.float32)\n",
    "        ### Run lookup_token() for each character in the surname sequentially, return an index\n",
    "        ### Assign the corresponding element in the array to 1.\n",
    "        for token in surname:\n",
    "            one_hot[self.surname_vocab.lookup_token(token)] = 1\n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d61628",
   "metadata": {},
   "source": [
    "# 1. SurnameDataset class\n",
    "### - The Dataset class will characterize the key features of the dataset.\n",
    "### - In the initialization function of the class, make the class inherit the properties of torch.utils.data.Dataset so that we can later leverage its functionalities.\n",
    "### - In the \\_\\_init\\_\\_() function and the set_split() function, store important information such as labels and the features that we wish to generate at each pass.\n",
    "### - Each call requests a sample index for which the upperbound is specified in the \\_\\_len\\_\\_() method.\n",
    "### - When the sample corresponding to a given index is called, the generator executes the \\_\\_getitem\\_\\_() method to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336ed30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self,surname_df,vectorizer):\n",
    "        self.surname_df  = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df    = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size  = len(self.train_df)\n",
    "\n",
    "        self.val_df      = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df     = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size   = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val'  : (self.val_df, self.validation_size),\n",
    "                             'test' : (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # Class weights\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "    @classmethod\n",
    "    def load_csv_and_make_vectorizer(cls,surname_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        ### make vectorizer using training dataset\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        new_vectorizer  = SurnameVectorizer.from_dataframe(train_surname_df)\n",
    "        return cls(surname_df,new_vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_df_and_make_vectorizer(cls,surname_df):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "            surname_df: dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        ### make vectorizer using training dataset\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        new_vectorizer  = SurnameVectorizer.from_dataframe(train_surname_df)\n",
    "        return cls(surname_df,new_vectorizer)\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        ### when split = 'train', _target_df means the training set\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        ### _target_size is defined in set_split() \n",
    "        return self._target_size        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        \n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        surname_vector = \\\n",
    "            self._vectorizer.vectorize(row.surname)\n",
    "\n",
    "        nationality_index = \\\n",
    "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "        return {'x_data': surname_vector,\n",
    "                'y_target': nationality_index}\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eeb7c0",
   "metadata": {},
   "source": [
    "# 2. Instantiate a SurnameDataset from the training data\n",
    "### There are two classmethods can be used to instantiate a SurnameDataset: load_csv_and_make_vectorizer() and load_df_and_make_vectorizer(). The difference is whether the input data is from a csv file or a pd.DataFrame file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851359a",
   "metadata": {},
   "source": [
    "### First draw a (static, fixed random seed) from the entire datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6784708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_all.sample(100,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "064517be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5362</th>\n",
       "      <td>English</td>\n",
       "      <td>12</td>\n",
       "      <td>test</td>\n",
       "      <td>Hepples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sarkis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10686</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>Garza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8902</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Zhurikhin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>val</td>\n",
       "      <td>Aswad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index  split    surname\n",
       "5362      English                 12   test    Hepples\n",
       "900        Arabic                 15  train     Sarkis\n",
       "10686     Spanish                  6  train      Garza\n",
       "8902      Russian                 13  train  Zhurikhin\n",
       "1296       Arabic                 15    val      Aswad"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef53e322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>split</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nationality</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Czech</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Irish</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "split        test  train  val\n",
       "nationality                  \n",
       "Arabic          3     12    2\n",
       "Chinese         1      2    0\n",
       "Czech           1      2    0\n",
       "English         4     18    1\n",
       "French          0      2    0\n",
       "German          1      4    1\n",
       "Irish           0      3    0\n",
       "Italian         2      5    1\n",
       "Japanese        2      4    3\n",
       "Korean          0      1    0\n",
       "Polish          0      2    0\n",
       "Russian         5     12    2\n",
       "Spanish         1      3    0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_sample['nationality'], df_sample['split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c98b6",
   "metadata": {},
   "source": [
    "### Create a SurnameDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c111ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = SurnameDataset.load_df_and_make_vectorizer(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0225ae",
   "metadata": {},
   "source": [
    "## 2.1 - Attributes of a SurnameDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc92dc7",
   "metadata": {},
   "source": [
    "### .review_df: the input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ecf3088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5362</th>\n",
       "      <td>English</td>\n",
       "      <td>12</td>\n",
       "      <td>test</td>\n",
       "      <td>Hepples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sarkis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10686</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>Garza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8902</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Zhurikhin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>val</td>\n",
       "      <td>Aswad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>Czech</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>Oborny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6192</th>\n",
       "      <td>German</td>\n",
       "      <td>9</td>\n",
       "      <td>test</td>\n",
       "      <td>Bader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6784</th>\n",
       "      <td>Italian</td>\n",
       "      <td>17</td>\n",
       "      <td>train</td>\n",
       "      <td>De</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9666</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Harash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10722</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>Ybarra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index  split    surname\n",
       "5362      English                 12   test    Hepples\n",
       "900        Arabic                 15  train     Sarkis\n",
       "10686     Spanish                  6  train      Garza\n",
       "8902      Russian                 13  train  Zhurikhin\n",
       "1296       Arabic                 15    val      Aswad\n",
       "...           ...                ...    ...        ...\n",
       "2083        Czech                  5  train     Oborny\n",
       "6192       German                  9   test      Bader\n",
       "6784      Italian                 17  train         De\n",
       "9666      Russian                 13  train     Harash\n",
       "10722     Spanish                  6  train     Ybarra\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.surname_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "177ca416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.surname_df.equals(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7ee85",
   "metadata": {},
   "source": [
    "### ._vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3387bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note that the vectorizer is derived from the training split. \n",
    "v = dataset_sample._vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea070b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Smith\"\n",
    "one_hot      = v.vectorize(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34c23dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Vocabulary: the words appear >50 times\n",
      "------------------------------------------------------------\n",
      "_idx_to_token:  {0: '@', 1: 'S', 2: 'a', 3: 'r', 4: 'k', 5: 'i', 6: 's', 7: 'G', 8: 'z', 9: 'Z', 10: 'h', 11: 'u', 12: 'n', 13: 'A', 14: 'w', 15: 'e', 16: 't', 17: 'c', 18: 'H', 19: 'v', 20: 'o', 21: 'g', 22: 'b', 23: 'y', 24: 'Y', 25: 'm', 26: 'V', 27: 'l', 28: 'D', 29: 'N', 30: 'T', 31: 'C', 32: 'd', 33: 'J', 34: 'p', 35: 'K', 36: 'M', 37: 'L', 38: 'j', 39: 'W', 40: 'B', 41: 'P', 42: 'q', 43: 'F', 44: 'è', 45: 'f', 46: 'R', 47: 'O'}\n",
      "------------------------------------------------------------\n",
      "One-hot representation: [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f'Review Vocabulary: the words appear >50 times')\n",
    "print('-'*60)\n",
    "print(\"_idx_to_token: \", v.surname_vocab._idx_to_token)\n",
    "print('-'*60)\n",
    "print('One-hot representation:', one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f76a9b",
   "metadata": {},
   "source": [
    "### ._target_df, _target_size\n",
    "**Defined by method set_split()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c526883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sarkis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10686</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>Garza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8902</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Zhurikhin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9241</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Aweritchkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8707</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Hatskevich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>English</td>\n",
       "      <td>12</td>\n",
       "      <td>train</td>\n",
       "      <td>Rogerson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>Czech</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>Oborny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6784</th>\n",
       "      <td>Italian</td>\n",
       "      <td>17</td>\n",
       "      <td>train</td>\n",
       "      <td>De</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9666</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Harash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10722</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>Ybarra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index  split      surname\n",
       "900        Arabic                 15  train       Sarkis\n",
       "10686     Spanish                  6  train        Garza\n",
       "8902      Russian                 13  train    Zhurikhin\n",
       "9241      Russian                 13  train  Aweritchkin\n",
       "8707      Russian                 13  train   Hatskevich\n",
       "...           ...                ...    ...          ...\n",
       "2662      English                 12  train     Rogerson\n",
       "2083        Czech                  5  train       Oborny\n",
       "6784      Italian                 17  train           De\n",
       "9666      Russian                 13  train       Harash\n",
       "10722     Spanish                  6  train       Ybarra\n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample._target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30bb66b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample._target_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952364c",
   "metadata": {},
   "source": [
    "### ._lookup_dict - will be used in the method set_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec3860e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (      nationality  nationality_index  split      surname\n",
       "  900        Arabic                 15  train       Sarkis\n",
       "  10686     Spanish                  6  train        Garza\n",
       "  8902      Russian                 13  train    Zhurikhin\n",
       "  9241      Russian                 13  train  Aweritchkin\n",
       "  8707      Russian                 13  train   Hatskevich\n",
       "  ...           ...                ...    ...          ...\n",
       "  2662      English                 12  train     Rogerson\n",
       "  2083        Czech                  5  train       Oborny\n",
       "  6784      Italian                 17  train           De\n",
       "  9666      Russian                 13  train       Harash\n",
       "  10722     Spanish                  6  train       Ybarra\n",
       "  \n",
       "  [70 rows x 4 columns],\n",
       "  70),\n",
       " 'val': (      nationality  nationality_index split    surname\n",
       "  1296       Arabic                 15   val      Aswad\n",
       "  9991      Russian                 13   val  Lebedenko\n",
       "  4807      English                 12   val      Vinny\n",
       "  7831     Japanese                  7   val  Toyoshima\n",
       "  7037      Italian                 17   val    Amadori\n",
       "  6105       German                  9   val      Brose\n",
       "  7819     Japanese                  7   val      Okada\n",
       "  7786     Japanese                  7   val  Funakoshi\n",
       "  10178     Russian                 13   val  Gudarenko\n",
       "  1283       Arabic                 15   val       Seif,\n",
       "  10),\n",
       " 'test': (      nationality  nationality_index split      surname\n",
       "  5362      English                 12  test      Hepples\n",
       "  10450     Russian                 13  test       Yakhno\n",
       "  4998      English                 12  test        Eagle\n",
       "  10894     Spanish                  6  test     Zapatero\n",
       "  10514     Russian                 13  test   Anoprienko\n",
       "  1528       Arabic                 15  test        Wasem\n",
       "  5073      English                 12  test        Antic\n",
       "  10527     Russian                 13  test        Bakov\n",
       "  1793      Chinese                  3  test          Chi\n",
       "  7925     Japanese                  7  test        Kamon\n",
       "  7170      Italian                 17  test      Negrini\n",
       "  5042      English                 12  test     Conneely\n",
       "  1401       Arabic                 15  test       Sayegh\n",
       "  1402       Arabic                 15  test        Daher\n",
       "  2188        Czech                  5  test       Hradek\n",
       "  7152      Italian                 17  test    Napoleoni\n",
       "  10524     Russian                 13  test    Babakulov\n",
       "  7939     Japanese                  7  test         Hama\n",
       "  10458     Russian                 13  test  Kalistratov\n",
       "  6192       German                  9  test        Bader,\n",
       "  20)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample._lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57c0d32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      nationality  nationality_index  split      surname\n",
       " 900        Arabic                 15  train       Sarkis\n",
       " 10686     Spanish                  6  train        Garza\n",
       " 8902      Russian                 13  train    Zhurikhin\n",
       " 9241      Russian                 13  train  Aweritchkin\n",
       " 8707      Russian                 13  train   Hatskevich\n",
       " ...           ...                ...    ...          ...\n",
       " 2662      English                 12  train     Rogerson\n",
       " 2083        Czech                  5  train       Oborny\n",
       " 6784      Italian                 17  train           De\n",
       " 9666      Russian                 13  train       Harash\n",
       " 10722     Spanish                  6  train       Ybarra\n",
       " \n",
       " [70 rows x 4 columns],\n",
       " 70)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### A dictionary which contains a df and a scalar\n",
    "dataset_sample._lookup_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fdc9532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sarkis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10686</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>Garza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8902</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Zhurikhin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9241</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Aweritchkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8707</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Hatskevich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>English</td>\n",
       "      <td>12</td>\n",
       "      <td>train</td>\n",
       "      <td>Rogerson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>Czech</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>Oborny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6784</th>\n",
       "      <td>Italian</td>\n",
       "      <td>17</td>\n",
       "      <td>train</td>\n",
       "      <td>De</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9666</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Harash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10722</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>Ybarra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index  split      surname\n",
       "900        Arabic                 15  train       Sarkis\n",
       "10686     Spanish                  6  train        Garza\n",
       "8902      Russian                 13  train    Zhurikhin\n",
       "9241      Russian                 13  train  Aweritchkin\n",
       "8707      Russian                 13  train   Hatskevich\n",
       "...           ...                ...    ...          ...\n",
       "2662      English                 12  train     Rogerson\n",
       "2083        Czech                  5  train       Oborny\n",
       "6784      Italian                 17  train           De\n",
       "9666      Russian                 13  train       Harash\n",
       "10722     Spanish                  6  train       Ybarra\n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### the dataframe\n",
    "dataset_sample._lookup_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ea4c33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### the sample size\n",
    "dataset_sample._lookup_dict['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b974f",
   "metadata": {},
   "source": [
    "## 2.2 - Methods of a ReviewDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014c440",
   "metadata": {},
   "source": [
    "### \\_\\_len()\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2a9ee4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd9617",
   "metadata": {},
   "source": [
    "### \\_\\_getitem()\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c81d7520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " 'y_target': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The 4th element in the \"train\" split\n",
    "### In the __init__ function, self.set_split('train') defines ._target_df\n",
    "dataset_sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3f8e011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5362</th>\n",
       "      <td>English</td>\n",
       "      <td>12</td>\n",
       "      <td>test</td>\n",
       "      <td>Hepples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sarkis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10686</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>Garza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8902</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Zhurikhin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>val</td>\n",
       "      <td>Aswad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index  split    surname\n",
       "5362      English                 12   test    Hepples\n",
       "900        Arabic                 15  train     Sarkis\n",
       "10686     Spanish                  6  train      Garza\n",
       "8902      Russian                 13  train  Zhurikhin\n",
       "1296       Arabic                 15    val      Aswad"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b3eb72",
   "metadata": {},
   "source": [
    "### set_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd13cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = SurnameDataset.load_df_and_make_vectorizer(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f0c92fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sarkis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10686</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>Garza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8902</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Zhurikhin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9241</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Aweritchkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8707</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Hatskevich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>English</td>\n",
       "      <td>12</td>\n",
       "      <td>train</td>\n",
       "      <td>Rogerson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>Czech</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>Oborny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6784</th>\n",
       "      <td>Italian</td>\n",
       "      <td>17</td>\n",
       "      <td>train</td>\n",
       "      <td>De</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9666</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Harash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10722</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>Ybarra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index  split      surname\n",
       "900        Arabic                 15  train       Sarkis\n",
       "10686     Spanish                  6  train        Garza\n",
       "8902      Russian                 13  train    Zhurikhin\n",
       "9241      Russian                 13  train  Aweritchkin\n",
       "8707      Russian                 13  train   Hatskevich\n",
       "...           ...                ...    ...          ...\n",
       "2662      English                 12  train     Rogerson\n",
       "2083        Czech                  5  train       Oborny\n",
       "6784      Italian                 17  train           De\n",
       "9666      Russian                 13  train       Harash\n",
       "10722     Spanish                  6  train       Ybarra\n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now the split for ._target_df and _target_size is 'train'\n",
    "dataset_sample._target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27626144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14848e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " 'y_target': 2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a931c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "### run set_split, switch the split to 'val'\n",
    "dataset_sample.set_split('val')\n",
    "# or \n",
    "# ReviewDataset.set_split(dataset_sample,'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f847e938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>val</td>\n",
       "      <td>Aswad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>val</td>\n",
       "      <td>Lebedenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4807</th>\n",
       "      <td>English</td>\n",
       "      <td>12</td>\n",
       "      <td>val</td>\n",
       "      <td>Vinny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7831</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>7</td>\n",
       "      <td>val</td>\n",
       "      <td>Toyoshima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7037</th>\n",
       "      <td>Italian</td>\n",
       "      <td>17</td>\n",
       "      <td>val</td>\n",
       "      <td>Amadori</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6105</th>\n",
       "      <td>German</td>\n",
       "      <td>9</td>\n",
       "      <td>val</td>\n",
       "      <td>Brose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7819</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>7</td>\n",
       "      <td>val</td>\n",
       "      <td>Okada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7786</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>7</td>\n",
       "      <td>val</td>\n",
       "      <td>Funakoshi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10178</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>val</td>\n",
       "      <td>Gudarenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>val</td>\n",
       "      <td>Seif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index split    surname\n",
       "1296       Arabic                 15   val      Aswad\n",
       "9991      Russian                 13   val  Lebedenko\n",
       "4807      English                 12   val      Vinny\n",
       "7831     Japanese                  7   val  Toyoshima\n",
       "7037      Italian                 17   val    Amadori\n",
       "6105       German                  9   val      Brose\n",
       "7819     Japanese                  7   val      Okada\n",
       "7786     Japanese                  7   val  Funakoshi\n",
       "10178     Russian                 13   val  Gudarenko\n",
       "1283       Arabic                 15   val       Seif"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now the split for ._target_df and _target_size is 'val'\n",
    "dataset_sample._target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fef7f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04955d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " 'y_target': 4}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f2c3f",
   "metadata": {},
   "source": [
    "### get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7d3c839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SurnameVectorizer at 0x7fee28d0a520>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e427ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SurnameVectorizer at 0x7fee28d0a520>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Equivalently\n",
    "dataset_sample._vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750f609",
   "metadata": {},
   "source": [
    "### get_num_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f2dacf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.get_num_batches(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5bff7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample._target_df)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f88d7ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "278b79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Switch the split to 'train'\n",
    "dataset_sample.set_split('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d697ed6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.get_num_batches(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62c67788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample._target_df)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7464e8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_sample._target_df)//10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedca03f",
   "metadata": {},
   "source": [
    "# 3. Define a batch generator\n",
    "### - Wrap the DataLoader\n",
    "### - Switch the data between the CPU and the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "255860b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device='cpu'):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d4140",
   "metadata": {},
   "source": [
    "## 3.1 Dataset Class\n",
    "### - The Dataset class characterizes the key features of the dataset you want to generate.\n",
    "### - The class uses \\_\\_init\\_\\_(), \\_\\_len\\_\\_(), and \\_\\_getitem\\_\\_() to store important information, and generate samples. \n",
    "### - The Dataset class is an important argument of the DataLoader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cb9f686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: {'x1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'x2': [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], 'y': [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0]}\n",
      "------------------------------------------------------------\n",
      "df:     x1  x2  y\n",
      "0    1  13  0\n",
      "1    2  14  1\n",
      "2    3  15  0\n",
      "3    4  16  1\n",
      "4    5  17  1\n",
      "5    6  18  0\n",
      "6    7  19  0\n",
      "7    8  20  1\n",
      "8    9  21  1\n",
      "9   10  22  0\n",
      "10  11  23  1\n",
      "11  12  24  0\n"
     ]
    }
   ],
   "source": [
    "data = {'x1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "        'x2': [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
    "        'y': [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0]}\n",
    "data\n",
    "df = pd.DataFrame(data)\n",
    "print(\"data:\" ,data)\n",
    "print(\"-\"*60)\n",
    "print(\"df:\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffe2a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "[tensor([[ 1., 13.],\n",
      "        [ 2., 14.],\n",
      "        [ 3., 15.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "[tensor([[ 4., 16.],\n",
      "        [ 5., 17.],\n",
      "        [ 6., 18.]]), tensor([1., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "[tensor([[ 7., 19.],\n",
      "        [ 8., 20.],\n",
      "        [ 9., 21.]]), tensor([0., 1., 1.])]\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "[tensor([[10., 22.],\n",
      "        [11., 23.],\n",
      "        [12., 24.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##### Define Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = torch.tensor(self.data.iloc[index, :-1].values, dtype=torch.float32)\n",
    "        target = torch.tensor(self.data.iloc[index, -1], dtype=torch.float32)\n",
    "        return sample, target\n",
    "\n",
    "##### Instantiate the Dataset class\n",
    "custom_dataset = CustomDataset(df)\n",
    "\n",
    "##### Instantiate the DataLoader class\n",
    "batch_size  = 3\n",
    "data_loader = DataLoader(dataset=custom_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##### Obtain the batch\n",
    "i = 0\n",
    "for batch in data_loader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(batch)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c885a",
   "metadata": {},
   "source": [
    "### An alternative is to use TensorDataset() directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bbc1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3828382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n",
      "x2: tensor([13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24.])\n",
      "y: tensor([0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.from_numpy(df['x1'].values).float()\n",
    "x2 = torch.from_numpy(df['x2'].values).float()\n",
    "y  = torch.from_numpy(df['y'].values).float()\n",
    "print(\"x1:\", x1)\n",
    "print(\"x2:\", x2)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0989dcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 13.],\n",
       "        [ 2., 14.],\n",
       "        [ 3., 15.],\n",
       "        [ 4., 16.],\n",
       "        [ 5., 17.],\n",
       "        [ 6., 18.],\n",
       "        [ 7., 19.],\n",
       "        [ 8., 20.],\n",
       "        [ 9., 21.],\n",
       "        [10., 22.],\n",
       "        [11., 23.],\n",
       "        [12., 24.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.stack([x1, x2], dim=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb029abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "[tensor([[ 1., 13.],\n",
      "        [ 2., 14.],\n",
      "        [ 3., 15.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "[tensor([[ 4., 16.],\n",
      "        [ 5., 17.],\n",
      "        [ 6., 18.]]), tensor([1., 1., 0.])]\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "[tensor([[ 7., 19.],\n",
      "        [ 8., 20.],\n",
      "        [ 9., 21.]]), tensor([0., 1., 1.])]\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "[tensor([[10., 22.],\n",
      "        [11., 23.],\n",
      "        [12., 24.]]), tensor([0., 1., 0.])]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##### Create Tensor dataset\n",
    "dataset     = TensorDataset(features, y)\n",
    "batch_size  = 3\n",
    "\n",
    "##### Instantiate the DataLoader class\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "##### Obtain the batch\n",
    "i = 0\n",
    "for batch in data_loader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(batch)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace0dd4",
   "metadata": {},
   "source": [
    "### The two methods below are equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d431af5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 13.],\n",
       "        [ 2., 14.],\n",
       "        [ 3., 15.],\n",
       "        [ 4., 16.],\n",
       "        [ 5., 17.],\n",
       "        [ 6., 18.],\n",
       "        [ 7., 19.],\n",
       "        [ 8., 20.],\n",
       "        [ 9., 21.],\n",
       "        [10., 22.],\n",
       "        [11., 23.],\n",
       "        [12., 24.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.from_numpy(df['x1'].values).float()\n",
    "x2 = torch.from_numpy(df['x2'].values).float()\n",
    "torch.stack([x1, x2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "724eebb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 13],\n",
       "        [ 2, 14],\n",
       "        [ 3, 15],\n",
       "        [ 4, 16],\n",
       "        [ 5, 17],\n",
       "        [ 6, 18],\n",
       "        [ 7, 19],\n",
       "        [ 8, 20],\n",
       "        [ 9, 21],\n",
       "        [10, 22],\n",
       "        [11, 23],\n",
       "        [12, 24]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_array = df[['x1', 'x2']].to_numpy()\n",
    "torch.from_numpy(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d5cc0",
   "metadata": {},
   "source": [
    "## 3.2 DataLoader\n",
    "### - batch_size: denotes the number of samples contained in each generated batch.\n",
    "### - shuffle: if set to True, we will get a new order of exploration at each pass (or just keep a linear exploration scheme otherwise). Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.\n",
    "### - drop_last: set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47cf3dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_sample[0]['x_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2fda99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = SurnameDataset.load_df_and_make_vectorizer(df_sample)\n",
    "batch_size     = 10\n",
    "shuffle        = True\n",
    "drop_last      = True\n",
    "dataloader     = DataLoader(dataset=dataset_sample, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ac53c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x in one batch\n",
      "tensor([[0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "size of x_data: torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "y in one batch\n",
      "tensor([ 3,  8, 11,  3,  0,  0,  7,  5,  2, 10])\n",
      "size of y_data: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "one_batch = next(iter(dataloader))\n",
    "print('x in one batch')\n",
    "print(one_batch['x_data'])\n",
    "print('size of x_data:', one_batch['x_data'].shape)\n",
    "print('-' * 60)\n",
    "print('y in one batch')\n",
    "print(one_batch['y_target'])\n",
    "print('size of y_data:', one_batch['y_target'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7622b",
   "metadata": {},
   "source": [
    "### In this example, dataloader utilizes the return from the \\_\\_getitem\\_\\_() method, which extracts related rows from the _target_df of dataset, with _target_size=65. Also, batch_size=10, and drop_last=True so there are 6 batches created (the last 5 rows are dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37215098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in the target_df:  70\n",
      "number of rows in the target_df:  70\n",
      "The number of batches is: 7\n"
     ]
    }
   ],
   "source": [
    "print('number of rows in the target_df: ', len(dataset_sample._target_df))\n",
    "print('number of rows in the target_df: ', dataset_sample._target_size)\n",
    "print(\"The number of batches is:\",dataset_sample.get_num_batches(batch_size = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bcf58a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "{'x_data': tensor([[0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'y_target': tensor([ 3, 11,  3,  6,  1,  6,  3,  7,  3,  3])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "{'x_data': tensor([[0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]), 'y_target': tensor([ 2,  0,  9,  8,  0,  2,  3, 10,  2,  3])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "{'x_data': tensor([[0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]), 'y_target': tensor([ 2, 12,  4,  0,  2,  5,  0, 11,  2, 11])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "{'x_data': tensor([[0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'y_target': tensor([ 0,  7,  0,  3, 10,  3,  3, 12,  0,  2])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "Batch 4\n",
      "{'x_data': tensor([[0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'y_target': tensor([3, 2, 2, 6, 3, 3, 3, 3, 7, 0])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "Batch 5\n",
      "{'x_data': tensor([[0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'y_target': tensor([ 8,  2,  0, 11,  0,  2,  3,  3,  2,  1])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6\n",
      "{'x_data': tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'y_target': tensor([ 0,  4,  4,  3,  1,  6,  5,  4, 11,  0])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for data_dict in dataloader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(data_dict)\n",
    "    print(data_dict['x_data'].shape)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2433c26",
   "metadata": {},
   "source": [
    "### This is equvalent to defining and using the generator function generate_batches()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b396fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "{'x_data': tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'y_target': tensor([6, 2, 3, 1, 1, 0, 3, 8, 3, 2])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "Batch 1\n",
      "{'x_data': tensor([[0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'y_target': tensor([2, 3, 2, 2, 0, 3, 3, 3, 0, 5])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "{'x_data': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]), 'y_target': tensor([11,  0,  0,  8,  4,  6,  0,  3,  7, 11])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "{'x_data': tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'y_target': tensor([ 0,  4, 12,  7,  0, 11, 12,  2,  2,  0])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "Batch 4\n",
      "{'x_data': tensor([[0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'y_target': tensor([ 6,  3,  2,  6,  2,  2,  3,  7, 11,  2])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "Batch 5\n",
      "{'x_data': tensor([[0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'y_target': tensor([11,  4, 10,  3,  4,  3,  3,  9,  1,  3])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n",
      "Batch 6\n",
      "{'x_data': tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]), 'y_target': tensor([ 5,  2,  0,  0,  0,  3,  3, 10,  3,  3])}\n",
      "torch.Size([10, 48])\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for data_dict in dataloader:\n",
    "    print('Batch '+str(i))\n",
    "    i+=1\n",
    "    print(data_dict)\n",
    "    print(data_dict['x_data'].shape)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0743de",
   "metadata": {},
   "source": [
    "## 3.3 Generator\n",
    "### - Generator functions declare a function that behaves like an iterator, i.e. it can be used in a for loop.\n",
    "### - A generator function is defined just like a normal function, but whenever it needs to generate a value, it does so with the yield keyword rather than return. \n",
    "### - Yield is used in Python generators. If the body of a def contains yield, the function automatically becomes a generator function. \n",
    "### - *return* sends a specified value back to its caller whereas *yield* can produce a sequence of values. We should use *yield* when we want to iterate over a sequence, but don’t want to store the entire sequence in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a01ec",
   "metadata": {},
   "source": [
    "### Consider a task to calculate the sum of the first n integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c313379f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### The function below builds the full list in memory\n",
    "def first_n(n):\n",
    "    num, nums = 0, []\n",
    "    while num < n:\n",
    "        nums.append(num)\n",
    "        num += 1\n",
    "    return nums\n",
    "sum(first_n(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac676abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vars(a): {'n': 10, 'num': 0}\n",
      "sum(a): 45\n"
     ]
    }
   ],
   "source": [
    "##### The following implements generator as an iterable object.\n",
    "class first_n(object):\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.num = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    # Python 3 compatibility\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        if self.num < self.n:\n",
    "            cur, self.num = self.num, self.num+1\n",
    "            return cur\n",
    "        raise StopIteration\n",
    "        \n",
    "a = first_n(10)\n",
    "print('vars(a):', vars(a))\n",
    "print('sum(a):', sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55b34e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next(a): 0\n",
      "sum(a): 45\n",
      "next(a): None\n"
     ]
    }
   ],
   "source": [
    "##### a generator that yields items instead of returning a list\n",
    "\n",
    "def first_n(n):\n",
    "    num = 0\n",
    "    while num < n:\n",
    "        yield num\n",
    "        num += 1\n",
    "\n",
    "a = first_n(10)\n",
    "\n",
    "print('next(a):', next(a))\n",
    "print('sum(a):', sum(a))\n",
    "##### If the generator has already produced all its values, calling next() \n",
    "##### again will raise a StopIteration exception, indicating that the \n",
    "##### generator has been exhausted. use next(generator, default) to \n",
    "##### provide a default value, avoiding the occurrence of an exception.\n",
    "print('next(a):', next(a,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8999326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Now next(a) = None so the code will not print anything \n",
    "for i in a:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c06edf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "##### using a new generator\n",
    "for i in first_n(10):\n",
    "    print (i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
