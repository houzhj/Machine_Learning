{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9e6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d09f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"surnames_with_splits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52785cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data:  (10980, 4)\n",
      "------------------------------------------------------------\n",
      "     nationality  nationality_index  split     surname\n",
      "9297     Russian                 13  train     Iskakov\n",
      "1261      Arabic                 15    val      Dagher\n",
      "7711    Japanese                  7  train      Narato\n",
      "5767      German                  9  train       Reier\n",
      "5222     English                 12   test      Gately\n",
      "8798     Russian                 13  train  Jachmenkov\n",
      "1708     Chinese                  3  train         Wei\n",
      "2057       Czech                  5  train   Kucharova\n",
      "6550       Irish                  1    val       Flann\n",
      "2212       Czech                  5   test         Opp\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the data: \", df_all.shape)\n",
    "print('-'*60)\n",
    "print(df_all.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afde8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            ### add a new element to _token_to_idx\n",
    "            self._token_to_idx[token] = index\n",
    "            ### add a new element to _idx_to_token\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "   \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, \n",
    "                 token_to_idx    = None, \n",
    "                 unk_token       = \"<UNK>\",\n",
    "                 mask_token      = \"<MASK>\", \n",
    "                 begin_seq_token = \"<BEGIN>\",\n",
    "                 end_seq_token   = \"<END>\"):\n",
    "        \n",
    "        \n",
    "        super().__init__(token_to_idx)\n",
    "        \"\"\"\n",
    "        The follow attributes have been defined in the Vocabulary class:\n",
    "            - ._token_to_idx\n",
    "            - ._idx_to_token\n",
    "        \"\"\"\n",
    "\n",
    "        self._mask_token      = mask_token      # default: \"<MASK>\"\n",
    "        self._unk_token       = unk_token       # default: \"<UNK>\"\n",
    "        self._begin_seq_token = begin_seq_token # default: \"<BEGIN>\"\n",
    "        self._end_seq_token   = end_seq_token   # default: \"<END>\"\n",
    "\n",
    "        self.mask_index       = self.add_token(self._mask_token)      # return 0\n",
    "        self.unk_index        = self.add_token(self._unk_token)       # return 1\n",
    "        self.begin_seq_index  = self.add_token(self._begin_seq_token) # return 2\n",
    "        self.end_seq_index    = self.add_token(self._end_seq_token)   # return 3\n",
    "        \n",
    "    \n",
    "    ### Overriding the self.lookup_token() method\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbdb2b7",
   "metadata": {},
   "source": [
    "# 1. SurnameVectorizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2aadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_vocab (Vocabulary): maps characters to integers\n",
    "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
    "        \"\"\"\n",
    "        self.surname_vocab       = surname_vocab\n",
    "        self.nationality_vocab   = nationality_vocab\n",
    "         \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): the surnames dataset\n",
    "        Returns:\n",
    "            an instance of the SurnameVectorizer\n",
    "        \"\"\"\n",
    "        surname_vocab      = SequenceVocabulary()\n",
    "        nationality_vocab  = Vocabulary()\n",
    "        \n",
    "        ########## Add tokens to surname_vocab and nationality_vocab\n",
    "        for index, row in surname_df.iterrows():\n",
    "\n",
    "            # Add tokens(characters) to surname_vocab\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            # Add tokens(words) to nationality_vocab\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    def vectorize(self, surname, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname (str): the string of characters\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        \"\"\"\n",
    "        ### set the first index to be begin_seq_index=2 (defined in SequenceVocabulary)\n",
    "        indices = [self.surname_vocab.begin_seq_index]\n",
    "        \n",
    "        ### adding the indeces for the surname after the first index\n",
    "        indices.extend(self.surname_vocab.lookup_token(token) \n",
    "                       for token in surname)\n",
    "        \n",
    "        ### set the last index to be end_seq_index=3 (defined in SequenceVocabulary)\n",
    "        indices.append(self.surname_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices) - 1\n",
    "\n",
    "        ### from_vector, will be used as SurnameDataset.x_data\n",
    "        ### The slice of the indices that doesn’t include the last index is placed inside from_vector\n",
    "        from_vector = np.empty(vector_length, dtype=np.int64)         \n",
    "        from_indices = indices[:-1]\n",
    "        from_vector[:len(from_indices)] = from_indices\n",
    "        ### the sequences are filled (or padded) to the right with the mask_index\n",
    "        from_vector[len(from_indices):] = self.surname_vocab.mask_index\n",
    "        \n",
    "        ### to_vector, will be used as SurnameDataset.y_target\n",
    "        ### The slice of the indices that doesn’t include the first index is placed inside to_vector\n",
    "        to_vector = np.empty(vector_length, dtype=np.int64)\n",
    "        to_indices = indices[1:]\n",
    "        to_vector[:len(to_indices)] = to_indices\n",
    "        ### the sequences are filled (or padded) to the right with the mask_index\n",
    "        to_vector[len(to_indices):] = self.surname_vocab.mask_index\n",
    "        \n",
    "        return from_vector, to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268a8b6",
   "metadata": {},
   "source": [
    "# 2. Instantiate a SurnameVectorizer from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "667e8d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Totah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Abboud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Fakhoury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Srour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sayegh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nationality  nationality_index  split   surname\n",
       "0      Arabic                 15  train     Totah\n",
       "1      Arabic                 15  train    Abboud\n",
       "2      Arabic                 15  train  Fakhoury\n",
       "3      Arabic                 15  train     Srour\n",
       "4      Arabic                 15  train    Sayegh"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df_all.copy()\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d526fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instantiate a vectorizer\n",
    "vectorizer_sample = SurnameVectorizer.from_dataframe(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378100ce",
   "metadata": {},
   "source": [
    "### A vectorizer has two vocabularies(attributes), one for review, one for rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3621bd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'surname_vocab': <__main__.SequenceVocabulary at 0x7fccbcc2dac0>,\n",
       " 'nationality_vocab': <__main__.Vocabulary at 0x7fccbcc2dc40>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(vectorizer_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad80b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surname_vocab includes 88 tokens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "_token_to_idx:\n",
      "[('<MASK>', 0), ('<UNK>', 1), ('<BEGIN>', 2), ('<END>', 3), ('T', 4), ('o', 5), ('t', 6), ('a', 7), ('h', 8), ('A', 9), ('b', 10), ('u', 11), ('d', 12), ('F', 13), ('k', 14), ('r', 15), ('y', 16), ('S', 17), ('e', 18), ('g', 19), ('C', 20), ('m', 21), ('H', 22), ('i', 23), ('K', 24), ('n', 25), ('W', 26), ('s', 27), ('f', 28), ('G', 29), ('M', 30), ('l', 31), ('B', 32), ('z', 33), ('N', 34), ('I', 35), ('w', 36), ('D', 37), ('Q', 38), ('j', 39), ('E', 40), ('R', 41), ('Z', 42), ('c', 43), ('Y', 44), ('J', 45), ('L', 46), ('O', 47), ('-', 48), ('P', 49), ('X', 50), ('p', 51), (':', 52), ('v', 53), ('U', 54), ('1', 55), ('V', 56), ('x', 57), ('/', 58), ('q', 59), ('é', 60), ('É', 61), (\"'\", 62), ('ç', 63), ('ê', 64), ('ß', 65), ('ö', 66), ('ä', 67), ('ü', 68), ('ú', 69), ('à', 70), ('ò', 71), ('è', 72), ('ó', 73), ('ù', 74), ('ì', 75), ('Ś', 76), ('ą', 77), ('ń', 78), ('á', 79), ('ż', 80), ('Ż', 81), ('ł', 82), ('õ', 83), ('ã', 84), ('í', 85), ('ñ', 86), ('Á', 87)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"surname_vocab includes {len(vectorizer_sample.surname_vocab)} tokens\")\n",
    "print(\"-\"*100)\n",
    "print(\"_token_to_idx:\")\n",
    "print(list(vectorizer_sample.surname_vocab._token_to_idx.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2486c",
   "metadata": {},
   "source": [
    "# 3. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f78a7",
   "metadata": {},
   "source": [
    "### (classmethod) from_dataframe(surname_df): Instantiate the vectorizer from the dataset dataframe.\n",
    "1. First instantiate a Vocabulariy for nationalities and a SequenceVocabulary for surnames, based on the input data \"surnames_with_splits.csv\".\n",
    "2. Use the surname_vocab and nationality_vocab as the inputs to instantiate a vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b5097",
   "metadata": {},
   "source": [
    "### vectorize(review): It takes as an argument a string representing a surname, and returns a vectorized representation of the surname. This is the key functionality of the Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a1417b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surname_vocab: {0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>', 4: 'T', 5: 'o', 6: 't', 7: 'a', 8: 'h', 9: 'A', 10: 'b', 11: 'u', 12: 'd', 13: 'F', 14: 'k', 15: 'r', 16: 'y', 17: 'S', 18: 'e', 19: 'g', 20: 'C', 21: 'm', 22: 'H', 23: 'i', 24: 'K', 25: 'n', 26: 'W', 27: 's', 28: 'f', 29: 'G', 30: 'M', 31: 'l', 32: 'B', 33: 'z', 34: 'N', 35: 'I', 36: 'w', 37: 'D', 38: 'Q', 39: 'j', 40: 'E', 41: 'R', 42: 'Z', 43: 'c', 44: 'Y', 45: 'J', 46: 'L', 47: 'O', 48: '-', 49: 'P', 50: 'X', 51: 'p', 52: ':', 53: 'v', 54: 'U', 55: '1', 56: 'V', 57: 'x', 58: '/', 59: 'q', 60: 'é', 61: 'É', 62: \"'\", 63: 'ç', 64: 'ê', 65: 'ß', 66: 'ö', 67: 'ä', 68: 'ü', 69: 'ú', 70: 'à', 71: 'ò', 72: 'è', 73: 'ó', 74: 'ù', 75: 'ì', 76: 'Ś', 77: 'ą', 78: 'ń', 79: 'á', 80: 'ż', 81: 'Ż', 82: 'ł', 83: 'õ', 84: 'ã', 85: 'í', 86: 'ñ', 87: 'Á'}\n"
     ]
    }
   ],
   "source": [
    "##### Initializing SurnameVectorizer\n",
    "vectorizer     = SurnameVectorizer.from_dataframe(df_sample)\n",
    "print('Surname_vocab:',vectorizer.surname_vocab._idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edbea8ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example surname Nakamura\n",
      "----------------------------------------------------------------------------------------------------\n",
      "from_vector: [ 2 34  7 14  7 21 11 15  7]\n",
      "to_vector: [34  7 14  7 21 11 15  7  3]\n"
     ]
    }
   ],
   "source": [
    "example_surname = \"Nakamura\"\n",
    "example_vector = vectorizer.vectorize(example_surname)\n",
    "print('Example surname', example_surname)\n",
    "print('-'*100)\n",
    "print('from_vector:', example_vector[0])\n",
    "print('to_vector:', example_vector[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86cc68f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example surname Miller\n",
      "----------------------------------------------------------------------------------------------------\n",
      "from_vector: [ 2 30 23 31 31 18 15]\n",
      "to_vector: [30 23 31 31 18 15  3]\n"
     ]
    }
   ],
   "source": [
    "example_surname = \"Miller\"\n",
    "example_vector = vectorizer.vectorize(example_surname)\n",
    "print('Example surname', example_surname)\n",
    "print('-'*100)\n",
    "print('from_vector:', example_vector[0])\n",
    "print('to_vector:', example_vector[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dc79287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example surname Li\n",
      "----------------------------------------------------------------------------------------------------\n",
      "from_vector: [ 2 46 23]\n",
      "to_vector: [46 23  3]\n"
     ]
    }
   ],
   "source": [
    "example_surname = \"Li\"\n",
    "example_vector = vectorizer.vectorize(example_surname)\n",
    "print('Example surname', example_surname)\n",
    "print('-'*100)\n",
    "print('from_vector:', example_vector[0])\n",
    "print('to_vector:', example_vector[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ee41b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example surname Li\n",
      "----------------------------------------------------------------------------------------------------\n",
      "from_vector: [ 2 46 23  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "to_vector: [46 23  3  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "example_surname = \"Li\"\n",
    "example_vector = vectorizer.vectorize(example_surname,15)\n",
    "print('Example surname', example_surname)\n",
    "print('-'*100)\n",
    "print('from_vector:', example_vector[0])\n",
    "print('to_vector:', example_vector[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3498f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
