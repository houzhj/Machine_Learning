{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e05c1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "from argparse import Namespace\n",
    "import os\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0859fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 'TRAIN' a new model or 'LOAD' an existing model \n",
    "get_model = 'TRAIN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5318bea",
   "metadata": {},
   "source": [
    "# @@@@@ 0. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb61156",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Training or loading\n",
    "    get_model   = get_model,\n",
    "    \n",
    "    # Data and Path information\n",
    "    input_path  = os.getcwd(),\n",
    "    output_path = os.getcwd()+'/OUTPUT/',\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    learning_rate   = 0.001,\n",
    "    batch_size      = 128,\n",
    "    hidden_dim      = 300,\n",
    "    num_channels    = 256,\n",
    "    device          = 'cpu',\n",
    "    num_epochs      = 100,\n",
    "    dropout_p       = 0.1,\n",
    "    early_stopping_criteria = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9839875a",
   "metadata": {},
   "source": [
    "# @@@@@ 1. Data Preparation\n",
    "## The data preparation part is to perform a text-to-vectorized-minibatch pipeline: converting text inputs to vectorized minibatches.\n",
    "- ### Step 1: Creating a Vocabulary - mapping each token (characters in the context of surnames) in the surname data to a numerical version of itself.\n",
    "- ### Step 2: Vectorization - going from a text dataset to a vector. The Vectorizer turns different surnames to vectors of integers with the same length.\n",
    "- ### Step 3: Group the vectorized data points into batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8bfd43",
   "metadata": {},
   "source": [
    "## 1.1 - Read Data\n",
    "### **Train partition**: a dataset to derive the model parameters\n",
    "### **Valid partition**: a dataset for selecting among hyperparameters and making modeling decisions\n",
    "### **Test partition**: a dataset for final evaluation and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e01a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10370</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>test</td>\n",
       "      <td>Vertegel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6393</th>\n",
       "      <td>Greek</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Dasios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>Dutch</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>Richard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7538</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>Shimada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5851</th>\n",
       "      <td>German</td>\n",
       "      <td>9</td>\n",
       "      <td>train</td>\n",
       "      <td>Schröder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index  split   surname\n",
       "10370     Russian                 13   test  Vertegel\n",
       "6393        Greek                  4   test    Dasios\n",
       "2437        Dutch                  2   test   Richard\n",
       "7538     Japanese                  7  train   Shimada\n",
       "5851       German                  9  train  Schröder"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv('surnames_with_splits.csv')\n",
    "df_all.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00e77513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>split</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nationality</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>241</td>\n",
       "      <td>1122</td>\n",
       "      <td>240</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>33</td>\n",
       "      <td>154</td>\n",
       "      <td>33</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Czech</th>\n",
       "      <td>63</td>\n",
       "      <td>289</td>\n",
       "      <td>62</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>36</td>\n",
       "      <td>165</td>\n",
       "      <td>35</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>447</td>\n",
       "      <td>2080</td>\n",
       "      <td>445</td>\n",
       "      <td>2972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>35</td>\n",
       "      <td>160</td>\n",
       "      <td>34</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>87</td>\n",
       "      <td>403</td>\n",
       "      <td>86</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greek</th>\n",
       "      <td>24</td>\n",
       "      <td>109</td>\n",
       "      <td>23</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Irish</th>\n",
       "      <td>28</td>\n",
       "      <td>128</td>\n",
       "      <td>27</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>90</td>\n",
       "      <td>420</td>\n",
       "      <td>90</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>117</td>\n",
       "      <td>542</td>\n",
       "      <td>116</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>13</td>\n",
       "      <td>53</td>\n",
       "      <td>11</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>18</td>\n",
       "      <td>84</td>\n",
       "      <td>18</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>357</td>\n",
       "      <td>1661</td>\n",
       "      <td>355</td>\n",
       "      <td>2373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scottish</th>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>40</td>\n",
       "      <td>180</td>\n",
       "      <td>38</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vietnamese</th>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>1660</td>\n",
       "      <td>7680</td>\n",
       "      <td>1640</td>\n",
       "      <td>10980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "split        test  train   val  total\n",
       "nationality                          \n",
       "Arabic        241   1122   240   1603\n",
       "Chinese        33    154    33    220\n",
       "Czech          63    289    62    414\n",
       "Dutch          36    165    35    236\n",
       "English       447   2080   445   2972\n",
       "French         35    160    34    229\n",
       "German         87    403    86    576\n",
       "Greek          24    109    23    156\n",
       "Irish          28    128    27    183\n",
       "Italian        90    420    90    600\n",
       "Japanese      117    542   116    775\n",
       "Korean         13     53    11     77\n",
       "Polish         18     84    18    120\n",
       "Portuguese      9     38     8     55\n",
       "Russian       357   1661   355   2373\n",
       "Scottish       12     52    11     75\n",
       "Spanish        40    180    38    258\n",
       "Vietnamese     10     40     8     58\n",
       "Total        1660   7680  1640  10980"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crosstab = pd.crosstab(df_all['nationality'], df_all['split'])\n",
    "Crosstab['total'] = Crosstab.sum(axis=1)\n",
    "Crosstab.loc['Total'] = Crosstab.sum(axis=0)\n",
    "Crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088259e1",
   "metadata": {},
   "source": [
    "## 1.2 - The Vocabulary class\n",
    "### [A walkthrough of codes](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/Surname_Nationality/class_Vocabulary.ipynb)\n",
    "### - Creating a mapping between the tokens and integers, in terms of dictionaries. To make this mapping reversible, create two dictionaries, one is from-token-to-index, one is from-index-to-token. Then encapsulate this mapping (bijection) into a Vocabulary class.\n",
    "### - By using the UNK token, we can handle tokens at test time that were never seen in training.\n",
    "### - Restricting infrequent tokens from the Vocabularies with a pre-specified parameter cut_off. This is essential in limiting the memory used by the Vocabulary class. \n",
    "### - Expected behaviors:\n",
    "#### (1) add_token(): to add new tokens to the Vocabulary\n",
    "#### (2) lookup_token(): to retrieve the index for a token\n",
    "#### (3) lookup_index(): to retrieve the token corresponding to a specific index.\n",
    "### - The Vocabulary objects will be used in the Vectorization step (discussed next). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be21eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk   = add_unk\n",
    "        self._unk_token = unk_token      \n",
    "        self.unk_index  = -999\n",
    "        ### the unk_token, i.e, \"<UNK>\" is the first added token if add_unk=True\n",
    "        ### self.unk_index is changed from -999 to 0\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "   \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            ### .get(): return self.unk_index if the key \"token\" does not exist. \n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7d55c",
   "metadata": {},
   "source": [
    "## 1.3 - Vectorization\n",
    "### [A walkthrough of codes](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/Surname_Nationality/class_Vectorizer_matrix_of_one_hots.ipynb)\n",
    "### - The class method **from_dataframe()** is used to instantiate a SurnameVectorizer object from a dataframe.\n",
    "### - The SurnameVectorizer encapsulates the nationality vocabulary (nationality_vocab) and the surname vocabulary (surname_vocab).\n",
    "### - The surname_vocab will be used as the reference for one-hot representation. \n",
    "### - The class method **vectorize()** is the core functionality of the Vectorizer. It takes as an argument a string representing a surname and returns a matrix of one-hots representation for the surname. The size of the matrix is (represents the number of tokens in surname_vocab, the length of the longest surname). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d4b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, surname_vocab, nationality_vocab,max_surname_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_vocab (Vocabulary): maps characters to integers\n",
    "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
    "            max_surname_length (int): the length of the longest surname\n",
    "        \"\"\"\n",
    "        self.surname_vocab       = surname_vocab\n",
    "        self.nationality_vocab   = nationality_vocab\n",
    "        self._max_surname_length = max_surname_length\n",
    "         \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): the surnames dataset\n",
    "        Returns:\n",
    "            an instance of the SurnameVectorizer\n",
    "        \"\"\"\n",
    "        surname_vocab      = Vocabulary(add_unk=True, unk_token=\"@\")\n",
    "        nationality_vocab  = Vocabulary(add_unk=False)\n",
    "        # initialzed value of max_surname_length, updated in the for loop below \n",
    "        max_surname_length = 0\n",
    "        \n",
    "        ########## Add tokens to surname_vocab and nationality_vocab\n",
    "        for index, row in surname_df.iterrows():\n",
    "            # update max_surname_length\n",
    "            max_surname_length = max(max_surname_length, len(row.surname))\n",
    "            \n",
    "            # Add tokens(characters) to surname_vocab\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            # Add tokens(words) to nationality_vocab\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab, max_surname_length)\n",
    "\n",
    "    ### This is the key functionality of the Vectorizer.\n",
    "    ### It takes as an argument a string representing a surname, \n",
    "    ### and the length of the longest surname, and returns a vectorized \n",
    "    ### representation of the surname.\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\"\n",
    "        Create a matrix of one-hots representation for the surname\n",
    "        The number of rows: the longest \n",
    "        \n",
    "        Args:\n",
    "            surname (str): the surname \n",
    "        Returns:\n",
    "            one_hot_matrix (np.ndarray): a matrix of one-hot vectors\n",
    "        \"\"\"\n",
    "        ### Create a matrix with size (len(self.surname_vocab),self._max_surname_length)\n",
    "        ### len(self.surname_vocab) represents the number of tokens in surname_vocab\n",
    "        ### self._max_surname_length represents the length of the longest surname.\n",
    "\n",
    "        ### Run lookup_token() for each character in the surname sequentially, return an index\n",
    "        ### Assign the corresponding element in the matrix to 1.\n",
    "        one_hot_matrix_size = (len(self.surname_vocab), self._max_surname_length)\n",
    "        one_hot_matrix      = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
    "                               \n",
    "        for position_index, character in enumerate(surname):\n",
    "            character_index = self.surname_vocab.lookup_token(character)\n",
    "            one_hot_matrix[character_index][position_index] = 1\n",
    "        return one_hot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d071d3",
   "metadata": {},
   "source": [
    "## 1.4 - Batches\n",
    "### [A walkthrough of codes](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/Surname_Nationality/batch_generator.ipynb)\n",
    "\n",
    "### - Group the vectorized data points into batches. \n",
    "### - The grouping is conducted throught the built in class **[DataLoader](https://pytorch.org/docs/stable/data.html)** in PyTorch. \n",
    "### - The class SurnameDataset inherits from the [**Dataset**](https://pytorch.org/vision/0.16/datasets.html) class. Instances of the derived class **SurnameDataset** can then be used with data loading tools like **DataLoader()** for efficient batch loading during model training.\n",
    "### - The methods \\_\\_len\\_\\_(), and \\_\\_getitem\\_\\_() are defined in class **SurnameDataset** - these magic functions are expected by the **DataLoader()**. An object equipped with \\_\\_len\\_\\_() can be passed to the len() Python build-in function. For objects equipped with \\_\\_getitem\\_\\_() we can use the standard subscript for indexing tuples and lists to access individual items. \n",
    "### - The **DataLoader()** function utilizes the return results of the \\_\\_getitem\\_\\_() method in the dataset to construct batches of data. In each iteration, **DataLoader()** calls the \\_\\_getitem\\_\\_() method of the dataset to retrieve a sample, and then combines these samples into a batch. \n",
    "### - In **DataLoader()**, the \\_\\_getitem\\_\\_() method uses an index generated by the **Sampler** object. The **Sampler** is responsible for determining the indices of samples in each batch. This index may be generated sequentially or randomly, depending on the setting of the shuffle parameter.\n",
    "### - Define a batch generator function that wraps the DataLoader and switch the data between the CPU and the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7980b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self,surname_df,vectorizer):\n",
    "        self.surname_df  = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df    = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size  = len(self.train_df)\n",
    "\n",
    "        self.val_df      = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df     = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size   = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val'  : (self.val_df, self.validation_size),\n",
    "                             'test' : (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # Class weights - Assign a weight to each class that inversely proportional to its frequency.\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "    @classmethod\n",
    "    def load_csv_and_make_vectorizer(cls,surname_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        ### make vectorizer using training dataset\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        new_vectorizer  = SurnameVectorizer.from_dataframe(train_surname_df)\n",
    "        return cls(surname_df,new_vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_df_and_make_vectorizer(cls,surname_df):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "            surname_df: dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        ### make vectorizer using training dataset\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        new_vectorizer  = SurnameVectorizer.from_dataframe(train_surname_df)\n",
    "        return cls(surname_df,new_vectorizer)\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        ### when split = 'train', _target_df means the training set\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        ### _target_size is defined in set_split() \n",
    "        return self._target_size        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        \n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        surname_matrix = \\\n",
    "            self._vectorizer.vectorize(row.surname)\n",
    "\n",
    "        nationality_index = \\\n",
    "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "        return {'x_data': surname_matrix,\n",
    "                'y_target': nationality_index}\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c27fae",
   "metadata": {},
   "source": [
    "### Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5dd7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device='cpu'):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0e3ed4",
   "metadata": {},
   "source": [
    "### An example of one data batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90f3643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x in one batch\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "size of x_data: torch.Size([10, 48, 15])\n",
      "------------------------------------------------------------\n",
      "y in one batch\n",
      "tensor([8, 1, 9, 0, 5, 7, 2, 3, 2, 4])\n",
      "size of y_data: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "df_sample      = df_all.sample(100,random_state=100)\n",
    "dataset_sample = SurnameDataset.load_df_and_make_vectorizer(df_sample)\n",
    "batch_size     = 10\n",
    "shuffle        = True\n",
    "drop_last      = True\n",
    "dataloader     = DataLoader(dataset=dataset_sample, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "one_batch = next(iter(dataloader))\n",
    "print('x in one batch')\n",
    "print(one_batch['x_data'])\n",
    "print('size of x_data:', one_batch['x_data'].shape)\n",
    "print('-' * 60)\n",
    "print('y in one batch')\n",
    "print(one_batch['y_target'])\n",
    "print('size of y_data:', one_batch['y_target'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f6833",
   "metadata": {},
   "source": [
    "# @@@@@ 2. Model / Optimizer / Loss\n",
    "## 2.1 - The model (CNN) and activate function (Exponential Linear Unit, ELU)\n",
    "### - The **SurnameClassifier** inherits from PyTorch’s **Module** and creates CNN classifier. The model is created using [nn.Sequential()](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).\n",
    "### - Some details about convolutional layers is discussed [in this study](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/Surname_Nationality/convolutional_layer.ipynb). \n",
    "### - In PyTorch, the **nn.Module** class implements the **\\_\\_call\\_\\_** method, enabling model instances to be invoked like functions. Calling an instance from nn.Module with a set of arguments ends up calling a method named forward with the same argument. The forward function executes the forward computation, while **\\_\\_call\\_\\_** does other important chores before and after calling forward. In general, the correct way to call the module as a function is to use **Classifier(input)**, rather than **Classifier.forward(input)**, although they will produce the same outputs (silient errors, since there are steps not called properly if just using **forward(...)** directly).\n",
    "### - The **forward()** method allows for the softmax function (working as the nonlinear activation function) to be optionally applied. Here the default is do not apply the sigmoid function. \n",
    "### - ELU (Exponential Linear Unit) is the activation function.  [nn.ELU()](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html) is used to apply the ELU function.  ELU is a nonlinearity similar to the ReLU, but rather than clipping values below 0, it exponentiates them. ELU has been shown to be a promising nonlinearity to use between convolutional layers (Clevert et al., 2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51be6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifier(nn.Module):\n",
    "    \"\"\"  2-layer Multilayer Perceptron \"\"\"\n",
    "    def __init__(self, initial_num_channels, num_classes, num_channels, dropout_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            initial_num_channels (int): size of the incoming feature vector\n",
    "                                        (used in the first convnet layer)\n",
    "                                        the length of surname vocabulary\n",
    "                                        \n",
    "            num_classes (int) : size of the output prediction vector \n",
    "                                (used in the final fc layer)\n",
    "                                the number of nationality classes\n",
    "                                \n",
    "            num_channels (int): constant channel size to use throughout network \n",
    "                                (used in the convnet layers as output channels)\n",
    "                                in this example, the num_channels for each of the convolutions\n",
    "                                is set to be the same value. This is not necessary. We can\n",
    "                                alternatively chosen a different number of channels for each \n",
    "                                convolution operation separately. Doing so would entail optimizing \n",
    "                                more hyperparameters. \n",
    "                                \n",
    "            dropout_prob(float): apply dropout layer if dropout_prob>0\n",
    "                                \n",
    "        \"\"\"\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        \n",
    "        ### Define the Convolutional Neural Network\n",
    "        self.convnet = nn.Sequential(\n",
    "            ### conv1\n",
    "            nn.Conv1d(in_channels  = initial_num_channels, \n",
    "                      out_channels = num_channels, \n",
    "                      kernel_size  = 3),\n",
    "            ### act1 - ELU has been shown to be a promising nonlinearity to use between\n",
    "            ###        convolutional layers (Clevert et al., 2015)\n",
    "            nn.ELU(),\n",
    "            ### conv2\n",
    "            nn.Conv1d(in_channels  = num_channels, \n",
    "                      out_channels = num_channels,\n",
    "                      kernel_size  = 3, \n",
    "                      stride       = 2),\n",
    "            ### act2\n",
    "            nn.ELU(),\n",
    "            ### conv3\n",
    "            nn.Conv1d(in_channels  = num_channels, \n",
    "                      out_channels = num_channels, \n",
    "                      kernel_size  = 3, \n",
    "                      stride       = 2),\n",
    "            ### act3\n",
    "            nn.ELU(),\n",
    "            ### conv4\n",
    "            nn.Conv1d(in_channels  = num_channels, \n",
    "                      out_channels = num_channels, \n",
    "                      kernel_size  = 3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=args.dropout_p)\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False, apply_dropout=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor\n",
    "            x_in.shape should be (batch, initial_num_channels, max_surname_length)\n",
    "            apply_softmax (bool): a flag for the softmax activation should be false if \n",
    "                                  used with the cross-entropy losses\n",
    "        \n",
    "        Returns: the resulting tensor. tensor.shape should be (batch, num_classes).\n",
    "        \"\"\"\n",
    "        ### .squeeze(dim=2): Transforming the shape of a tensor  \n",
    "        ### from (batch_size,num_channels,1) to (batch_size,num_channels)\n",
    "        features = self.convnet(x_in).squeeze(dim=2)\n",
    "        \n",
    "        features = self.dropout(features)\n",
    "       \n",
    "        prediction_vector = self.fc(features)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc957605",
   "metadata": {},
   "source": [
    "### Define the model \n",
    "### - Data: df_all\n",
    "### - Training hyperparameters (batch_size, device): defined in args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de7bb413",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset    = SurnameDataset.load_df_and_make_vectorizer(df_all)\n",
    "dataloader = DataLoader(dataset = dataset,\n",
    "                        batch_size = 128,\n",
    "                        shuffle = True,\n",
    "                        drop_last = True)\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a54e1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SurnameClassifier(\n",
       "  (convnet): Sequential(\n",
       "    (0): Conv1d(77, 10, kernel_size=(3,), stride=(1,))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Conv1d(10, 10, kernel_size=(3,), stride=(2,))\n",
       "    (3): ELU(alpha=1.0)\n",
       "    (4): Conv1d(10, 10, kernel_size=(3,), stride=(2,))\n",
       "    (5): ELU(alpha=1.0)\n",
       "    (6): Conv1d(10, 10, kernel_size=(3,), stride=(1,))\n",
       "    (7): ELU(alpha=1.0)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=10, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SurnameClassifier(initial_num_channels = len(vectorizer.surname_vocab),\n",
    "                               num_classes  = len(vectorizer.nationality_vocab),\n",
    "                               num_channels = 10,\n",
    "                               dropout_prob = 0)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4caf97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input: torch.Size([128, 77, 17])\n",
      "--------------------------------------------------------------------------------\n",
      "Outout (using initialized parameters)\n",
      "torch.return_types.max(\n",
      "values=tensor([0.3129, 0.3121, 0.3042, 0.3046, 0.3628, 0.3173, 0.3156, 0.2802, 0.3414,\n",
      "        0.3142, 0.2945, 0.3087, 0.2813, 0.3512, 0.3170, 0.2985, 0.3248, 0.3113,\n",
      "        0.3190, 0.2929, 0.3171, 0.3131, 0.3192, 0.2970, 0.3157, 0.3173, 0.3192,\n",
      "        0.3127, 0.3643, 0.3122, 0.2948, 0.2951, 0.3092, 0.2975, 0.2659, 0.2655,\n",
      "        0.2754, 0.3257, 0.3175, 0.3086, 0.3597, 0.3155, 0.2895, 0.3000, 0.2887,\n",
      "        0.3195, 0.3648, 0.2909, 0.3146, 0.3058, 0.3145, 0.3150, 0.3490, 0.2862,\n",
      "        0.3265, 0.3171, 0.3052, 0.2960, 0.3094, 0.3551, 0.3153, 0.3000, 0.2872,\n",
      "        0.3143, 0.3134, 0.3148, 0.3312, 0.3220, 0.2982, 0.3158, 0.3122, 0.3672,\n",
      "        0.3134, 0.2822, 0.3085, 0.2933, 0.2649, 0.2833, 0.2981, 0.3221, 0.2803,\n",
      "        0.3162, 0.2897, 0.2968, 0.3097, 0.3203, 0.3147, 0.3086, 0.3205, 0.3238,\n",
      "        0.3604, 0.3595, 0.3562, 0.2838, 0.3437, 0.3119, 0.3096, 0.3015, 0.3247,\n",
      "        0.3034, 0.2829, 0.3160, 0.3147, 0.3043, 0.3110, 0.3030, 0.2864, 0.3144,\n",
      "        0.3135, 0.2798, 0.3068, 0.3052, 0.3064, 0.3195, 0.3012, 0.3098, 0.3233,\n",
      "        0.2941, 0.3197, 0.2978, 0.3160, 0.3063, 0.2669, 0.3169, 0.3202, 0.3144,\n",
      "        0.3110, 0.3182], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([ 8, 16,  8,  8,  8,  8,  8,  8,  8,  8,  8, 16,  8,  8,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  5, 16, 16, 12,  5, 12,\n",
      "         8, 16,  8,  8,  8,  8,  5,  8,  8,  8,  8,  5,  8,  8,  8,  8,  8,  5,\n",
      "         8,  8,  8,  5,  8,  8,  8,  5,  5,  8,  8,  8, 16,  8,  8,  8,  8,  8,\n",
      "         8,  5, 16,  5,  5,  8,  8,  8,  8, 16,  5,  8, 16,  8,  8,  8,  8,  8,\n",
      "         8,  8, 16,  8,  8,  8,  8,  5,  8,  8,  5, 16,  8,  8,  8,  8, 12,  8,\n",
      "         8,  8,  8,  8,  8, 16,  8,  8,  8,  5,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "         8,  8]))\n"
     ]
    }
   ],
   "source": [
    "one_batch = next(iter(dataloader))\n",
    "print(\"shape of input:\", one_batch['x_data'].shape)\n",
    "print('-'*80)\n",
    "print(\"Outout (using initialized parameters)\")\n",
    "print(classifier(one_batch['x_data'],\n",
    "                 apply_softmax=False,\n",
    "                 apply_dropout=False).max(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b2a27",
   "metadata": {},
   "source": [
    "## 2.2 - The loss function (Binary Cross-Entropy)\n",
    "### - The loss - \"how far off\" the model predictions are from the target.\n",
    "### - The gradient of the loss function - a signal for “how much” the parameters should change (according to \"how much\" each parameter contributed to the loss function).\n",
    "### - As mentioned, the loss function should be appropriate for the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d453ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weighted_func   = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "loss_unweighted_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabfcf93",
   "metadata": {},
   "source": [
    "## 2.3 - The optimizer and scheduler\n",
    "### The initialized state of the classifier\n",
    "### Using [torch.nn.Module.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters)\n",
    "#### These are the tensors that the optimizer will get. After calling **model.backward()** the parameters are populated with their grad, and the optimizer then updates their values accordingly during the **optimizer.step()** call.\n",
    "#### The requires_grad = True argument is telling PyTorch to track the entire family tree of tensors resulting from operations on *parameters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1384db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SurnameClassifier(\n",
       "  (convnet): Sequential(\n",
       "    (0): Conv1d(77, 10, kernel_size=(3,), stride=(1,))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Conv1d(10, 10, kernel_size=(3,), stride=(2,))\n",
       "    (3): ELU(alpha=1.0)\n",
       "    (4): Conv1d(10, 10, kernel_size=(3,), stride=(2,))\n",
       "    (5): ELU(alpha=1.0)\n",
       "    (6): Conv1d(10, 10, kernel_size=(3,), stride=(1,))\n",
       "    (7): ELU(alpha=1.0)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=10, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a6d7e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.parameters() includes 10 sets of parameters.\n",
      "1 - Parameters for the convolutional layers: conv1.weight\n",
      "shape: torch.Size([10, 77, 3])\n",
      "------------------------------------------------------------\n",
      "2 - Parameters for the convolutional layers: conv1.bias\n",
      "shape: torch.Size([10])\n",
      "------------------------------------------------------------\n",
      "3 - Parameters for the convolutional layers: conv2.weight\n",
      "shape: torch.Size([10, 10, 3])\n",
      "------------------------------------------------------------\n",
      "4 - Parameters for the convolutional layers: conv2.bias\n",
      "shape: torch.Size([10])\n",
      "------------------------------------------------------------\n",
      "5 - Parameters for the convolutional layers: conv3.weight\n",
      "shape: torch.Size([10, 10, 3])\n",
      "------------------------------------------------------------\n",
      "6 - Parameters for the convolutional layers: conv3.bias\n",
      "shape: torch.Size([10])\n",
      "------------------------------------------------------------\n",
      "7 - Parameters for the convolutional layers: conv4.weight\n",
      "shape: torch.Size([10, 10, 3])\n",
      "------------------------------------------------------------\n",
      "8 - Parameters for the convolutional layers: conv4.bias\n",
      "shape: torch.Size([10])\n",
      "------------------------------------------------------------\n",
      "9 - Parameters for the convolutional layers: fc.weight\n",
      "shape: torch.Size([18, 10])\n",
      "------------------------------------------------------------\n",
      "10 - Parameters for the convolutional layers: fc.bias\n",
      "shape: torch.Size([18])\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_param = len(list(classifier.parameters()))\n",
    "print(f\"classifier.parameters() includes {n_param} sets of parameters.\")\n",
    "\n",
    "print(\"1 - Parameters for the convolutional layers: conv1.weight\")\n",
    "print(\"shape:\", list(classifier.parameters())[0].shape)\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"2 - Parameters for the convolutional layers: conv1.bias\")\n",
    "print(\"shape:\", list(classifier.parameters())[1].shape)\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"3 - Parameters for the convolutional layers: conv2.weight\")\n",
    "print(\"shape:\", list(classifier.parameters())[2].shape)\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"4 - Parameters for the convolutional layers: conv2.bias\")\n",
    "print(\"shape:\", list(classifier.parameters())[3].shape)\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"5 - Parameters for the convolutional layers: conv3.weight\")\n",
    "print(\"shape:\", list(classifier.parameters())[4].shape)\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"6 - Parameters for the convolutional layers: conv3.bias\")\n",
    "print(\"shape:\", list(classifier.parameters())[5].shape)\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"7 - Parameters for the convolutional layers: conv4.weight\")\n",
    "print(\"shape:\", list(classifier.parameters())[6].shape)\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"8 - Parameters for the convolutional layers: conv4.bias\")\n",
    "print(\"shape:\", list(classifier.parameters())[7].shape)\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"9 - Parameters for the convolutional layers: fc.weight\")\n",
    "print(\"shape:\", list(classifier.parameters())[8].shape)\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"10 - Parameters for the convolutional layers: fc.bias\")\n",
    "print(\"shape:\", list(classifier.parameters())[9].shape)\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a302c6",
   "metadata": {},
   "source": [
    "### Define the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer \n",
    "#### - the learning rate is set adaptively\n",
    "#### - it is not sensitive to the scaling of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af6f98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d7e6c89c",
   "metadata": {},
   "source": [
    "### Apply a scheduler for adjusting learning rate\n",
    "### - [torch.optim.lr_scheduler](https://pytorch.org/docs/stable/optim.html) provides several methods to adjust the learning rate based on the number of epochs.\n",
    "### - [torch.optim.lr_scheduler.ReduceLROnPlateau](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau) allows dynamic learning rate reducing based on some validation measurements.\n",
    "### - This scheduler reads a metrics quantity and if no improvement is seen for a ‘patience’ number of epochs, the learning rate is reduced.\n",
    "### - Parameters:\n",
    "1. **mode(str)**: In \"min\" mode, lr will be reduced when the quantity monitored has stopped decreasing; in \"max\" mode it will be reduced when the quantity monitored has stopped increasing. Default: 0.1.\n",
    "2. **factor(float)**: Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1.\n",
    "3. **patience (int)**: Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the 3rd epoch if the loss still hasn’t improved then. Default: 10.\n",
    "4. **threshold (float)** – Threshold for measuring the new optimum, to only focus on significant changes. Default: 1e-4.\n",
    "\n",
    "### - A **scheduler.step(val_loss)** method is called at the end of each epoch to execute the update of the learning rate. The parameters “val_loss” represents the loss (or other monitoring metric) computed for the model on the validation set. This loss value is typically used by the scheduler to assess the model's performance on the validation set and update the learning rate accordingly.\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1774bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer,\n",
    "                                                 mode      = 'min', \n",
    "                                                 factor    = 0.5,\n",
    "                                                 patience  = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12292f73",
   "metadata": {},
   "source": [
    "# @@@@@ 3. Training Routine\n",
    "## 3.1 - Helper function: tracking the training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfa47269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_state(args, save_model_name):\n",
    "    train_state_dict = {'stop_early'    : False,\n",
    "                        'early_stopping_step'     : 0,\n",
    "                        'early_stopping_best_val' : 1e8,\n",
    "                        'learning_rate' : args.learning_rate,\n",
    "                        'epoch_index'   : 0,\n",
    "                        'train_loss'    : [],\n",
    "                        'train_acc'     : [],\n",
    "                        'val_loss'      : [],\n",
    "                        'val_acc'       : [],\n",
    "                        'test_loss'     : -1,\n",
    "                        'test_acc'      : -1,\n",
    "                        'model_filename': save_model_name\n",
    "                       }\n",
    "    return train_state_dict\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"\n",
    "    Handle the training state updates.\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "    \n",
    "    Args:\n",
    "        args:  arguments\n",
    "        model: model to train\n",
    "        train_state: a dictionary representing the training state values\n",
    "    \n",
    "    Returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save the first model\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss increased (not a better model)\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update early_stopping_step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # If loss decreased\n",
    "        else:\n",
    "            # Save the best model and update the early_stopping_best_val\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        # In the main training loop, if train_state['stop_early']: break\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b074e",
   "metadata": {},
   "source": [
    "## 3.2 - Helper function: compute accurary rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09548dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target, device):\n",
    "    y_target = y_target.to(device)\n",
    "\n",
    "    ##### tensor.max(dim=1): the results include two output tensors (max, max_indices)\n",
    "    _, y_pred_indices = y_pred.to(device).max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea8242d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.3199, 0.3153, 0.2973, 0.3092, 0.3125, 0.2728, 0.3022, 0.3157, 0.2793,\n",
       "        0.2648, 0.3156, 0.3065, 0.3092, 0.3153, 0.2909, 0.3193, 0.3091, 0.3087,\n",
       "        0.3567, 0.3199, 0.3124, 0.3168, 0.3056, 0.3016, 0.2991, 0.3109, 0.3093,\n",
       "        0.3141, 0.2791, 0.3163, 0.3107, 0.3032, 0.2791, 0.3100, 0.2668, 0.3125,\n",
       "        0.3542, 0.3173, 0.2792, 0.3084, 0.2815, 0.3137, 0.3091, 0.3091, 0.3190,\n",
       "        0.3123, 0.3007, 0.3173, 0.3158, 0.2789, 0.3133, 0.3130, 0.3075, 0.3166,\n",
       "        0.2764, 0.3174, 0.3016, 0.2907, 0.2960, 0.3540, 0.2922, 0.3134, 0.3094,\n",
       "        0.3140, 0.3177, 0.3129, 0.3080, 0.3600, 0.3095, 0.3582, 0.3239, 0.2573,\n",
       "        0.3193, 0.3094, 0.2864, 0.3155, 0.3132, 0.3583, 0.2844, 0.3129, 0.3027,\n",
       "        0.3074, 0.2373, 0.3423, 0.3186, 0.3069, 0.3199, 0.3120, 0.3592, 0.2849,\n",
       "        0.2954, 0.3046, 0.3225, 0.2909, 0.3101, 0.3092, 0.3162, 0.3164, 0.3061,\n",
       "        0.3092, 0.3162, 0.2876, 0.2989, 0.3136, 0.2798, 0.3132, 0.3222, 0.3086,\n",
       "        0.2885, 0.2728, 0.3580, 0.2787, 0.3127, 0.3202, 0.3163, 0.2959, 0.2650,\n",
       "        0.3143, 0.3123, 0.3179, 0.3073, 0.3048, 0.3169, 0.2987, 0.3170, 0.3110,\n",
       "        0.2819, 0.3075], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([ 8,  8,  8, 16,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8, 16, 12,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  5,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8, 16,  8,  8,  8,  8,  8,  5,  8,  8,  8,  8,\n",
       "         8,  8,  8,  5,  8,  8,  5,  8,  8,  8,  8,  8,  8,  8, 16,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8, 16,  5,  8,  8,  8,  8,  8,  8,  5,\n",
       "         8,  8,  8,  8,  8, 16,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 16,\n",
       "         8, 16,  8,  5, 16,  8,  8,  5,  8,  8, 16,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### An example\n",
    "one_batch = next(iter(dataloader))\n",
    "classifier(one_batch['x_data']).max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b5a6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets: tensor([14,  2,  5,  2, 14, 14,  4,  4, 10,  9, 10,  2,  4,  4,  1,  8, 14,  4,\n",
      "         2,  8, 14,  0, 10,  0,  4,  4,  4,  4,  0, 16,  0, 16, 14,  4,  4,  0,\n",
      "        14, 14, 14,  6,  4,  9,  0,  0, 10, 14,  0,  8,  4,  0,  4,  9, 14,  8,\n",
      "        10,  6,  2,  1,  6,  0,  4, 14,  9, 14, 14, 14,  4,  4,  1, 16, 14,  2,\n",
      "         6,  0,  3,  4, 10,  6, 10,  0,  4,  6,  0,  7,  0, 14,  4, 16,  4, 14,\n",
      "        14,  4, 14,  0, 14,  4,  4,  0,  4,  4, 16,  4, 14,  4,  8,  4,  2,  8,\n",
      "         2, 10, 14,  4,  6,  9,  8,  4,  9,  5,  4,  0, 14,  1,  1, 14,  0,  4,\n",
      "        14,  6])\n",
      "--------------------------------------------------------------------------------\n",
      "pred: tensor([ 8,  5,  8,  8, 16, 16,  8,  8,  8,  5,  8,  8,  8,  8,  8, 16,  8,  8,\n",
      "         8,  8,  8,  8,  8,  5,  8, 16,  8,  8, 16,  8,  8,  8,  8,  8,  5,  8,\n",
      "         8,  8,  5, 16, 16,  8,  8,  8,  5,  8,  8,  8,  8,  8,  8,  8, 16,  8,\n",
      "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  5,  8,  5,  8,  8,  8,  8,  8,\n",
      "         8,  8,  5,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  5,  8,  8,  8,\n",
      "         8,  8, 16, 16,  8,  8, 16,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8, 16,  8,  8,  8, 16, 16,  5,  8,  8,  8,  8,  8,  8, 16,\n",
      "         5,  8])\n",
      "--------------------------------------------------------------------------------\n",
      "accurary rate: 4.6875\n"
     ]
    }
   ],
   "source": [
    "outputs = classifier(one_batch['x_data'])\n",
    "_, pred = outputs.max(dim=1)  \n",
    "targets = one_batch['y_target']\n",
    "print('targets:', targets)\n",
    "print('-'*80)\n",
    "print('pred:',pred)\n",
    "print('-'*80)\n",
    "print('accurary rate:',compute_accuracy(outputs,targets,device='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97859f97",
   "metadata": {},
   "source": [
    "## 3.3 - Training loop\n",
    "### - The training loop updates the model parameters so that it improves over time.\n",
    "### - The training loop is composed of two loops: an inner loop over batches in the dataset, and an outer loop, which repeats the inner loop a number of times.\n",
    "### - The inner loop (batch), losses are computed for each batch, and the optimizer is used to update the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24d220c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_train_state(train_state):\n",
    "    print('Train Loss:',round(train_state['train_loss'][-1],5))\n",
    "    print('Train Accuracy:',round(train_state['train_acc'][-1],5))\n",
    "    print('Valid Loss:',round(train_state['val_loss'][-1],5))\n",
    "    print('Valid Accuracy:',round(train_state['val_acc'][-1],5))\n",
    "    print('early_stopping_best_val:',round(train_state['early_stopping_best_val'],5))\n",
    "    print('early_stopping_step:',train_state['early_stopping_step'])\n",
    "    print('stop_early:',train_state['stop_early'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3fb2a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(get_model='TRAIN', input_path='/Users/home/JH/Machine Learning/Surnames', output_path='/Users/home/JH/Machine Learning/Surnames/OUTPUT/', learning_rate=0.001, batch_size=128, hidden_dim=300, num_channels=256, device='cpu', num_epochs=100, dropout_p=0.1, early_stopping_criteria=5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57bdc8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingClassifier(classifier,args,dataset,use_weight,use_dropout):\n",
    "    \n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                     mode='min', factor=0.5,\n",
    "                                                     patience=1)\n",
    "    \n",
    "    if use_weight:\n",
    "        loss_func       = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "        if use_dropout:\n",
    "            save_model_name = args.output_path+'/cnn_weighted_dropout_model.pth'\n",
    "        else: \n",
    "            save_model_name = args.output_path+'/cnn_weighted_nodropout_model.pth'\n",
    "    else:\n",
    "        loss_func       = nn.CrossEntropyLoss()\n",
    "        if use_dropout:\n",
    "            save_model_name = args.output_path+'/cnn_unweighted_dropout_model.pth'\n",
    "        else: \n",
    "            save_model_name = args.output_path+'/cnn_unweighted_nodropout_model.pth'\n",
    "        \n",
    "\n",
    "    if args.get_model == 'TRAIN':\n",
    "        ##### Get an initialized train_state\n",
    "        train_state = init_train_state(args,save_model_name)\n",
    "\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "            print('-'*60)\n",
    "            print(f'Epoch {epoch_index}...')\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "\n",
    "            ##################################################\n",
    "            #####     Iterate over training dataset      #####\n",
    "            ##################################################\n",
    "            print('Training Iteration...')\n",
    "\n",
    "            ##### Create a batch_generator using training data\n",
    "            dataset.set_split('train')\n",
    "            batch_generator = generate_batches(dataset,\n",
    "                                               batch_size = args.batch_size,\n",
    "                                               device     = args.device)\n",
    "\n",
    "            ##### running_loss and running_acc are equivalent to the moving averages of loss and accuracy.\n",
    "            ##### when the loop ends, a moving average is just an average. \n",
    "            ##### In each epoch loop, they are reset to zero before the batch loop.\n",
    "            running_loss = 0.0\n",
    "            running_acc  = 0.0\n",
    "\n",
    "            ##### Indicate that the model is in “training mode” \n",
    "            # makes the model parameters mutable \n",
    "            # and enables regularization mechanisms like dropout\n",
    "            classifier.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # the training routine is these 5 steps:\n",
    "                # --------------------------------------\n",
    "                ##### STEP 1. zero the gradients\n",
    "                # Inside each batch iteration, the optimizer’s gradients are first reset\n",
    "                # Calling backward (step 4 below) will ACCUMULATE gradients, so if the backward()\n",
    "                # is called earlier, the new gradient is accumulated on top of the one computed \n",
    "                # in previous iterations, which leads ao an incorrect value for the gradient.\n",
    "                # Therefore, use this zero_ method to reset the gradients.\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                ##### STEP 2. compute the output\n",
    "                y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "                ##### STEP 3. compute the loss\n",
    "                loss   = loss_func(y_pred, batch_dict['y_target'])\n",
    "                loss_t = loss.item()\n",
    "                # update the moving average of loss, batch by batch\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                ##### STEP 4. use loss to produce gradients (gradients are propagated to each parameter)\n",
    "                # Calling backward() and the gradients at each leaf is ACCUMULATED, not stored.\n",
    "                # Note that the loss is the loss in train split. There is no valid_loss.backward()\n",
    "                # because we don't want to train the model on the validation data. \n",
    "                loss.backward()\n",
    "\n",
    "                ##### STEP 5. use optimizer to update parameters\n",
    "                # the optimizer uses the propagated gradients to perform parameter updates\n",
    "                # The value of classifier.parameters(), i.e., params is automatically updated in this step. \n",
    "                # In specific, the optimizer looks into params.grad and updates params, by substracting \n",
    "                # learning_rate * grad from it. \n",
    "                optimizer.step()\n",
    "\n",
    "                ##### Tracking the accuracy\n",
    "                # compute the accuracy\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'],args.device)\n",
    "                # update the moving average of acc, batch by batch\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            ##### After this inner loop (training) ends\n",
    "            # Append the running_loss and running_acc to train_state\n",
    "            # (the average of loss and acc in all the batches in the current epoch)\n",
    "            train_state['train_loss'].append(running_loss)\n",
    "            train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            ##################################################\n",
    "            #####     Iterate over validation dataset    #####\n",
    "            ##################################################\n",
    "            print('Validation Iteration...')\n",
    "\n",
    "            ##### Create a batch_generator using validation data\n",
    "            dataset.set_split('val')    \n",
    "            batch_generator = generate_batches(dataset, \n",
    "                                               batch_size = args.batch_size, \n",
    "                                               device     = args.device)\n",
    "            ##### Create new running loss, and running accuracy\n",
    "            running_loss = 0.0\n",
    "            running_acc  = 0.0\n",
    "\n",
    "            ##### Indicate that the model is in “evaluation mode”\n",
    "            # makes the model parameters immutable \n",
    "            # disables dropout\n",
    "            # disables computation of the loss and propagation of gradients back to the parameters\n",
    "            classifier.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                ##### compute the output\n",
    "                y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "                ##### STEP 3. compute the loss\n",
    "                loss   = loss_func(y_pred, batch_dict['y_target'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                ##### compute the accuracy\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'],args.device)\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            ##### After this inner loop (validation) ends\n",
    "            # Append the running_loss and running_acc to train_state\n",
    "            # (the average of loss and acc in all the batches in the current epoch)\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            ##### Update the status of Early Stopping and Model Checkpoint\n",
    "            # Input: the current classifier and current train_state (end of the current epoch loop)\n",
    "            # Update three items in train_state\n",
    "            # 1.\"early_stopping_step\" +=1 or reset to 0 (comparing val_loss and early_stopping_best_val)\n",
    "            # 2.\"early_stopping_best_val\" update to the current val_loss if it is the best model\n",
    "            # 3.\"stop_early\" if early_stopping_step reaches early_stopping_criteria. If True, break all loops below. \n",
    "            # Save a new model if the current model has early_stopping_best_val\n",
    "            train_state = update_train_state(args = args, model = classifier,\n",
    "                                             train_state = train_state)\n",
    "\n",
    "            scheduler.step(train_state['val_loss'][-1])\n",
    "            print('Current lr:', optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            print_train_state(train_state)\n",
    "\n",
    "            if train_state['stop_early']:\n",
    "                break\n",
    "\n",
    "    elif args.get_model == 'LOAD':\n",
    "        print('Please load the model.')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a2aca",
   "metadata": {},
   "source": [
    "### Model 1 - Use weight, no dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "601270be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch 0...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 2.63226\n",
      "Train Accuracy: 17.05729\n",
      "Valid Loss: 2.25687\n",
      "Valid Accuracy: 26.23698\n",
      "early_stopping_best_val: 100000000.0\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 1...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.95257\n",
      "Train Accuracy: 37.57812\n",
      "Valid Loss: 2.03483\n",
      "Valid Accuracy: 40.6901\n",
      "early_stopping_best_val: 2.03483\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 2...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.68467\n",
      "Train Accuracy: 41.99219\n",
      "Valid Loss: 1.91348\n",
      "Valid Accuracy: 40.625\n",
      "early_stopping_best_val: 1.91348\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 3...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.51954\n",
      "Train Accuracy: 45.70313\n",
      "Valid Loss: 1.84556\n",
      "Valid Accuracy: 40.95052\n",
      "early_stopping_best_val: 1.84556\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 4...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.40214\n",
      "Train Accuracy: 48.46354\n",
      "Valid Loss: 1.88739\n",
      "Valid Accuracy: 41.99219\n",
      "early_stopping_best_val: 1.84556\n",
      "early_stopping_step: 1\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 5...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 1.33062\n",
      "Train Accuracy: 50.2474\n",
      "Valid Loss: 1.85021\n",
      "Valid Accuracy: 43.55469\n",
      "early_stopping_best_val: 1.84556\n",
      "early_stopping_step: 2\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 6...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 1.16008\n",
      "Train Accuracy: 53.17708\n",
      "Valid Loss: 1.83893\n",
      "Valid Accuracy: 48.4375\n",
      "early_stopping_best_val: 1.83893\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 7...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 1.08038\n",
      "Train Accuracy: 55.85938\n",
      "Valid Loss: 1.83955\n",
      "Valid Accuracy: 52.79948\n",
      "early_stopping_best_val: 1.83893\n",
      "early_stopping_step: 1\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 8...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 1.02509\n",
      "Train Accuracy: 56.32812\n",
      "Valid Loss: 1.83545\n",
      "Valid Accuracy: 53.125\n",
      "early_stopping_best_val: 1.83545\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 9...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 0.97753\n",
      "Train Accuracy: 58.64583\n",
      "Valid Loss: 1.85794\n",
      "Valid Accuracy: 48.17708\n",
      "early_stopping_best_val: 1.83545\n",
      "early_stopping_step: 1\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 10...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.00025\n",
      "Train Loss: 0.9291\n",
      "Train Accuracy: 59.9349\n",
      "Valid Loss: 1.88993\n",
      "Valid Accuracy: 51.10677\n",
      "early_stopping_best_val: 1.83545\n",
      "early_stopping_step: 2\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 11...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.00025\n",
      "Train Loss: 0.84907\n",
      "Train Accuracy: 62.10937\n",
      "Valid Loss: 1.83119\n",
      "Valid Accuracy: 54.23177\n",
      "early_stopping_best_val: 1.83119\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 12...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.00025\n",
      "Train Loss: 0.81861\n",
      "Train Accuracy: 63.54167\n",
      "Valid Loss: 1.91324\n",
      "Valid Accuracy: 55.59896\n",
      "early_stopping_best_val: 1.83119\n",
      "early_stopping_step: 1\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 13...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.000125\n",
      "Train Loss: 0.79855\n",
      "Train Accuracy: 63.64583\n",
      "Valid Loss: 1.89638\n",
      "Valid Accuracy: 55.33854\n",
      "early_stopping_best_val: 1.83119\n",
      "early_stopping_step: 2\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 14...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.000125\n",
      "Train Loss: 0.74381\n",
      "Train Accuracy: 65.91146\n",
      "Valid Loss: 1.82206\n",
      "Valid Accuracy: 56.90104\n",
      "early_stopping_best_val: 1.82206\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 15...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.000125\n",
      "Train Loss: 0.74236\n",
      "Train Accuracy: 65.71615\n",
      "Valid Loss: 1.89311\n",
      "Valid Accuracy: 58.13802\n",
      "early_stopping_best_val: 1.82206\n",
      "early_stopping_step: 1\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 16...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 6.25e-05\n",
      "Train Loss: 0.72143\n",
      "Train Accuracy: 66.23698\n",
      "Valid Loss: 1.95738\n",
      "Valid Accuracy: 56.96615\n",
      "early_stopping_best_val: 1.82206\n",
      "early_stopping_step: 2\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 17...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 6.25e-05\n",
      "Train Loss: 0.7102\n",
      "Train Accuracy: 66.6276\n",
      "Valid Loss: 1.97669\n",
      "Valid Accuracy: 56.96615\n",
      "early_stopping_best_val: 1.82206\n",
      "early_stopping_step: 3\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 18...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 3.125e-05\n",
      "Train Loss: 0.69954\n",
      "Train Accuracy: 66.43229\n",
      "Valid Loss: 1.9215\n",
      "Valid Accuracy: 57.09635\n",
      "early_stopping_best_val: 1.82206\n",
      "early_stopping_step: 4\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 19...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 3.125e-05\n",
      "Train Loss: 0.68653\n",
      "Train Accuracy: 67.21354\n",
      "Valid Loss: 1.96852\n",
      "Valid Accuracy: 59.11458\n",
      "early_stopping_best_val: 1.82206\n",
      "early_stopping_step: 5\n",
      "stop_early: True\n"
     ]
    }
   ],
   "source": [
    "dataset    = SurnameDataset.load_df_and_make_vectorizer(df_all)\n",
    "classifier = SurnameClassifier(initial_num_channels = len(vectorizer.surname_vocab),\n",
    "                               num_classes          = len(vectorizer.nationality_vocab),\n",
    "                               num_channels         = args.num_channels,\n",
    "                               dropout_prob         = 0)\n",
    "TrainingClassifier(classifier  = classifier,\n",
    "                   args        = args,\n",
    "                   dataset     = dataset,\n",
    "                   use_weight  = True,\n",
    "                   use_dropout = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a1b02",
   "metadata": {},
   "source": [
    "### Model 2 - No weight, no dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a581844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch 0...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.98284\n",
      "Train Accuracy: 40.49479\n",
      "Valid Loss: 1.6269\n",
      "Valid Accuracy: 54.88281\n",
      "early_stopping_best_val: 100000000.0\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 1...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.42789\n",
      "Train Accuracy: 58.82812\n",
      "Valid Loss: 1.38884\n",
      "Valid Accuracy: 59.375\n",
      "early_stopping_best_val: 1.38884\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 2...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.25304\n",
      "Train Accuracy: 63.46354\n",
      "Valid Loss: 1.28807\n",
      "Valid Accuracy: 61.06771\n",
      "early_stopping_best_val: 1.28807\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 3...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.1378\n",
      "Train Accuracy: 66.10677\n",
      "Valid Loss: 1.22198\n",
      "Valid Accuracy: 64.0625\n",
      "early_stopping_best_val: 1.22198\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 4...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.03736\n",
      "Train Accuracy: 68.82812\n",
      "Valid Loss: 1.12939\n",
      "Valid Accuracy: 67.57812\n",
      "early_stopping_best_val: 1.12939\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 5...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 0.97791\n",
      "Train Accuracy: 70.79427\n",
      "Valid Loss: 1.08662\n",
      "Valid Accuracy: 69.33594\n",
      "early_stopping_best_val: 1.08662\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 6...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 0.91204\n",
      "Train Accuracy: 72.07031\n",
      "Valid Loss: 1.05545\n",
      "Valid Accuracy: 70.50781\n",
      "early_stopping_best_val: 1.05545\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 7...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 0.85697\n",
      "Train Accuracy: 73.73698\n",
      "Valid Loss: 1.03073\n",
      "Valid Accuracy: 69.85677\n",
      "early_stopping_best_val: 1.03073\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 8...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 0.80443\n",
      "Train Accuracy: 75.57292\n",
      "Valid Loss: 1.01384\n",
      "Valid Accuracy: 69.53125\n",
      "early_stopping_best_val: 1.01384\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 9...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 0.75433\n",
      "Train Accuracy: 76.75781\n",
      "Valid Loss: 0.97118\n",
      "Valid Accuracy: 72.20052\n",
      "early_stopping_best_val: 0.97118\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 10...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 0.70936\n",
      "Train Accuracy: 77.40885\n",
      "Valid Loss: 0.98312\n",
      "Valid Accuracy: 72.00521\n",
      "early_stopping_best_val: 0.97118\n",
      "early_stopping_step: 1\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 11...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 0.66548\n",
      "Train Accuracy: 78.97135\n",
      "Valid Loss: 0.98684\n",
      "Valid Accuracy: 72.00521\n",
      "early_stopping_best_val: 0.97118\n",
      "early_stopping_step: 2\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 12...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 0.57418\n",
      "Train Accuracy: 81.39323\n",
      "Valid Loss: 0.93641\n",
      "Valid Accuracy: 73.56771\n",
      "early_stopping_best_val: 0.93641\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 13...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 0.52871\n",
      "Train Accuracy: 83.15104\n",
      "Valid Loss: 0.93303\n",
      "Valid Accuracy: 73.76302\n",
      "early_stopping_best_val: 0.93303\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 14...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 0.50391\n",
      "Train Accuracy: 83.6849\n",
      "Valid Loss: 0.98753\n",
      "Valid Accuracy: 73.24219\n",
      "early_stopping_best_val: 0.93303\n",
      "early_stopping_step: 1\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 15...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.00025\n",
      "Train Loss: 0.48474\n",
      "Train Accuracy: 84.21875\n",
      "Valid Loss: 0.9695\n",
      "Valid Accuracy: 74.60938\n",
      "early_stopping_best_val: 0.93303\n",
      "early_stopping_step: 2\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 16...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.00025\n",
      "Train Loss: 0.43918\n",
      "Train Accuracy: 85.80729\n",
      "Valid Loss: 0.97975\n",
      "Valid Accuracy: 73.89323\n",
      "early_stopping_best_val: 0.93303\n",
      "early_stopping_step: 3\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 17...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.000125\n",
      "Train Loss: 0.41596\n",
      "Train Accuracy: 86.57552\n",
      "Valid Loss: 0.9959\n",
      "Valid Accuracy: 74.21875\n",
      "early_stopping_best_val: 0.93303\n",
      "early_stopping_step: 4\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 18...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.000125\n",
      "Train Loss: 0.39296\n",
      "Train Accuracy: 87.16146\n",
      "Valid Loss: 0.9918\n",
      "Valid Accuracy: 73.63281\n",
      "early_stopping_best_val: 0.93303\n",
      "early_stopping_step: 5\n",
      "stop_early: True\n"
     ]
    }
   ],
   "source": [
    "dataset    = SurnameDataset.load_df_and_make_vectorizer(df_all)\n",
    "classifier = SurnameClassifier(initial_num_channels = len(vectorizer.surname_vocab),\n",
    "                               num_classes          = len(vectorizer.nationality_vocab),\n",
    "                               num_channels         = args.num_channels,\n",
    "                               dropout_prob         = 0)\n",
    "TrainingClassifier(classifier  = classifier,\n",
    "                   args        = args,\n",
    "                   dataset     = dataset,\n",
    "                   use_weight  = False,\n",
    "                   use_dropout = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2eb81",
   "metadata": {},
   "source": [
    "### Model 3 - Use weight, use dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "006da586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch 0...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 2.59456\n",
      "Train Accuracy: 20.0\n",
      "Valid Loss: 2.1953\n",
      "Valid Accuracy: 28.0599\n",
      "early_stopping_best_val: 100000000.0\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 1...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.93544\n",
      "Train Accuracy: 35.70313\n",
      "Valid Loss: 1.98677\n",
      "Valid Accuracy: 36.65365\n",
      "early_stopping_best_val: 1.98677\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 2...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.70394\n",
      "Train Accuracy: 41.61458\n",
      "Valid Loss: 1.9236\n",
      "Valid Accuracy: 38.28125\n",
      "early_stopping_best_val: 1.9236\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 3...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.52775\n",
      "Train Accuracy: 45.52083\n",
      "Valid Loss: 1.90299\n",
      "Valid Accuracy: 43.8151\n",
      "early_stopping_best_val: 1.90299\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 4...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.407\n",
      "Train Accuracy: 48.00781\n",
      "Valid Loss: 1.81575\n",
      "Valid Accuracy: 41.08073\n",
      "early_stopping_best_val: 1.81575\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 5...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.001\n",
      "Train Loss: 1.35043\n",
      "Train Accuracy: 48.90625\n",
      "Valid Loss: 1.83109\n",
      "Valid Accuracy: 44.66146\n",
      "early_stopping_best_val: 1.81575\n",
      "early_stopping_step: 1\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 6...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 1.27443\n",
      "Train Accuracy: 51.32813\n",
      "Valid Loss: 1.90698\n",
      "Valid Accuracy: 43.6849\n",
      "early_stopping_best_val: 1.81575\n",
      "early_stopping_step: 2\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 7...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 1.08581\n",
      "Train Accuracy: 54.89583\n",
      "Valid Loss: 1.78208\n",
      "Valid Accuracy: 48.69792\n",
      "early_stopping_best_val: 1.78208\n",
      "early_stopping_step: 0\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 8...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.0005\n",
      "Train Loss: 1.02115\n",
      "Train Accuracy: 56.53646\n",
      "Valid Loss: 1.85024\n",
      "Valid Accuracy: 51.10677\n",
      "early_stopping_best_val: 1.78208\n",
      "early_stopping_step: 1\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 9...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.00025\n",
      "Train Loss: 0.97175\n",
      "Train Accuracy: 57.7474\n",
      "Valid Loss: 1.84617\n",
      "Valid Accuracy: 51.17188\n",
      "early_stopping_best_val: 1.78208\n",
      "early_stopping_step: 2\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 10...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.00025\n",
      "Train Loss: 0.88314\n",
      "Train Accuracy: 60.76823\n",
      "Valid Loss: 1.87701\n",
      "Valid Accuracy: 51.10677\n",
      "early_stopping_best_val: 1.78208\n",
      "early_stopping_step: 3\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 11...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.000125\n",
      "Train Loss: 0.85693\n",
      "Train Accuracy: 62.13542\n",
      "Valid Loss: 1.88154\n",
      "Valid Accuracy: 49.02344\n",
      "early_stopping_best_val: 1.78208\n",
      "early_stopping_step: 4\n",
      "stop_early: False\n",
      "------------------------------------------------------------\n",
      "Epoch 12...\n",
      "Training Iteration...\n",
      "Validation Iteration...\n",
      "Current lr: 0.000125\n",
      "Train Loss: 0.81525\n",
      "Train Accuracy: 61.54948\n",
      "Valid Loss: 1.86852\n",
      "Valid Accuracy: 54.6224\n",
      "early_stopping_best_val: 1.78208\n",
      "early_stopping_step: 5\n",
      "stop_early: True\n"
     ]
    }
   ],
   "source": [
    "dataset    = SurnameDataset.load_df_and_make_vectorizer(df_all)\n",
    "classifier = SurnameClassifier(initial_num_channels = len(vectorizer.surname_vocab),\n",
    "                               num_classes          = len(vectorizer.nationality_vocab),\n",
    "                               num_channels         = args.num_channels,\n",
    "                               dropout_prob         = args.dropout_p)\n",
    "TrainingClassifier(classifier  = classifier,\n",
    "                   args        = args,\n",
    "                   dataset     = dataset,\n",
    "                   use_weight  = True,\n",
    "                   use_dropout = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1cc7a",
   "metadata": {},
   "source": [
    "# @@@@@ 4. Evaluation\n",
    "## 4.1 - Evaluation on Test Data\n",
    "### A. Weighted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79a7603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.79241\n",
      "Test Accuracy: 53.77604\n"
     ]
    }
   ],
   "source": [
    "classifier_weighted = SurnameClassifier(initial_num_channels = len(vectorizer.surname_vocab),\n",
    "                                        num_classes          = len(vectorizer.nationality_vocab),\n",
    "                                        num_channels         = args.num_channels,\n",
    "                                        dropout_prob         = args.dropout_p)\n",
    "\n",
    "filename   = args.output_path+'/cnn_weighted_nodropout_model.pth'\n",
    "classifier_weighted.load_state_dict(torch.load(filename))\n",
    "classifier_weighted = classifier_weighted.to(args.device)\n",
    "\n",
    "loss_func  = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "##### Create a batch_generator using test data\n",
    "# The test set should be run as little as possible\n",
    "# Avoid make a new model decision based on the evaluation on test data\n",
    "# Otherwise the model might be biased toward the test data, and the test data will \n",
    "# become meaningless as an measure of truly held-out data.\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "\n",
    "##### Create new running loss, and running accuracy\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "##### Indicate that the model is in “evaluation mode”\n",
    "classifier_weighted.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier_weighted(batch_dict['x_data'])\n",
    "\n",
    "    # compute the loss\n",
    "    loss   = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'],args.device)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "print(\"Test loss: {:.5f}\".format(running_loss))\n",
    "print(\"Test Accuracy: {:.5f}\".format(running_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a7dcc3",
   "metadata": {},
   "source": [
    "### B. Un-weighted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b56c8825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.97421\n",
      "Test Accuracy: 72.20052\n"
     ]
    }
   ],
   "source": [
    "classifier_unweighted = SurnameClassifier(initial_num_channels = len(vectorizer.surname_vocab),\n",
    "                                          num_classes          = len(vectorizer.nationality_vocab),\n",
    "                                          num_channels         = args.num_channels,\n",
    "                                          dropout_prob         = args.dropout_p)\n",
    "filename   = args.output_path+'/cnn_unweighted_nodropout_model.pth'\n",
    "classifier_unweighted.load_state_dict(torch.load(filename))\n",
    "classifier_unweighted = classifier_unweighted.to(args.device)\n",
    "\n",
    "loss_func  = nn.CrossEntropyLoss()\n",
    "\n",
    "##### Create a batch_generator using test data\n",
    "# The test set should be run as little as possible\n",
    "# Avoid make a new model decision based on the evaluation on test data\n",
    "# Otherwise the model might be biased toward the test data, and the test data will \n",
    "# become meaningless as an measure of truly held-out data.\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "\n",
    "##### Create new running loss, and running accuracy\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "##### Indicate that the model is in “evaluation mode”\n",
    "classifier_unweighted.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier_unweighted(batch_dict['x_data'])\n",
    "\n",
    "    # compute the loss\n",
    "    loss   = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'],args.device)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "print(\"Test loss: {:.5f}\".format(running_loss))\n",
    "print(\"Test Accuracy: {:.5f}\".format(running_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc86b9",
   "metadata": {},
   "source": [
    "## 4.2 Generation a prediction for a given surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16f6ba4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Smith': 'English',\n",
       " 'Yamamoto': 'Japanese',\n",
       " 'Dubois': 'French',\n",
       " 'Rossi': 'Italian',\n",
       " 'Zhang': 'Chinese',\n",
       " 'Petrov': 'Russian',\n",
       " 'Kowalski': 'Polish',\n",
       " 'Park': 'Korean'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames = {\n",
    "    'Smith': 'English',\n",
    "    'Yamamoto': 'Japanese',\n",
    "    'Dubois': 'French',\n",
    "    'Rossi': 'Italian',\n",
    "    'Zhang':  'Chinese',\n",
    "    'Petrov':  'Russian' ,\n",
    "    'Kowalski': 'Polish',\n",
    "    'Park': 'Korean'\n",
    " }\n",
    "surnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e8762d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_surname(surname,\n",
    "                    classifier,\n",
    "                    device, \n",
    "                    vectorizer,\n",
    "                    top_k):\n",
    "    \"\"\"Predict the nationality of a surname\n",
    "    \n",
    "    Args:\n",
    "        surname (str): the text of the surname\n",
    "        classifier (SurnameClassifier): the trained model\n",
    "        device: device\n",
    "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
    "        top_k: specify the k in “top-k”\n",
    "    \"\"\"\n",
    "    \n",
    "    classifier         = classifier.to(device)\n",
    "    vectorized_surname = torch.tensor(vectorizer.vectorize(surname)).unsqueeze(dim=0)\n",
    "    output             = classifier(vectorized_surname, apply_softmax=True)\n",
    "    \n",
    "    probability_value, index = torch.topk(output, k=top_k)\n",
    "    \n",
    "    ### Predicted label \n",
    "    index = index.detach().numpy()[0]\n",
    "    \n",
    "    ### Predicted probability \n",
    "    probability_value = probability_value.detach().numpy()[0]\n",
    "    \n",
    "    results = []\n",
    "    for p, i in zip(probability_value, index):\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(i)\n",
    "        results.append({'nationality': nationality, \n",
    "                        'probability': p})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "616c9891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Shape: torch.Size([1, 77, 17])\n",
      "--------------------------------------------------------------------------------\n",
      "Output tensor([[7.3557e-11, 1.5791e-07, 3.3766e-03, 5.9650e-03, 2.7705e-01, 3.3040e-03,\n",
      "         6.8944e-01, 5.9915e-08, 2.0650e-04, 5.9280e-07, 7.3073e-09, 5.1065e-07,\n",
      "         3.0034e-05, 1.4814e-06, 2.2151e-03, 1.8353e-02, 4.0501e-05, 1.0901e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Shape: torch.Size([1, 18])\n"
     ]
    }
   ],
   "source": [
    "surname    = 'Schmidt'\n",
    "classifier = classifier_unweighted\n",
    "device     = 'cpu'\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "input      = torch.tensor(vectorizer.vectorize(surname)).unsqueeze(dim=0)\n",
    "output     = classifier(input, apply_softmax=True)\n",
    "print('Input:', input)\n",
    "print('Shape:', input.shape)\n",
    "print('-'*80)\n",
    "print('Output', output)\n",
    "print('Shape:', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7ba6988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nationality': 'German', 'probability': 0.6894437},\n",
       " {'nationality': 'English', 'probability': 0.27705163},\n",
       " {'nationality': 'Scottish', 'probability': 0.01835314},\n",
       " {'nationality': 'Dutch', 'probability': 0.0059650303},\n",
       " {'nationality': 'Czech', 'probability': 0.0033765961}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surname    = 'Schmidt'\n",
    "classifier = classifier_unweighted\n",
    "device     = 'cpu'\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "predict_surname(surname,\n",
    "                classifier,\n",
    "                device, \n",
    "                vectorizer,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0db78fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nationality': 'German', 'probability': 0.6894437}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surname    = 'Schmidt'\n",
    "classifier = classifier_unweighted\n",
    "device     = 'cpu'\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "predict_surname(surname,\n",
    "                classifier,\n",
    "                device, \n",
    "                vectorizer,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b0d73",
   "metadata": {},
   "source": [
    "### A. Weighted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0499afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surname: Smith\n",
      "nationality: English\n",
      "predicted(top k)\n",
      "[{'nationality': 'Scottish', 'probability': 0.611422}, {'nationality': 'German', 'probability': 0.13715632}, {'nationality': 'English', 'probability': 0.13082278}, {'nationality': 'Dutch', 'probability': 0.08253884}, {'nationality': 'Czech', 'probability': 0.01834927}]\n",
      "------------------------------------------------------------\n",
      "surname: Yamamoto\n",
      "nationality: Japanese\n",
      "predicted(top k)\n",
      "[{'nationality': 'Japanese', 'probability': 0.9988399}, {'nationality': 'Greek', 'probability': 0.00055738416}, {'nationality': 'Czech', 'probability': 0.00028685236}, {'nationality': 'Russian', 'probability': 0.0001600626}, {'nationality': 'Italian', 'probability': 6.7969726e-05}]\n",
      "------------------------------------------------------------\n",
      "surname: Dubois\n",
      "nationality: French\n",
      "predicted(top k)\n",
      "[{'nationality': 'Portuguese', 'probability': 0.47490096}, {'nationality': 'French', 'probability': 0.21429545}, {'nationality': 'Arabic', 'probability': 0.17322567}, {'nationality': 'Greek', 'probability': 0.041471627}, {'nationality': 'Spanish', 'probability': 0.035996065}]\n",
      "------------------------------------------------------------\n",
      "surname: Rossi\n",
      "nationality: Italian\n",
      "predicted(top k)\n",
      "[{'nationality': 'French', 'probability': 0.40915346}, {'nationality': 'Italian', 'probability': 0.28616738}, {'nationality': 'Portuguese', 'probability': 0.07881498}, {'nationality': 'English', 'probability': 0.07235599}, {'nationality': 'Spanish', 'probability': 0.07202854}]\n",
      "------------------------------------------------------------\n",
      "surname: Zhang\n",
      "nationality: Chinese\n",
      "predicted(top k)\n",
      "[{'nationality': 'Chinese', 'probability': 0.9677664}, {'nationality': 'Korean', 'probability': 0.029574983}, {'nationality': 'Vietnamese', 'probability': 0.0019714136}, {'nationality': 'Russian', 'probability': 0.00040900908}, {'nationality': 'English', 'probability': 0.0001370542}]\n",
      "------------------------------------------------------------\n",
      "surname: Petrov\n",
      "nationality: Russian\n",
      "predicted(top k)\n",
      "[{'nationality': 'Russian', 'probability': 0.64850235}, {'nationality': 'Czech', 'probability': 0.18917733}, {'nationality': 'English', 'probability': 0.078261025}, {'nationality': 'Dutch', 'probability': 0.07154895}, {'nationality': 'French', 'probability': 0.008428718}]\n",
      "------------------------------------------------------------\n",
      "surname: Kowalski\n",
      "nationality: Polish\n",
      "predicted(top k)\n",
      "[{'nationality': 'Polish', 'probability': 0.9658658}, {'nationality': 'Czech', 'probability': 0.032749627}, {'nationality': 'Russian', 'probability': 0.0006532623}, {'nationality': 'English', 'probability': 0.0006432939}, {'nationality': 'German', 'probability': 4.528251e-05}]\n",
      "------------------------------------------------------------\n",
      "surname: Park\n",
      "nationality: Korean\n",
      "predicted(top k)\n",
      "[{'nationality': 'Korean', 'probability': 0.7191787}, {'nationality': 'Czech', 'probability': 0.08616381}, {'nationality': 'German', 'probability': 0.06152884}, {'nationality': 'English', 'probability': 0.052790236}, {'nationality': 'Chinese', 'probability': 0.029792167}]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_weighted\n",
    "device     = 'cpu'\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "for s,n in surnames.items():\n",
    "    print('surname:', s)\n",
    "    print('nationality:', n)\n",
    "    print('predicted(top k)')\n",
    "    print(predict_surname(s,classifier,\n",
    "                          device,vectorizer,5))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530bf1d8",
   "metadata": {},
   "source": [
    "### B. Un-weighted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f51cc81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surname: Smith\n",
      "nationality: English\n",
      "predicted(top k)\n",
      "[{'nationality': 'English', 'probability': 0.64895386}, {'nationality': 'German', 'probability': 0.110422485}, {'nationality': 'Russian', 'probability': 0.10835821}, {'nationality': 'Scottish', 'probability': 0.049528986}, {'nationality': 'Czech', 'probability': 0.031344082}]\n",
      "------------------------------------------------------------\n",
      "surname: Yamamoto\n",
      "nationality: Japanese\n",
      "predicted(top k)\n",
      "[{'nationality': 'Japanese', 'probability': 0.9981171}, {'nationality': 'Russian', 'probability': 0.0014242005}, {'nationality': 'Greek', 'probability': 0.00040092773}, {'nationality': 'Czech', 'probability': 3.1969957e-05}, {'nationality': 'Italian', 'probability': 1.0447679e-05}]\n",
      "------------------------------------------------------------\n",
      "surname: Dubois\n",
      "nationality: French\n",
      "predicted(top k)\n",
      "[{'nationality': 'English', 'probability': 0.5398026}, {'nationality': 'French', 'probability': 0.15025407}, {'nationality': 'Portuguese', 'probability': 0.10373695}, {'nationality': 'Spanish', 'probability': 0.08948359}, {'nationality': 'Greek', 'probability': 0.04463889}]\n",
      "------------------------------------------------------------\n",
      "surname: Rossi\n",
      "nationality: Italian\n",
      "predicted(top k)\n",
      "[{'nationality': 'Italian', 'probability': 0.82932913}, {'nationality': 'English', 'probability': 0.05658858}, {'nationality': 'Russian', 'probability': 0.029171567}, {'nationality': 'Portuguese', 'probability': 0.022711663}, {'nationality': 'French', 'probability': 0.019539041}]\n",
      "------------------------------------------------------------\n",
      "surname: Zhang\n",
      "nationality: Chinese\n",
      "predicted(top k)\n",
      "[{'nationality': 'Chinese', 'probability': 0.9792533}, {'nationality': 'Vietnamese', 'probability': 0.01005157}, {'nationality': 'Korean', 'probability': 0.0067005926}, {'nationality': 'Russian', 'probability': 0.001560414}, {'nationality': 'Irish', 'probability': 0.0009889512}]\n",
      "------------------------------------------------------------\n",
      "surname: Petrov\n",
      "nationality: Russian\n",
      "predicted(top k)\n",
      "[{'nationality': 'Russian', 'probability': 0.96394145}, {'nationality': 'English', 'probability': 0.02067275}, {'nationality': 'Czech', 'probability': 0.010889209}, {'nationality': 'Dutch', 'probability': 0.002706345}, {'nationality': 'French', 'probability': 0.0009843206}]\n",
      "------------------------------------------------------------\n",
      "surname: Kowalski\n",
      "nationality: Polish\n",
      "predicted(top k)\n",
      "[{'nationality': 'Polish', 'probability': 0.9250246}, {'nationality': 'Czech', 'probability': 0.07306922}, {'nationality': 'Russian', 'probability': 0.0013768944}, {'nationality': 'Greek', 'probability': 0.00026954495}, {'nationality': 'Scottish', 'probability': 9.980661e-05}]\n",
      "------------------------------------------------------------\n",
      "surname: Park\n",
      "nationality: Korean\n",
      "predicted(top k)\n",
      "[{'nationality': 'English', 'probability': 0.5894591}, {'nationality': 'German', 'probability': 0.20682728}, {'nationality': 'Czech', 'probability': 0.09657112}, {'nationality': 'Russian', 'probability': 0.04572653}, {'nationality': 'Dutch', 'probability': 0.027396}]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_unweighted\n",
    "device     = 'cpu'\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "for s,n in surnames.items():\n",
    "    print('surname:', s)\n",
    "    print('nationality:', n)\n",
    "    print('predicted(top k)')\n",
    "    print(predict_surname(s,classifier,\n",
    "                          device,vectorizer,5))\n",
    "    print('-'*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
