{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fddef8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2f0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"surnames_with_splits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e82b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data:  (10980, 4)\n",
      "------------------------------------------------------------\n",
      "      nationality  nationality_index  split     surname\n",
      "2462        Dutch                  2   test      Klerks\n",
      "10225     Russian                 13    val  Halymbadja\n",
      "2382        Dutch                  2  train  Meeuwissen\n",
      "8089       Polish                 14  train      Smol√°k\n",
      "1050       Arabic                 15  train      Morcos\n",
      "10018     Russian                 13    val    Timashov\n",
      "1192       Arabic                 15    val      Morcos\n",
      "5763       German                  9  train       Acker\n",
      "5123      English                 12   test       Eltis\n",
      "10669     Spanish                  6  train     Montero\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the data: \", df_all.shape)\n",
    "print('-'*60)\n",
    "print(df_all.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "153524e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Vocabulary class\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            ### add a new element to _token_to_idx\n",
    "            self._token_to_idx[token] = index\n",
    "            ### add a new element to _idx_to_token\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "   \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804fc58",
   "metadata": {},
   "source": [
    "# 1. SequenceVocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd1f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, \n",
    "                 token_to_idx    = None, \n",
    "                 unk_token       = \"<UNK>\",\n",
    "                 mask_token      = \"<MASK>\", \n",
    "                 begin_seq_token = \"<BEGIN>\",\n",
    "                 end_seq_token   = \"<END>\"):\n",
    "        \n",
    "        \n",
    "        super().__init__(token_to_idx)\n",
    "        \"\"\"\n",
    "        The follow attributes have been defined in the Vocabulary class:\n",
    "            - ._token_to_idx\n",
    "            - ._idx_to_token\n",
    "        \"\"\"\n",
    "\n",
    "        self._mask_token      = mask_token      # default: \"<MASK>\"\n",
    "        self._unk_token       = unk_token       # default: \"<UNK>\"\n",
    "        self._begin_seq_token = begin_seq_token # default: \"<BEGIN>\"\n",
    "        self._end_seq_token   = end_seq_token   # default: \"<END>\"\n",
    "\n",
    "        self.mask_index       = self.add_token(self._mask_token)      # return 0\n",
    "        self.unk_index        = self.add_token(self._unk_token)       # return 1\n",
    "        self.begin_seq_index  = self.add_token(self._begin_seq_token) # return 2\n",
    "        self.end_seq_index    = self.add_token(self._end_seq_token)   # return 3\n",
    "        \n",
    "    \n",
    "    ### Overriding the self.lookup_token() method\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba2ede",
   "metadata": {},
   "source": [
    "# 2. Instantiate the SequenceVocabulary from the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40957aa",
   "metadata": {},
   "source": [
    "## (1) The vocabulary for the nationality - nationality_vocab\n",
    "### The corpus of nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c10dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus\n",
      "['Arabic' 'Chinese' 'Czech' 'Dutch' 'English' 'French' 'German' 'Greek'\n",
      " 'Irish' 'Italian' 'Japanese' 'Korean' 'Polish' 'Portuguese' 'Russian'\n",
      " 'Scottish' 'Spanish' 'Vietnamese']\n",
      "--------------------------------------------------------------------------------\n",
      "counts\n",
      "nationality\n",
      "English       2972\n",
      "Russian       2373\n",
      "Arabic        1603\n",
      "Japanese       775\n",
      "Italian        600\n",
      "German         576\n",
      "Czech          414\n",
      "Spanish        258\n",
      "Dutch          236\n",
      "French         229\n",
      "Chinese        220\n",
      "Irish          183\n",
      "Greek          156\n",
      "Polish         120\n",
      "Korean          77\n",
      "Scottish        75\n",
      "Vietnamese      58\n",
      "Portuguese      55\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('corpus')\n",
    "print(df_all.nationality.unique())\n",
    "print('-'*80)\n",
    "print('counts')\n",
    "print(df_all.nationality.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1babf99a",
   "metadata": {},
   "source": [
    "### Initializing nationality_vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2bb3f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {}, '_idx_to_token': {}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationality_vocab = Vocabulary()\n",
    "vars(nationality_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e059b8",
   "metadata": {},
   "source": [
    "### Add tokens appear in the nationality to nationality_vocab. \n",
    "### There are 18 tokens in the corpus of nationality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6485ce28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arabic',\n",
       " 'Chinese',\n",
       " 'Czech',\n",
       " 'Dutch',\n",
       " 'English',\n",
       " 'French',\n",
       " 'German',\n",
       " 'Greek',\n",
       " 'Irish',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Korean',\n",
       " 'Polish',\n",
       " 'Portuguese',\n",
       " 'Russian',\n",
       " 'Scottish',\n",
       " 'Spanish',\n",
       " 'Vietnamese']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(df_all.nationality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e13f97a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'Arabic': 0,\n",
       "  'Chinese': 1,\n",
       "  'Czech': 2,\n",
       "  'Dutch': 3,\n",
       "  'English': 4,\n",
       "  'French': 5,\n",
       "  'German': 6,\n",
       "  'Greek': 7,\n",
       "  'Irish': 8,\n",
       "  'Italian': 9,\n",
       "  'Japanese': 10,\n",
       "  'Korean': 11,\n",
       "  'Polish': 12,\n",
       "  'Portuguese': 13,\n",
       "  'Russian': 14,\n",
       "  'Scottish': 15,\n",
       "  'Spanish': 16,\n",
       "  'Vietnamese': 17},\n",
       " '_idx_to_token': {0: 'Arabic',\n",
       "  1: 'Chinese',\n",
       "  2: 'Czech',\n",
       "  3: 'Dutch',\n",
       "  4: 'English',\n",
       "  5: 'French',\n",
       "  6: 'German',\n",
       "  7: 'Greek',\n",
       "  8: 'Irish',\n",
       "  9: 'Italian',\n",
       "  10: 'Japanese',\n",
       "  11: 'Korean',\n",
       "  12: 'Polish',\n",
       "  13: 'Portuguese',\n",
       "  14: 'Russian',\n",
       "  15: 'Scottish',\n",
       "  16: 'Spanish',\n",
       "  17: 'Vietnamese'}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationality_vocab = Vocabulary()\n",
    "for n in sorted(set(df_all.nationality)):\n",
    "    nationality_vocab.add_token(n)\n",
    "vars(nationality_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd57b7",
   "metadata": {},
   "source": [
    "### Another way to add tokens to nationality_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b025b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'Arabic': 0,\n",
       "  'Chinese': 1,\n",
       "  'Czech': 2,\n",
       "  'Dutch': 3,\n",
       "  'English': 4,\n",
       "  'French': 5,\n",
       "  'German': 6,\n",
       "  'Greek': 7,\n",
       "  'Irish': 8,\n",
       "  'Italian': 9,\n",
       "  'Japanese': 10,\n",
       "  'Korean': 11,\n",
       "  'Polish': 12,\n",
       "  'Portuguese': 13,\n",
       "  'Russian': 14,\n",
       "  'Scottish': 15,\n",
       "  'Spanish': 16,\n",
       "  'Vietnamese': 17},\n",
       " '_idx_to_token': {0: 'Arabic',\n",
       "  1: 'Chinese',\n",
       "  2: 'Czech',\n",
       "  3: 'Dutch',\n",
       "  4: 'English',\n",
       "  5: 'French',\n",
       "  6: 'German',\n",
       "  7: 'Greek',\n",
       "  8: 'Irish',\n",
       "  9: 'Italian',\n",
       "  10: 'Japanese',\n",
       "  11: 'Korean',\n",
       "  12: 'Polish',\n",
       "  13: 'Portuguese',\n",
       "  14: 'Russian',\n",
       "  15: 'Scottish',\n",
       "  16: 'Spanish',\n",
       "  17: 'Vietnamese'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationality_vocab = Vocabulary()\n",
    "for index, row in df_all.iterrows():\n",
    "    nationality_vocab.add_token(row.nationality)\n",
    "vars(nationality_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d5c6a",
   "metadata": {},
   "source": [
    "### (2) The vocabulary for the surnames\n",
    "### The corpus - the difference between nationality and surname is that the In the corpus of nationality, each word is treated as a token, whereas in the corpus of surname, each character is treated as a token.\n",
    "### - Tokens in \"nationality_vocab\": 'Arabic', 'Chinese', 'Czech', 'Dutch', 'English', ...\n",
    "### - Tokens in \"surname_vocab\": 'T', 'a', 't', 'V', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f0ce837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           Totah\n",
       "1          Abboud\n",
       "2        Fakhoury\n",
       "3           Srour\n",
       "4          Sayegh\n",
       "           ...   \n",
       "10975        Dinh\n",
       "10976       Phung\n",
       "10977       Quang\n",
       "10978          Vu\n",
       "10979          Ha\n",
       "Name: surname, Length: 10980, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.surname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ddd87a",
   "metadata": {},
   "source": [
    "### Initializing surname_vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa4f6c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3},\n",
       " '_idx_to_token': {0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surname_vocab = SequenceVocabulary()\n",
    "vars(surname_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf0a7f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens is 88\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_all.iterrows():\n",
    "    for letter in row.surname:\n",
    "        surname_vocab.add_token(letter)\n",
    "print(f'The number of tokens is {len(surname_vocab)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93527692",
   "metadata": {},
   "source": [
    "# 3. Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0537983",
   "metadata": {},
   "source": [
    "### ._token_to_idx: a mapping of index and token added to the SequenceVocabulary (inherited from Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39830ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print out 20 tokens in the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<MASK>', 0),\n",
       " ('<UNK>', 1),\n",
       " ('<BEGIN>', 2),\n",
       " ('<END>', 3),\n",
       " ('T', 4),\n",
       " ('o', 5),\n",
       " ('t', 6),\n",
       " ('a', 7),\n",
       " ('h', 8),\n",
       " ('A', 9),\n",
       " ('b', 10),\n",
       " ('u', 11),\n",
       " ('d', 12),\n",
       " ('F', 13),\n",
       " ('k', 14),\n",
       " ('r', 15),\n",
       " ('y', 16),\n",
       " ('S', 17),\n",
       " ('e', 18),\n",
       " ('g', 19)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Print out 20 tokens in the vocabulary\")\n",
    "list(surname_vocab._token_to_idx.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7417ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print a few elements in surname_vocab._token_to_idx\n",
      "The index for \"a\" is 7\n",
      "The index for \"b\" is 10\n",
      "The index for \"c\" is 43\n",
      "The index for \"A\" is 9\n",
      "The index for \"B\" is 32\n",
      "The index for \"C\" is 20\n"
     ]
    }
   ],
   "source": [
    "tokens  = ['a','b','c','A','B','C']\n",
    "mapping = surname_vocab._token_to_idx\n",
    "print(\"Print a few elements in surname_vocab._token_to_idx\")\n",
    "for i in tokens:\n",
    "    print(f'The index for \"{i}\" is {mapping.get(i,0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ad486",
   "metadata": {},
   "source": [
    "### ._idx_to_token: a mapping of index and token added to the SequenceVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64b0569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print out 20 tokens in the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, '<MASK>'),\n",
       " (1, '<UNK>'),\n",
       " (2, '<BEGIN>'),\n",
       " (3, '<END>'),\n",
       " (4, 'T'),\n",
       " (5, 'o'),\n",
       " (6, 't'),\n",
       " (7, 'a'),\n",
       " (8, 'h'),\n",
       " (9, 'A'),\n",
       " (10, 'b'),\n",
       " (11, 'u'),\n",
       " (12, 'd'),\n",
       " (13, 'F'),\n",
       " (14, 'k'),\n",
       " (15, 'r'),\n",
       " (16, 'y'),\n",
       " (17, 'S'),\n",
       " (18, 'e'),\n",
       " (19, 'g')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Print out 20 tokens in the vocabulary\")\n",
    "list(surname_vocab._idx_to_token.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c97b0252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print a few elements in title_vocab._idx_to_token\n",
      "The token for index=0 is <MASK>\n",
      "The token for index=2 is <BEGIN>\n",
      "The token for index=6 is t\n",
      "The token for index=100 is 0\n"
     ]
    }
   ],
   "source": [
    "indices  = [0,2,6,100]\n",
    "mapping = surname_vocab._idx_to_token\n",
    "print(\"Print a few elements in title_vocab._idx_to_token\")\n",
    "for i in indices:\n",
    "    print(f'The token for index={i} is {mapping.get(i,0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66cef5",
   "metadata": {},
   "source": [
    "# 4. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cdc72",
   "metadata": {},
   "source": [
    "### add_token(token): Update mapping dicts based on the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40e60fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3},\n",
       " '_idx_to_token': {0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_vocab = SequenceVocabulary()\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "176db67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add one token apple\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0,\n",
       "  '<UNK>': 1,\n",
       "  '<BEGIN>': 2,\n",
       "  '<END>': 3,\n",
       "  'apple': 4},\n",
       " '_idx_to_token': {0: '<MASK>',\n",
       "  1: '<UNK>',\n",
       "  2: '<BEGIN>',\n",
       "  3: '<END>',\n",
       "  4: 'apple'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token = 'apple'\n",
    "example_vocab.add_token(new_token)\n",
    "print(f\"Add one token {new_token}\")\n",
    "print('-'*60)\n",
    "vars(example_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1ce9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add one token m\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0,\n",
       "  '<UNK>': 1,\n",
       "  '<BEGIN>': 2,\n",
       "  '<END>': 3,\n",
       "  'apple': 4,\n",
       "  'm': 5},\n",
       " '_idx_to_token': {0: '<MASK>',\n",
       "  1: '<UNK>',\n",
       "  2: '<BEGIN>',\n",
       "  3: '<END>',\n",
       "  4: 'apple',\n",
       "  5: 'm'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token = 'm'\n",
    "example_vocab.add_token(new_token)\n",
    "print(f\"Add one token {new_token}\")\n",
    "print('-'*60)\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec5871",
   "metadata": {},
   "source": [
    "### lookup_token(token): Retrieve the index associated with the token or the UNK index if token isn't present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1b5cb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3},\n",
       " '_idx_to_token': {0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_vocab = SequenceVocabulary()\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7fe5e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple added\n",
      "banana added\n",
      "peach added\n",
      "orange added\n",
      "coconut added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_token_to_idx': {'<MASK>': 0,\n",
       "  '<UNK>': 1,\n",
       "  '<BEGIN>': 2,\n",
       "  '<END>': 3,\n",
       "  'apple': 4,\n",
       "  'banana': 5,\n",
       "  'peach': 6,\n",
       "  'orange': 7,\n",
       "  'coconut': 8},\n",
       " '_idx_to_token': {0: '<MASK>',\n",
       "  1: '<UNK>',\n",
       "  2: '<BEGIN>',\n",
       "  3: '<END>',\n",
       "  4: 'apple',\n",
       "  5: 'banana',\n",
       "  6: 'peach',\n",
       "  7: 'orange',\n",
       "  8: 'coconut'},\n",
       " '_mask_token': '<MASK>',\n",
       " '_unk_token': '<UNK>',\n",
       " '_begin_seq_token': '<BEGIN>',\n",
       " '_end_seq_token': '<END>',\n",
       " 'mask_index': 0,\n",
       " 'unk_index': 1,\n",
       " 'begin_seq_index': 2,\n",
       " 'end_seq_index': 3}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_add = ['apple','banana','peach','orange','coconut']\n",
    "for i in tokens_to_add:\n",
    "    example_vocab.add_token(i)\n",
    "    print(i + ' added')\n",
    "vars(example_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7634d002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index for orange is 7\n",
      "The index for rice is 1\n"
     ]
    }
   ],
   "source": [
    "tokens_list = ['orange','rice']\n",
    "for i in tokens_list:\n",
    "    print(f\"The index for {i} is {example_vocab.lookup_token(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d2166ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index for orange is 7\n",
      "The index for rice is 1\n"
     ]
    }
   ],
   "source": [
    "### Equivalent codes\n",
    "for i in tokens_list:\n",
    "    print(f\"The index for {i} is {SequenceVocabulary.lookup_token(example_vocab,i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5e2f7",
   "metadata": {},
   "source": [
    "### lookup_index(index): Return the token associated with the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc06f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token with index=1 is <UNK>\n",
      "The token with index=4 is apple\n"
     ]
    }
   ],
   "source": [
    "indices_list = [1,4]\n",
    "for i in indices_list:\n",
    "    print(f\"The token with index={i} is {example_vocab.lookup_index(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b8800d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token with index=1 is <UNK>\n",
      "The token with index=4 is apple\n"
     ]
    }
   ],
   "source": [
    "### Equivalent codes\n",
    "for i in indices_list:\n",
    "    print(f\"The token with index={i} is {Vocabulary.lookup_index(example_vocab,i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1a705",
   "metadata": {},
   "source": [
    "### \\_\\_len\\_\\_(): Return the length of _token_to_idx (i.e, the number of tokens in the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84349006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<MASK>',\n",
       " 1: '<UNK>',\n",
       " 2: '<BEGIN>',\n",
       " 3: '<END>',\n",
       " 4: 'token1',\n",
       " 5: 'token2',\n",
       " 6: 'token3',\n",
       " 7: 'token4'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_vocab = SequenceVocabulary()\n",
    "tokens_to_add = ['token1','token2','token3','token4']\n",
    "for i in tokens_to_add:\n",
    "    example_vocab.add_token(i)\n",
    "example_vocab._idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af5eee36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
