{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data Manipulating and Visualization\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport random\n\n\n# Operating System\nimport os\nfrom datetime import datetime\n\n# Machine Learning Algorithms\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Performance metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import accuracy_score\n\n# Hyperparameter\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer, Categorical\n\n## Mathematics and Statistics\nimport scipy.stats as stats\nfrom scipy.stats import uniform\nfrom scipy.stats import loguniform\n\n# NLP related \nimport string\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem.porter import *\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-03T20:04:42.380173Z","iopub.execute_input":"2023-10-03T20:04:42.380506Z","iopub.status.idle":"2023-10-03T20:04:45.862879Z","shell.execute_reply.started":"2023-10-03T20:04:42.380481Z","shell.execute_reply":"2023-10-03T20:04:45.861807Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### [Introduction to the IMDB Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)","metadata":{}},{"cell_type":"code","source":"review_df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:04:49.671920Z","iopub.execute_input":"2023-10-03T20:04:49.672276Z","iopub.status.idle":"2023-10-03T20:04:51.472270Z","shell.execute_reply.started":"2023-10-03T20:04:49.672250Z","shell.execute_reply":"2023-10-03T20:04:51.471077Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Exploration Analysis and Data Treatment\n## 1.1 Diuplicated reviews","metadata":{}},{"cell_type":"code","source":"review_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:05:35.078238Z","iopub.execute_input":"2023-10-03T20:05:35.078698Z","iopub.status.idle":"2023-10-03T20:05:35.228251Z","shell.execute_reply.started":"2023-10-03T20:05:35.078662Z","shell.execute_reply":"2023-10-03T20:05:35.227115Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                   review sentiment\ncount                                               50000     50000\nunique                                              49582         2\ntop     Loved today's show!!! It was a variety and not...  positive\nfreq                                                    5     25000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>50000</td>\n      <td>50000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>49582</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Loved today's show!!! It was a variety and not...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>5</td>\n      <td>25000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"review_df['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:05:39.608671Z","iopub.execute_input":"2023-10-03T20:05:39.609255Z","iopub.status.idle":"2023-10-03T20:05:39.624763Z","shell.execute_reply.started":"2023-10-03T20:05:39.609211Z","shell.execute_reply":"2023-10-03T20:05:39.623978Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"sentiment\npositive    25000\nnegative    25000\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"**As we can see that are 49,582 unique reviews. That means there are duplicated reviews.**\n\n**Some of the duplicated reviews are printed below.**","metadata":{}},{"cell_type":"code","source":"duplicates = review_df['review'].duplicated(keep=False)\nreview_df.loc[duplicates,:].sort_values('review').head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:05:44.352248Z","iopub.execute_input":"2023-10-03T20:05:44.352831Z","iopub.status.idle":"2023-10-03T20:05:44.382788Z","shell.execute_reply.started":"2023-10-03T20:05:44.352803Z","shell.execute_reply":"2023-10-03T20:05:44.381568Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment\n34058  \"Go Fish\" garnered Rose Troche rightly or wron...  negative\n47467  \"Go Fish\" garnered Rose Troche rightly or wron...  negative\n29956  \"Three\" is a seriously dumb shipwreck movie. M...  negative\n31488  \"Three\" is a seriously dumb shipwreck movie. M...  negative\n47527  \"Witchery\" might just be the most incoherent a...  negative\n2976   \"Witchery\" might just be the most incoherent a...  negative\n7949   'Dead Letter Office' is a low-budget film abou...  negative\n32260  'Dead Letter Office' is a low-budget film abou...  negative\n18022  (Spoilers)<br /><br />Oh sure it's based on Mo...  negative\n22449  (Spoilers)<br /><br />Oh sure it's based on Mo...  negative","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>34058</th>\n      <td>\"Go Fish\" garnered Rose Troche rightly or wron...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>47467</th>\n      <td>\"Go Fish\" garnered Rose Troche rightly or wron...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>29956</th>\n      <td>\"Three\" is a seriously dumb shipwreck movie. M...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>31488</th>\n      <td>\"Three\" is a seriously dumb shipwreck movie. M...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>47527</th>\n      <td>\"Witchery\" might just be the most incoherent a...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2976</th>\n      <td>\"Witchery\" might just be the most incoherent a...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>7949</th>\n      <td>'Dead Letter Office' is a low-budget film abou...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>32260</th>\n      <td>'Dead Letter Office' is a low-budget film abou...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>18022</th>\n      <td>(Spoilers)&lt;br /&gt;&lt;br /&gt;Oh sure it's based on Mo...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>22449</th>\n      <td>(Spoilers)&lt;br /&gt;&lt;br /&gt;Oh sure it's based on Mo...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Drop the duplicated rows**\n- The [pandas.DataFrame.drop_duplicates](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) function is applied. ","metadata":{}},{"cell_type":"code","source":"review_df_dd = review_df.drop_duplicates(ignore_index=True)\nreview_df_dd.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:05:48.268854Z","iopub.execute_input":"2023-10-03T20:05:48.269438Z","iopub.status.idle":"2023-10-03T20:05:48.539315Z","shell.execute_reply.started":"2023-10-03T20:05:48.269393Z","shell.execute_reply":"2023-10-03T20:05:48.538278Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                   review sentiment\ncount                                               49582     49582\nunique                                              49582         2\ntop     One of the other reviewers has mentioned that ...  positive\nfreq                                                    1     24884","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>49582</td>\n      <td>49582</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>49582</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>24884</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 1.2 Data Treatment","metadata":{}},{"cell_type":"code","source":"review_df_dd.loc[11,'review']","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:05:51.366136Z","iopub.execute_input":"2023-10-03T20:05:51.366839Z","iopub.status.idle":"2023-10-03T20:05:51.374328Z","shell.execute_reply.started":"2023-10-03T20:05:51.366799Z","shell.execute_reply":"2023-10-03T20:05:51.373194Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"\"I saw this movie when I was about 12 when it came out. I recall the scariest scene was the big bird eating men dangling helplessly from parachutes right out of the air. The horror. The horror.<br /><br />As a young kid going to these cheesy B films on Saturday afternoons, I still was tired of the formula for these monster type movies that usually included the hero, a beautiful woman who might be the daughter of a professor and a happy resolution when the monster died in the end. I didn't care much for the romantic angle as a 12 year old and the predictable plots. I love them now for the unintentional humor.<br /><br />But, about a year or so later, I saw Psycho when it came out and I loved that the star, Janet Leigh, was bumped off early in the film. I sat up and took notice at that point. Since screenwriters are making up the story, make it up to be as scary as possible and not from a well-worn formula. There are no rules.\""},"metadata":{}}]},{"cell_type":"markdown","source":"**Take a look at one sample review. There are several things we need to do:**\n1. Remove the \"br/\" tags\n2. Remove stop words\n3. Remove punctuations\n4. Text Stemming ","metadata":{}},{"cell_type":"code","source":"##### Returns the lowercase string from the given string\ndef lower_string(string):\n    return string.lower()\n\n##### Remove special terms\ndef remove_special_strips(text,term):\n    return text.replace(term, \"\")\n\n##### Remove punctuations and stop words\n# 1: Tokenize the text.\n# 2: Remove tokens that are stop words or punctuations\n# (note that punctuations that are not in string.punctuation will be kept.)\n# (note that removing punctuations in numbers might change the meaning, e.g.,, \"1.5\" becomes \"15\", 100% becomes 100).\n# 3: If a sentence ends with a period, the last word and the period are combined into a single token. Remove such periods.\ndef text_treatment(text):\n    text = lower_string(text)\n    \n    text = remove_special_strips(text,'<br />')\n    \n    stop_punc = set(stopwords.words('english') + list(string.punctuation))\n    tokenizer=ToktokTokenizer()\n    filtered_tokens = [i for i in tokenizer.tokenize(text) if i not in stop_punc]\n    filtered_tokens = [i.replace('.', \"\") for i in filtered_tokens]\n    filtered_text = ' '.join(filtered_tokens) \n    return(filtered_text)\n\n##### Text stemming\n# See a comparison between two stemmers from NLTK\n# https://www.nltk.org/howto/stem.html\ndef stemmer(text):\n    sinlge_stemmer = PorterStemmer()\n    words = text.split()\n    words_stem = [sinlge_stemmer.stem(i) for i in words]\n    text_stem  = ' '.join(words_stem)\n    return text_stem","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:05:53.880998Z","iopub.execute_input":"2023-10-03T20:05:53.881406Z","iopub.status.idle":"2023-10-03T20:05:53.890250Z","shell.execute_reply.started":"2023-10-03T20:05:53.881378Z","shell.execute_reply":"2023-10-03T20:05:53.888764Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"[Text Stemmers from NLTK](https://www.nltk.org/howto/stem.html)\n\n**Remark:** \n- **An alternative to stemming is lemmatizing (not used for this study). Lemmatization produces a linguistically valid word while stemming is faster but may generate non-words. Meanwhile, lemmatization is computationally expensive since it involves look-up tables and what not.**\n- **In this study, PorterStemmer is used. The codes and results below provide a comparison between two stemmers from NLTK. [Also see the NLTK reference here](https://www.nltk.org/howto/stem.html).**","metadata":{}},{"cell_type":"code","source":"words = ['caresses', 'flies', 'denied', 'agreed','meeting', 'stating', 'sensational', \n           'reference', 'colonizer','plotted','running','generously','happily','successfully']\n\ncompare_stemmers = pd.DataFrame(columns=['Original','Porter','Snowball'],index=range(len(words)))\ncompare_stemmers['Original'] = words\n\nsinlge_stemmer = PorterStemmer()\nsingles = [sinlge_stemmer.stem(i) for i in words]\n\nsnowball_stemmer = SnowballStemmer(\"english\")\nsnowballs = [snowball_stemmer.stem(i) for i in words]\n\ncompare_stemmers['Porter']   = singles\ncompare_stemmers['Snowball'] = snowballs\n\ncompare_stemmers","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:05:56.394921Z","iopub.execute_input":"2023-10-03T20:05:56.395250Z","iopub.status.idle":"2023-10-03T20:05:56.412617Z","shell.execute_reply.started":"2023-10-03T20:05:56.395226Z","shell.execute_reply":"2023-10-03T20:05:56.411458Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"        Original   Porter  Snowball\n0       caresses   caress    caress\n1          flies      fli       fli\n2         denied     deni      deni\n3         agreed     agre      agre\n4        meeting     meet      meet\n5        stating    state     state\n6    sensational   sensat    sensat\n7      reference    refer     refer\n8      colonizer    colon     colon\n9        plotted     plot      plot\n10       running      run       run\n11    generously    gener  generous\n12       happily  happili   happili\n13  successfully  success   success","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Original</th>\n      <th>Porter</th>\n      <th>Snowball</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>caresses</td>\n      <td>caress</td>\n      <td>caress</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>flies</td>\n      <td>fli</td>\n      <td>fli</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>denied</td>\n      <td>deni</td>\n      <td>deni</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>agreed</td>\n      <td>agre</td>\n      <td>agre</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>meeting</td>\n      <td>meet</td>\n      <td>meet</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>stating</td>\n      <td>state</td>\n      <td>state</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>sensational</td>\n      <td>sensat</td>\n      <td>sensat</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>reference</td>\n      <td>refer</td>\n      <td>refer</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>colonizer</td>\n      <td>colon</td>\n      <td>colon</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>plotted</td>\n      <td>plot</td>\n      <td>plot</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>running</td>\n      <td>run</td>\n      <td>run</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>generously</td>\n      <td>gener</td>\n      <td>generous</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>happily</td>\n      <td>happili</td>\n      <td>happili</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>successfully</td>\n      <td>success</td>\n      <td>success</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Compare the texts before and after these treatments. A review randomly selected from the first 100 rows.**","metadata":{}},{"cell_type":"code","source":"a = np.random.randint(100)\nprint('Review '+str(a))\ntext = review_df_dd.loc[a,'review']\nprint('------original text------')\nprint(text)\nprint()\n\nprint('------after removing punctuations and stop words------')\ntext = text_treatment(text)\nprint(text)\nprint()\n\nprint('------after stemming------')\ntext= stemmer(text)\nprint(text)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:06:00.823895Z","iopub.execute_input":"2023-10-03T20:06:00.824398Z","iopub.status.idle":"2023-10-03T20:06:00.841060Z","shell.execute_reply.started":"2023-10-03T20:06:00.824365Z","shell.execute_reply":"2023-10-03T20:06:00.839639Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Review 79\n------original text------\nThis film took me by surprise. I make it a habit of finding out as little as possible about films before attending because trailers and reviews provide spoiler after spoiler. All I knew upon entering the theater is that it was a documentary about a long married couple and that IMDb readers gave it a 7.8, Rotten Tomatoes users ranked it at 7.9 and the critics averaged an amazing 8.2! If anything, they UNDERRATED this little gem.<br /><br />Filmmaker Doug Block decided to record his parents \"for posterity\" and at the beginning of the film we are treated to the requisite interviews with his parents, outspoken mother Mina, and less than forthcoming dad, Mike. I immediately found this couple interesting and had no idea where the filmmaker (Mike & Mina's son Doug) was going to take us. As a matter of fact, I doubt that Doug himself knew where he was going with this!<br /><br />Life takes unexpected twists and turns and this beautifully expressive film follows the journey. It is difficult to verbalize just how moved I was with this story and the unique way in which it was told. Absolutely riveting from beginning to end and it really is a must-see even if you aren't a fan of the documentary genre. This film will make you think of your own life and might even evoke memories that you thought were long forgotten. \"51 Birch Street\" is one of those rare filmgoing experiences that makes a deep impression and never leaves you. The best news of all is that HBO had a hand in the production so instead of playing to a limited art house audience, eventually, millions of people will have a chance to view this incredible piece of work. BRAVO!!!!!!!!\n\n------after removing punctuations and stop words------\nfilm took surprise make habit finding little possible films attending trailers reviews provide spoiler spoiler knew upon entering theater documentary long married couple imdb readers gave 78 rotten tomatoes users ranked 79 critics averaged amazing 82 anything underrated little gemfilmmaker doug block decided record parents posterity beginning film treated requisite interviews parents outspoken mother mina less forthcoming dad mike immediately found couple interesting idea filmmaker mike &amp; mina son doug going take us matter fact doubt doug knew going life takes unexpected twists turns beautifully expressive film follows journey difficult verbalize moved story unique way told absolutely riveting beginning end really must-see even fan documentary genre film make think life might even evoke memories thought long forgotten 51 birch street one rare filmgoing experiences makes deep impression never leaves you best news hbo hand production instead playing limited art house audience eventually millions people chance view incredible piece work bravo\n\n------after stemming------\nfilm took surpris make habit find littl possibl film attend trailer review provid spoiler spoiler knew upon enter theater documentari long marri coupl imdb reader gave 78 rotten tomato user rank 79 critic averag amaz 82 anyth underr littl gemfilmmak doug block decid record parent poster begin film treat requisit interview parent outspoken mother mina less forthcom dad mike immedi found coupl interest idea filmmak mike &amp; mina son doug go take us matter fact doubt doug knew go life take unexpect twist turn beauti express film follow journey difficult verbal move stori uniqu way told absolut rivet begin end realli must-se even fan documentari genr film make think life might even evok memori thought long forgotten 51 birch street one rare filmgo experi make deep impress never leav you best news hbo hand product instead play limit art hous audienc eventu million peopl chanc view incred piec work bravo\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Apply the function to the entire review dataframe, and save the data**","metadata":{}},{"cell_type":"code","source":"review_df_model = review_df_dd.copy()\nreview_df_model['review']=review_df_model['review'].apply(text_treatment)\nreview_df_model['review']=review_df_model['review'].apply(stemmer)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:06:03.555885Z","iopub.execute_input":"2023-10-03T20:06:03.556538Z","iopub.status.idle":"2023-10-03T20:08:34.911518Z","shell.execute_reply.started":"2023-10-03T20:06:03.556498Z","shell.execute_reply":"2023-10-03T20:08:34.910246Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# 2 Creating TFIDF features\n- An alternative is **Bag of Words** (BoW) simply counts the frequency of words in a document. Thus the vector for a document has the frequency of each word in the corpus for that document. The key difference between bag of words and TF-IDF is that the former does not incorporate any sort of inverse document frequency (IDF)  and is only a frequency count (TF).\n- The [sklearn.feature_extraction.text.TfidfVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) function is used for creating the TF-IDF features.\n- See more about TFIDF in [this notebook](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/IMDB_Reviews/tfidf.ipynb)","metadata":{}},{"cell_type":"markdown","source":"#### Number of reviews in the data: 49,582","metadata":{}},{"cell_type":"code","source":"corpus= list(review_df_model['review'])\nlen(corpus)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:09:29.985306Z","iopub.execute_input":"2023-10-03T20:09:29.985651Z","iopub.status.idle":"2023-10-03T20:09:30.003099Z","shell.execute_reply.started":"2023-10-03T20:09:29.985626Z","shell.execute_reply":"2023-10-03T20:09:30.001605Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"49582"},"metadata":{}}]},{"cell_type":"markdown","source":"#### The total number of words contained in all reviews' vocabulary：130,312","metadata":{}},{"cell_type":"code","source":"##### tfidf_0 is a TfidfVectorizer with no constraints\ntfidf_0 = TfidfVectorizer()\n##### fit the TfidfVectorizer using corpus\nresult  = tfidf_0.fit_transform(corpus)\n##### number of words(tokens) in the vocabulary\nvocabulary = tfidf_0.vocabulary_\nprint(\"There are %d words in the corpus.\" %len(vocabulary))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:09:32.303883Z","iopub.execute_input":"2023-10-03T20:09:32.304263Z","iopub.status.idle":"2023-10-03T20:09:38.127227Z","shell.execute_reply.started":"2023-10-03T20:09:32.304232Z","shell.execute_reply":"2023-10-03T20:09:38.125801Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"There are 130312 words in the corpus.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Parameters for creating TFIDF features\n- Without these parameters, all the 130,312 words in the vocabulary will be considered, which is not optimal.\n- **max_features**: If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. Otherwise, all features are used.\n- **min_df**: Ignore terms that have a document frequency strictly lower than the given threshold. If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. ","metadata":{}},{"cell_type":"code","source":"def create_tfidf_df(max_feature,min_df,corpus):\n    Vectorizer = TfidfVectorizer(max_features=max_feature,min_df=min_df)\n    result     = Vectorizer.fit_transform(corpus)\n    tfidf_df   = pd.DataFrame(result.toarray(),columns=Vectorizer.get_feature_names_out())\n    return(tfidf_df)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:10:08.191915Z","iopub.execute_input":"2023-10-03T20:10:08.192405Z","iopub.status.idle":"2023-10-03T20:10:08.200620Z","shell.execute_reply.started":"2023-10-03T20:10:08.192367Z","shell.execute_reply":"2023-10-03T20:10:08.198924Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Define 12 **TfidfVectorizer()** using different combinations of the two parameters\n* #### *max_features*: 10000, 5000, 2000 \n* #### *min_df*: 0(i.e., no limit), 0,01, 0.02, 0.1","metadata":{"execution":{"iopub.status.busy":"2023-09-23T23:57:59.501814Z","iopub.execute_input":"2023-09-23T23:57:59.502231Z","iopub.status.idle":"2023-09-23T23:58:08.752826Z","shell.execute_reply.started":"2023-09-23T23:57:59.502199Z","shell.execute_reply":"2023-09-23T23:58:08.751329Z"}}},{"cell_type":"code","source":"tfidf_df_1  = create_tfidf_df(10000,0,corpus)\ntfidf_df_2  = create_tfidf_df(10000,0.01,corpus)\ntfidf_df_3  = create_tfidf_df(10000,0.02,corpus)\ntfidf_df_4  = create_tfidf_df(10000,0.1,corpus)\ntfidf_df_5  = create_tfidf_df(5000,0,corpus)\ntfidf_df_6  = create_tfidf_df(5000,0.01,corpus)\ntfidf_df_7  = create_tfidf_df(5000,0.02,corpus)\ntfidf_df_8  = create_tfidf_df(5000,0.1,corpus)\ntfidf_df_9  = create_tfidf_df(2000,0,corpus)\ntfidf_df_10 = create_tfidf_df(2000,0.01,corpus)\ntfidf_df_11 = create_tfidf_df(2000,0.02,corpus)\ntfidf_df_12 = create_tfidf_df(2000,0.1,corpus)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:10:11.546749Z","iopub.execute_input":"2023-10-03T20:10:11.547239Z","iopub.status.idle":"2023-10-03T20:11:38.122055Z","shell.execute_reply.started":"2023-10-03T20:10:11.547209Z","shell.execute_reply":"2023-10-03T20:11:38.120848Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### The numbers of TFIDF features using different combinations of max_features and min_df are shown below. The results imply that if we fix the min_df, it does not matter whether we consider the top 10000, 5000, or 2000 features.\n\n#### Going forward, we will use the top 2000 TFIDF features with no min_df constraints, and conduct thefeature selection from there. ","metadata":{}},{"cell_type":"code","source":"t = pd.DataFrame(columns = ['max_feature','min_df','N_of_columns'])\nt.loc[1,:]  = [10000,'no limit',tfidf_df_1.shape[1]]             \nt.loc[2,:]  = [10000,'0.01',    tfidf_df_2.shape[1]]            \nt.loc[3,:]  = [10000,'0.02',    tfidf_df_3.shape[1]]            \nt.loc[4,:]  = [10000,'0.1',     tfidf_df_4.shape[1]]            \nt.loc[5,:]  = [5000,'no limit', tfidf_df_5.shape[1]]     \nt.loc[6,:]  = [5000,'0.01',     tfidf_df_6.shape[1]]        \nt.loc[7,:]  = [5000,'0.02',     tfidf_df_7.shape[1]]            \nt.loc[8,:]  = [5000,'0.1',      tfidf_df_8.shape[1]]            \nt.loc[9,:]  = [2000,'no limit', tfidf_df_9.shape[1]]            \nt.loc[10,:] = [2000,'0.01',     tfidf_df_10.shape[1]]            \nt.loc[11,:] = [2000,'0.02',     tfidf_df_11.shape[1]]            \nt.loc[12,:] = [2000,'0.1',      tfidf_df_12.shape[1]]            \nt.sort_values(by='N_of_columns', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:12:00.283173Z","iopub.execute_input":"2023-10-03T20:12:00.283576Z","iopub.status.idle":"2023-10-03T20:12:00.310115Z","shell.execute_reply.started":"2023-10-03T20:12:00.283550Z","shell.execute_reply":"2023-10-03T20:12:00.308831Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"   max_feature    min_df N_of_columns\n1        10000  no limit        10000\n5         5000  no limit         5000\n9         2000  no limit         2000\n2        10000      0.01         1645\n6         5000      0.01         1645\n10        2000      0.01         1645\n3        10000      0.02          933\n7         5000      0.02          933\n11        2000      0.02          933\n4        10000       0.1          137\n8         5000       0.1          137\n12        2000       0.1          137","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>max_feature</th>\n      <th>min_df</th>\n      <th>N_of_columns</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>10000</td>\n      <td>no limit</td>\n      <td>10000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5000</td>\n      <td>no limit</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2000</td>\n      <td>no limit</td>\n      <td>2000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10000</td>\n      <td>0.01</td>\n      <td>1645</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5000</td>\n      <td>0.01</td>\n      <td>1645</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2000</td>\n      <td>0.01</td>\n      <td>1645</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10000</td>\n      <td>0.02</td>\n      <td>933</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>5000</td>\n      <td>0.02</td>\n      <td>933</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2000</td>\n      <td>0.02</td>\n      <td>933</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10000</td>\n      <td>0.1</td>\n      <td>137</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>5000</td>\n      <td>0.1</td>\n      <td>137</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2000</td>\n      <td>0.1</td>\n      <td>137</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Convert the label of 'sentiment' into a 0-1 variable: \"positive\" = 1, \"negative\" = 0.\n### Note that in the TFIDF dataframe there is one column called \"sentiment\", and one column called \"review\", which are same with the first and the second columns in the original review_df data. Need to keep that in mind and avoid duplicated column names.","metadata":{"execution":{"iopub.status.busy":"2023-09-23T23:32:43.541559Z","iopub.execute_input":"2023-09-23T23:32:43.542731Z","iopub.status.idle":"2023-09-23T23:32:43.549265Z","shell.execute_reply.started":"2023-09-23T23:32:43.542668Z","shell.execute_reply":"2023-09-23T23:32:43.548286Z"}}},{"cell_type":"code","source":"review_df_model = review_df_model.rename(columns={'review':'movie_review'}) \nreview_df_model['sentiment_label']  = review_df_model['sentiment']\n\n### Convert the label of 'sentiment' into a 0-1 variable: \"positive\" = 1, \"negative\" = 0.\nreview_df_model['sentiment_number'] = (review_df_model['sentiment'] == 'positive').astype(int)\n\n### Drop the \"sentiment\" column. \nreview_df_model.drop(['sentiment'],axis=1,inplace=True)\n\n### Merge the TFIDF features\nModeling_Date = pd.concat([review_df_model,tfidf_df_9],axis=1)\n\n### print 3 rows \nModeling_Date.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:12:02.780616Z","iopub.execute_input":"2023-10-03T20:12:02.781274Z","iopub.status.idle":"2023-10-03T20:12:04.751221Z","shell.execute_reply.started":"2023-10-03T20:12:02.781238Z","shell.execute_reply":"2023-10-03T20:12:04.750092Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                        movie_review sentiment_label  \\\n0  one review mention watch 1 oz episod hook righ...        positive   \n1  wonder littl product film techniqu unassuming-...        positive   \n2  thought wonder way spend time hot summer weeke...        positive   \n\n   sentiment_number  000   10  100   11   12   13   15  ...  yeah      year  \\\n0                 1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0  0.000000   \n1                 1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0  0.000000   \n2                 1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0  0.075983   \n\n   yet  york  you     young  younger  youth  zero  zombi  \n0  0.0   0.0  0.0  0.000000      0.0    0.0   0.0    0.0  \n1  0.0   0.0  0.0  0.000000      0.0    0.0   0.0    0.0  \n2  0.0   0.0  0.0  0.093514      0.0    0.0   0.0    0.0  \n\n[3 rows x 2003 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie_review</th>\n      <th>sentiment_label</th>\n      <th>sentiment_number</th>\n      <th>000</th>\n      <th>10</th>\n      <th>100</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>15</th>\n      <th>...</th>\n      <th>yeah</th>\n      <th>year</th>\n      <th>yet</th>\n      <th>york</th>\n      <th>you</th>\n      <th>young</th>\n      <th>younger</th>\n      <th>youth</th>\n      <th>zero</th>\n      <th>zombi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one review mention watch 1 oz episod hook righ...</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wonder littl product film techniqu unassuming-...</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>thought wonder way spend time hot summer weeke...</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.075983</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.093514</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 2003 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"Modeling_Date.to_csv('/kaggle/working/Modeling_Date.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:12:10.742674Z","iopub.execute_input":"2023-10-03T20:12:10.743078Z","iopub.status.idle":"2023-10-03T20:13:20.916746Z","shell.execute_reply.started":"2023-10-03T20:12:10.743050Z","shell.execute_reply":"2023-10-03T20:13:20.915540Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"X_Data = Modeling_Date.drop(['movie_review','sentiment_label','sentiment_number'], axis=1)\nY_Data = Modeling_Date.sentiment_number\nx1,x2,y1,y2 = train_test_split(X_Data,Y_Data,test_size = 0.3,random_state = 42)\n\nparams_LGB= {'boosting_type'    : 'gbdt',\n             'objective'        : 'binary',\n             'colsample_bytree' : 0.8,\n             'learning_rate'    : 0.05,\n             'min_child_samples': 10,\n             'min_child_weight' : 5,\n             'max_depth'        : -1,\n             'min_split_gain'   : 0,\n             'num_leaves'       : 31,\n             'subsample_for_bin': 50000,\n             'subsample_freq'   : 1\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:13:25.142969Z","iopub.execute_input":"2023-10-03T20:13:25.143364Z","iopub.status.idle":"2023-10-03T20:13:26.777491Z","shell.execute_reply.started":"2023-10-03T20:13:25.143337Z","shell.execute_reply":"2023-10-03T20:13:26.776153Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"LGB_2000 = lgb.LGBMClassifier(**params_LGB,importance_type='gain')\nLGB_2000.fit(X = x1, y = y1,\n             eval_metric=['auc','logloss'], eval_set=[(x1,y1),(x2,y2)],\n             callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\nimportance_df = pd.DataFrame(list(x1)).rename(columns={0:'Features'})\nimportance_df['importance'] = LGB_2000.feature_importances_\nimportance_df = importance_df.sort_values(by=['importance'],ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:13:29.515548Z","iopub.execute_input":"2023-10-03T20:13:29.515936Z","iopub.status.idle":"2023-10-03T20:13:55.873463Z","shell.execute_reply.started":"2023-10-03T20:13:29.515909Z","shell.execute_reply":"2023-10-03T20:13:55.872122Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's auc: 0.938437\ttraining's binary_logloss: 0.35314\tvalid_1's auc: 0.912574\tvalid_1's binary_logloss: 0.39339\n","output_type":"stream"}]},{"cell_type":"code","source":"importance_df","metadata":{"execution":{"iopub.status.busy":"2023-10-03T20:14:02.466442Z","iopub.execute_input":"2023-10-03T20:14:02.466776Z","iopub.status.idle":"2023-10-03T20:14:02.477628Z","shell.execute_reply.started":"2023-10-03T20:14:02.466750Z","shell.execute_reply":"2023-10-03T20:14:02.476852Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"     Features    importance\n160       bad  25628.850548\n1978    worst  20144.535370\n1923     wast  17687.451521\n797     great  10678.459848\n151        aw   9401.900133\n...       ...           ...\n703    follow      0.000000\n702      folk      0.000000\n701     focus      0.000000\n700      focu      0.000000\n1000    known      0.000000\n\n[2000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Features</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>160</th>\n      <td>bad</td>\n      <td>25628.850548</td>\n    </tr>\n    <tr>\n      <th>1978</th>\n      <td>worst</td>\n      <td>20144.535370</td>\n    </tr>\n    <tr>\n      <th>1923</th>\n      <td>wast</td>\n      <td>17687.451521</td>\n    </tr>\n    <tr>\n      <th>797</th>\n      <td>great</td>\n      <td>10678.459848</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>aw</td>\n      <td>9401.900133</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>follow</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>folk</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>focus</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>700</th>\n      <td>focu</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1000</th>\n      <td>known</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 2 columns</p>\n</div>"},"metadata":{}}]}]}
