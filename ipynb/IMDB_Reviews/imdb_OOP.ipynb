{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8130928,"sourceType":"datasetVersion","datasetId":4804038}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <a id=\"outlines\">Outlines:\n## <a href=\"#libraries\"><b>1. Libraries\n## <a href=\"#arguments\"><b>2. Global Arguments\n## <a href=\"#helper_functions\"><b>3. Helper Functions\n## <a href=\"#classes\"><b>4. Classes:\n### - <a href=\"#class_data_analysis\"><b>Class Data_Analysis()\n### - <a href=\"#class_x_data_treatment\"><b>Class X_Data_Treatment()\n### - <a href=\"#class_dataset\"><b>Class Dataset()\n### - <a href=\"#class_trainlgbclassifier\"><b>Class TrainLGBClassifier()\n### - <a href=\"#class_fs_ir\"><b>Class IterativeReduction()\n### - <a href=\"#class_fs_boruta\"><b>Class Boruta()\n## <a href=\"#prepare_data\"><b>5. Prepare Data    \n## <a href=\"#feature_selection\"><b>6. Feature Selection\n### - <a href=\"#run_iterative_reduction\"><b>6.1 Iterative Reduction Approach\n### - <a href=\"#run_boruta\"><b>6.2 Boruta Approach\n### - <a href=\"#model_performance\"><b>6.3 Model Performance","metadata":{}},{"cell_type":"markdown","source":"<a id=\"libraries\"></a>\n<b><h1>1. Libraries</h1>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{"execution":{"iopub.status.busy":"2024-04-14T02:52:33.377989Z","iopub.execute_input":"2024-04-14T02:52:33.378601Z","iopub.status.idle":"2024-04-14T02:52:33.384938Z","shell.execute_reply.started":"2024-04-14T02:52:33.378567Z","shell.execute_reply":"2024-04-14T02:52:33.383584Z"}}},{"cell_type":"code","source":"# Data Manipulating and Visualization\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport random\nimport joblib\n\n# Operating System\nimport os\nfrom datetime import datetime\nimport warnings\nimport pickle\nfrom argparse import Namespace\n\n# Machine Learning Algorithms\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Performance metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import accuracy_score\n\n# Hyperparameter\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer, Categorical\n\n## Mathematics and Statistics\nimport scipy.stats as stats\nfrom scipy.stats import uniform\nfrom scipy.stats import loguniform\n\n# NLP related \nimport string\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem.porter import *\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:23.294133Z","iopub.execute_input":"2024-04-18T14:43:23.294835Z","iopub.status.idle":"2024-04-18T14:43:25.796012Z","shell.execute_reply.started":"2024-04-18T14:43:23.294788Z","shell.execute_reply":"2024-04-18T14:43:25.794842Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"arguments\"></a>\n<b><h1>2. Global Arguments</h1>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{}},{"cell_type":"code","source":"args = Namespace(\n    ##### File names and path names\n    input_file_name     = '/kaggle/input/imdb-data/IMDb_dataset.csv',\n    output_path         = '/kaggle/working/',\n    model_data_name     = '/kaggle/input/imdb-data/Modeling_Data.csv',\n    IR_results_filename = '/kaggle/input/imdb-outputs/IR_results.pkl',\n    Boruta_results_filename = '/kaggle/input/imdb-outputs/Boruta_results.pkl',\n    \n    ##### Features\n    target_feature     = 'sentiment_number',\n    \n    ##### For generating TFIDF features\n    feature_tfidf      = 'movie_review',\n    feature_tfidf_abbr = 'rv',\n    max_features       = 2000,\n    min_df             = 0.01,\n    \n    ##### Data Treatment\n    missing_th         = 0.5,\n    \n    ##### Training\n    test_size          = 0.3,\n    partition_seed     = 42,\n    metric             = ['logloss','auc'],\n    params_LGB         = {'boosting_type'    : 'gbdt',\n                          'objective'        : 'binary',\n                          'colsample_bytree' : 0.2,\n                          'learning_rate'    : 0.05,\n                          'min_child_samples': 10,\n                          'min_child_weight' : 5,\n                          'max_depth'        : -1,\n                          'min_split_gain'   : 0,\n                          'num_leaves'       : 31,\n                          'subsample_for_bin': 50000,\n                          'subsample_freq'   : 1,\n                          'n_estimators'     : 5000}\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:25.797601Z","iopub.execute_input":"2024-04-18T14:43:25.798151Z","iopub.status.idle":"2024-04-18T14:43:25.808204Z","shell.execute_reply.started":"2024-04-18T14:43:25.798120Z","shell.execute_reply":"2024-04-18T14:43:25.806793Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"helper_functions\"></a>\n<b><h1>3. Helper Functions</h1>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{"execution":{"iopub.status.busy":"2024-04-14T03:05:49.658032Z","iopub.execute_input":"2024-04-14T03:05:49.658426Z","iopub.status.idle":"2024-04-14T03:05:49.665299Z","shell.execute_reply.started":"2024-04-14T03:05:49.658395Z","shell.execute_reply":"2024-04-14T03:05:49.663999Z"}}},{"cell_type":"code","source":"def print_description(text,n=100):\n    \"\"\"\n    This function prints a description.\n    Attribute: \n            text (str) - a text to print\n    \"\"\"\n    print(text)\n    print('-'*n)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:25.810041Z","iopub.execute_input":"2024-04-18T14:43:25.810525Z","iopub.status.idle":"2024-04-18T14:43:25.821640Z","shell.execute_reply.started":"2024-04-18T14:43:25.810483Z","shell.execute_reply":"2024-04-18T14:43:25.819772Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def test_nonrepeating_ascending_list(_list,_min,_max):\n    \"\"\"\n    This function is used in Boruta feature selection. \n    It tests whether the integers in _list are non-repeating, \n    sorted in ascending order, and within the range of [a, b].\n    \n    Attribute: \n            _list (a list of integer) - the list of interest\n            _min (integer) - lower bound\n            _max (integer) - upper bound\n    \n    Return:\n            True / False\n    \"\"\"\n    if len(_list) != len(set(_list)):\n        print('List elements are repeating')\n        return False\n    \n    _list_sorted = sorted(_list)\n    if _list_sorted != _list:\n        print('List elements are not ascending')\n        return False\n    \n    if max(_list)>_max or min(_list)<_min:\n        print('Some elements are out of valid range')\n        return False\n    \n    return True","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:25.824625Z","iopub.execute_input":"2024-04-18T14:43:25.824971Z","iopub.status.idle":"2024-04-18T14:43:25.832846Z","shell.execute_reply.started":"2024-04-18T14:43:25.824944Z","shell.execute_reply":"2024-04-18T14:43:25.831633Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"classes\"></a>\n<b><h1>4. Classes</h1>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"class_data_analysis\"></a>\n<h2>Class Data_Analysis()</h2>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{}},{"cell_type":"code","source":"class Data_Analysis():\n    \"\"\"\n    This class is used to conduct data analysis for a given dataset.\n    \"\"\"\n    \n    def __init__(self):\n        self.data_of_interest       = None\n        self.descriptive_statistics = None\n        print_description(\"Data_Analysis object is created\")\n    \n    @staticmethod\n    def get_missing_values(data):\n        \"\"\"\n        This function gets missing value information.\n        Attribute: \n            data (DataFrame) - the data of interest\n        Return: \n            (DataFrame) - inforamtion about missing value for all columns\n        \"\"\"\n        missing_info = data.apply(lambda x:x.isna().sum()).reset_index()\n        missing_info.columns = ['feature','n_miss']\n        missing_info['n_non_missing'] = data.apply(lambda x:x.count()).values\n        missing_info['n'] = missing_info['n_non_missing'] + missing_info['n_miss']\n        missing_info['p_miss'] = missing_info['n_miss'] / missing_info['n']\n        return missing_info\n    \n    def get_descriptive_statistics(self, data):\n        \"\"\"\n        This function gets missing value information.\n        Attribute: \n            data (DataFrame) - the data of interest\n        Return: \n            self.descriptive_statistics - updated \n        \"\"\"\n        self.data_of_interest = data\n        ds_df = self.get_missing_values(data)\n        ds_df['Type']     = data.dtypes.values\n        ds_df['N_unique'] = data.nunique().values\n        ds_df['Mean']     = data.apply(lambda x: x.mean() if x.dtype!='O' else pd.NA).values\n        \n        sample = data.sample(n=5,replace=True)\n        for i in range(data.shape[1]):\n            feature = data.columns[i]\n            ds_df.loc[i,'sample'] = ','.join(sample[feature].astype(str))\n        \n        self.descriptive_statistics = ds_df\n        print_description(\"Descriptive statistics are created\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:25.834877Z","iopub.execute_input":"2024-04-18T14:43:25.835478Z","iopub.status.idle":"2024-04-18T14:43:25.852002Z","shell.execute_reply.started":"2024-04-18T14:43:25.835401Z","shell.execute_reply":"2024-04-18T14:43:25.850651Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"class_x_data_treatment\"></a>\n<h2>Class X_Data_Treatment()</h2>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{}},{"cell_type":"code","source":"class X_Data_Treatment():\n    \"\"\"\n    This class is used to conduct data manipulation in the X_Data.\n        - missing data imputation\n        - create TFIDF features based on a given text feature in X_Data\n    \"\"\"\n    \n    def __init__(self, data_original):\n        self.data_original  = data_original\n        self.run_imputation = False \n        self.data_imputed   = data_original\n        self.run_tfidf      = False\n        self.data_tfidf     = None\n        print_description(\"Data_Treatment object is created\")\n    \n    def fill_na(self, strategy):\n        \"\"\"\n        This function conducts missing value imputation.\n        Attribute: \n            strategy (str) - a given strategy: 'mode','mean','median','none'\n        Return: \n            self.imputation - updated\n            self.data_imputed - updated\n        \"\"\"\n        data_to_fillna = self.data_original\n        \n        if strategy == 'mode':\n            data_filled = data_to_fillna.apply(lambda x:x.fillna(x.mode()[0]))\n        elif strategy == 'mean':\n            data_filled = data_to_fillna.apply(lambda x:x.fillna(x.mean()))\n        elif strategy == 'median':\n            data_filled = data_to_fillna.apply(lambda x:x.fillna(x.median()))\n        elif strategy == 'none':\n            data_filled = data_to_fillna\n            if data_to_fillna.isna().sum().any():\n                print_description(\"Warning - missing value detected, but not imputed\")\n        else:\n            print_description(\"Error - imputation strategy is not correctly specified\")\n\n        self.imputation      = True\n        self.data_imputed    = data_filled\n    \n    def create_tfidf_features(self, feature, feature_abbr, args):\n        \"\"\"\n        This function creates TFIDF features with a given feature. \n        Attribute:\n            feature (str) - the name for feature used for TFIDF process\n            feature_abbr (str) - an abbreviation for feature that will be \n                                  used in TFIDF feature names\n            args - global arguments    \n        Return: \n            self.data_tfidf - updated\n        \"\"\"\n        def lower_string(string):\n            return string.lower()\n        \n        def remove_special_strips(text,term):\n            return text.replace(term, \"\")\n        \n        def text_treatment(text):\n            text = lower_string(text)\n            text = remove_special_strips(text,'<br />')\n            stop_punc = set(stopwords.words('english') + list(string.punctuation))\n            tokenizer = ToktokTokenizer()\n            filtered_tokens = [i for i in tokenizer.tokenize(text) if i not in stop_punc]\n            filtered_tokens = [i.replace('.', \"\") for i in filtered_tokens]\n            filtered_text   = ' '.join(filtered_tokens) \n            return(filtered_text)\n        \n        def stemmer(text):\n            sinlge_stemmer = PorterStemmer()\n            words = text.split()\n            words_stem = [sinlge_stemmer.stem(i) for i in words]\n            text_stem  = ' '.join(words_stem)\n            return text_stem\n        \n        def create_features(max_features, min_df, corpus, feature_abbr):\n            Vectorizer = TfidfVectorizer(max_features=max_features,min_df=min_df)\n            result     = Vectorizer.fit_transform(corpus)\n            tfidf_df   = pd.DataFrame(result.toarray(),columns=Vectorizer.get_feature_names_out())\n            tfidf_df.columns = feature_abbr + '_' + tfidf_df.columns\n            return(tfidf_df)\n        \n        text_data = self.data_imputed\n        if text_data[feature].dtype == 'O':\n            text_data[feature] = text_data[feature].apply(text_treatment)\n            text_data[feature] = text_data[feature].apply(stemmer)\n            corpus = list(text_data[feature])\n            tfidf_df = create_features(max_features = args.max_features,\n                                       min_df       = args.min_df,\n                                       corpus       = corpus,\n                                       feature_abbr = feature_abbr\n                                      )\n            self.data_tfidf = tfidf_df\n        else:\n            print_description(\"Error - {} in is not a text feature\".format(feature))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:25.853985Z","iopub.execute_input":"2024-04-18T14:43:25.854402Z","iopub.status.idle":"2024-04-18T14:43:25.875774Z","shell.execute_reply.started":"2024-04-18T14:43:25.854371Z","shell.execute_reply":"2024-04-18T14:43:25.874829Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"class_dataset\"></a>\n<h2>Class Dataset()</h2>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{"execution":{"iopub.status.busy":"2024-04-14T04:23:17.792808Z","iopub.execute_input":"2024-04-14T04:23:17.793288Z","iopub.status.idle":"2024-04-14T04:23:17.801759Z","shell.execute_reply.started":"2024-04-14T04:23:17.793257Z","shell.execute_reply":"2024-04-14T04:23:17.799992Z"}}},{"cell_type":"code","source":"class Dataset():\n    \"\"\"\n    This class is used to create the modeling dataset. \n    It also instantiates class Data_Analysis() and class X_Data_Treatment().\n    \"\"\"\n    \n    def __init__(self):\n        self.data_raw     = None\n        self.data_imputed = None\n        self.data_tfidf   = None\n        self.X_Data       = None\n        self.X_Data_copy  = None\n        self.Y_Data       = None\n        self.Y_Data_copy  = None\n        self.YX_Data      = None\n\n        self.data_analysis    = Data_Analysis()\n        self.x_data_treatment = None\n        \n        self.tfidf_features     = None\n        self.add_tfidf_features = False\n        \n        print_description(\"Dataset object is created\")\n    \n    def load_modeling_data_from_csv(self,args):\n        \"\"\"\n        This function loads modeling data from a saved csv file.\n        Attribute: \n            args - global arguments\n        Return: \n            self.X_Data - updated\n            self.Y_Data - updated\n        \"\"\"\n        modeling_data = pd.read_csv(args.model_data_name)\n        self.X_Data   = modeling_data.drop(args.target_feature,axis=1)\n        self.Y_Data   = modeling_data[args.target_feature]\n        print_description(\"Loaded modeling data from csv\")        \n        \n    def add_raw_data_from_df(self, df):\n        \"\"\"\n        This function loads a raw data from a DataFrame.\n        Attribute: \n            df (DataFrame) - the DataFrame file for the raw data\n        Return: \n            self.data_raw - updated\n        \"\"\"\n        self.data_raw = df\n        print_description(\"Loaded the raw data from df\")\n    \n    def add_raw_data_from_csv(self, args):\n        \"\"\"\n        This function loads a raw data from a csv file.\n        Attribute: \n            args - global arguments\n        Return: \n            self.data_raw - updated\n        \"\"\"\n        self.data_raw = pd.read_csv(args.input_file_name)\n        print_description(\"Loaded the raw data from csv\") \n\n    def get_x_y_data(self, args, verbose=1):\n        \"\"\"\n        This function creates X_Data(explanatory features) and Y_Data(target \n        feature) from the raw data. Remove features with high missing rate \\\n        (>args.missing_th).\n        Attribute: \n            args - global arguments\n            verbose - if 1, print the dimension of X_Data and Y_Data. \n        Return: \n            self.X_Data           - updated\n            self.X_Data_copy      - updated\n            self.Y_Data           - updated\n            self.Y_Data_copy      - updated\n            self.YX_Data          - updated\n            self.x_data_treatment - updated\n        \"\"\"\n        if self.data_raw is None:\n            print_description(\"Error - raw data is not defined\") \n        else: \n            target = args.target_feature\n            Y_Data = self.data_raw[args.target_feature]\n            X_Data = self.data_raw.drop([args.target_feature], axis=1)\n            \n            missing_info = X_Data.apply(lambda x:x.isna().sum()).reset_index()\n            missing_info.columns = ['feature','n_miss']\n            missing_info['n_non_missing'] = X_Data.apply(lambda x:x.count()).values\n            missing_info['n'] = missing_info['n_non_missing'] + missing_info['n_miss']\n            missing_info['p_miss'] = missing_info['n_miss'] / missing_info['n']\n            \n            features_keep = missing_info.loc[(missing_info['p_miss']<=args.missing_th),\n                                             'feature']\n            X_Data = X_Data[features_keep]\n            \n            self.X_Data      = X_Data\n            self.X_Data_copy = X_Data\n            self.Y_Data      = Y_Data\n            self.Y_Data_copy = Y_Data\n            self.YX_Data     = pd.concat([Y_Data, X_Data], axis=1)\n            self.x_data_treatment = X_Data_Treatment(self.X_Data)\n            \n            if verbose==1:\n                print(\"Columns with missing percentage > {} are removed\".\\\n                      format(args.missing_th))\n                print(\"Shape of X_Data: {}\".format(X_Data.shape))\n                print(\"Shape of Y_Data: {}\".format(Y_Data.shape))\n                \n            print_description(\"X_Data, Y_Data are created\") \n    \n    def restore_X_Data(self):\n        \"\"\"\n        This function restores X_Data\n        Attribute: None. \n        Return: \n            self.X_Data - updated\n        \"\"\"\n        if self.X_Data_copy is None:\n            print_description(\"Error - No copy is available\") \n        else:\n            self.X_Data = self.X_Data_copy\n            print_description(\"Restored X_Data\") \n    \n    def drop_feautre_from_X(self, features_to_drop):\n        \"\"\"\n        This function removes a list of features from X_Data\n        Attribute:\n            features_to_drop (a list of string): the features to drop\n        Return: \n            self.X_Data - updated\n        \"\"\"\n        if self.X_Data is None: \n            print_description(\"Error - X_Data is not defined\") \n        else:\n            not_in_X = [elem for elem in features_to_drop \n                        if elem not in self.X_Data.columns] \n            if len(not_in_X)>0:\n                _str = ' '.join(not_in_X)\n                print_description(\"Error - feature(s) not in X_Data: {}\".format(_str))\n            else:\n                df = self.X_Data,copy()\n                df = df.drop(features_to_drop, axis = 1)\n                self.X_Data = df\n                _str = ' '.join(features_to_drop)\n                print_description(\"Feature(s) below are dropped: {}\".format(_str))\n\n    def _fill_na(self, strategy):\n        \"\"\"\n        This function applies the function x_data_treatment.fill_na()\n        Attribute: \n            strategy (str) - 'mode', 'mean', 'median', 'none' \n        Return: \n            self.data_imputed - updated\n        \"\"\"\n        if self.X_Data is None:\n            print_description(\"Error - original data is not defined\") \n        else: \n            self.x_data_treatment.fill_na(strategy)\n            self.data_imputed   = self.x_data_treatment.data_imputed\n            print_description(\"Missing data imputation is conducted, \\\n                               strategy: {}\".format(strategy)) \n    \n    def _create_tfidf_features(self, args):\n        \"\"\"\n        This function applies the function \\\n        x_data_treatment.create_tfidf_features()\n        Attribute: \n            args - global arguments\n        Return: \n            self.tfidf_features - updated\n            self.data_tfidf - updated\n        \"\"\"\n        self.x_data_treatment.create_tfidf_features(args.feature_tfidf,\n                                                    args.feature_tfidf_abbr,\n                                                    args)\n        tfidf_df = self.x_data_treatment.data_tfidf\n        self.tfidf_features = list(tfidf_df.columns)\n        self.data_tfidf     = tfidf_df\n        print_description(\"TFIDF features for {} are created\".\\\n                          format(args.feature_tfidf)) \n    \n    def adding_tfidf_features(self):\n        \"\"\"\n        This function adds self.data_tfidf to self.X_Data and \\\n        drops args.feature_tfidf\n        Attribute: None\n        Return: \n            self.X_Data - updated\n            self.add_tfidf_features - updated\n        \"\"\"\n        if self.X_Data is None:\n            print_description(\"Error - X_data is not defined\") \n        else:\n            if self.data_tfidf is None:\n                print_description(\"Error - TFIDF features are not defined\") \n            else:\n                x_df     = self.X_Data.copy()\n                tfidf_df = self.data_tfidf.copy()\n                _df      = pd.concat([x_df, tfidf_df], axis=1)\n                _df      = _df.drop(args.feature_tfidf, axis=1)\n                self.X_Data = _df\n                self.add_tfidf_features = True\n                print_description(\"Added TFIDF features to X_Data\")\n\n    def _get_descriptive_statistics(self, data_of_interest):\n        \"\"\"\n        This function applies the function data_analysis.\\\n        get_descriptive_statistics()\n        Attribute: \n            data_of_interest (DataFrame) - the data if interest\n        Return: \n            self.data_analysis.descriptive_statistics - updated\n        \"\"\"\n        self.data_analysis.get_descriptive_statistics(data_of_interest)\n \n    def save_modeling_data(self):\n        \"\"\"\n        This function saves the modeling data (Y_Data, X_Data) in a csv file\n        Attribute: None\n        Return: \n            Modeling_Data.csv\n        \"\"\"\n        modeling_data = pd.concat([self.Y_Data, self.X_Data], axis=1)\n        modeling_data.to_csv('Modeling_Data.csv', index=False)\n        print_description(\"Exported Modeling Data\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:25.877363Z","iopub.execute_input":"2024-04-18T14:43:25.878039Z","iopub.status.idle":"2024-04-18T14:43:25.909522Z","shell.execute_reply.started":"2024-04-18T14:43:25.878004Z","shell.execute_reply":"2024-04-18T14:43:25.908037Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"class_trainlgbclassifier\"></a>\n<h2>Class TrainLGBClassifier()</h2>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{}},{"cell_type":"code","source":"class TrainLGBClassifier():\n    \"\"\"\n    This class trains a LGB model with given data and settings.\n    \"\"\"\n    \n    def __init__(self, x_input, \n                 y_input, \n                 hyperparameters, \n                 importance_type,\n                 args, \n                 verbose):\n        self.X_all = x_input\n        self.Y_all = y_input\n        self.hp    = hyperparameters\n        self.model = lgb.LGBMClassifier(**hyperparameters, \n                                        importance_type=importance_type,\n                                        verbose = verbose)\n        self.best_score     = None\n        self.best_iteration = None\n        self.importance     = None\n        self.performance    = pd.DataFrame(columns = ['DataPartition',\n                                                      'AUC',\n                                                      'AUPRC',\n                                                      'Logloss'])\n        \n    def split_data(self, args, verbose = 0):\n        \"\"\"\n        This function splits the data into train and validation sets.\n        Attribute: \n            args - global arguments\n        Return: \n            self.x1 - updated\n            self.x2 - updated\n            self.y1 - updated\n            self.y2 - updated\n        \"\"\"\n        X_Data = self.X_all\n        Y_Data = self.Y_all\n        x1,x2,y1,y2 = train_test_split(X_Data,\n                                       Y_Data,\n                                       test_size    = args.test_size,\n                                       random_state = args.partition_seed)\n        self.x1 = x1\n        self.x2 = x2\n        self.y1 = y1\n        self.y2 = y2\n        if verbose >0:\n            print_description(\"x1, x2, y1, y2 are created\")\n\n    def train_classifier(self, args):\n        \"\"\"\n        This function trains the model.\n        Attribute: \n            args - global arguments\n        Return: \n            self.model - fitted and updated\n            self.best_score - updated\n            self.best_iteration - updated\n            self.importance - updated\n        \"\"\"\n        self.model.fit(X = self.x1, y = self.y1,\n                       eval_metric=args.metric, \n                       eval_set=[(self.x1,self.y1),(self.x2,self.y2)],\n                       callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n        self.best_score = self.model.best_score_\n        self.best_iteration = self.model.best_iteration_\n        importance_df = pd.DataFrame(list(self.x1))\n        importance_df.columns = ['feature']\n        importance_df['importance'] = self.model.feature_importances_\n        importance_df = importance_df.sort_values(['importance'], ascending = False)\n        importance_df['rank'] = importance_df['importance'].rank(method='min',\n                                                                 ascending = False)\n        self.importance = importance_df\n        print_description(\"Training is done\")\n    \n    def save_model(self, args, name=None):\n        \"\"\"\n        This function saves the model to a pkl file.\n        Attribute: \n            args - global arguments\n            name (str) - model name (suffix '.pkl' not included)\n        Return: \n            model.pkl or name.pkl\n        \"\"\"\n        path_name = args.output_path\n        if name is None:\n            model_name = 'model.pkl'\n        else:\n            model_name = name + '.pkl'\n        joblib.dump(self.model, path_name + model_name)\n        print_description(\"Model Saved\")\n    \n    def save_importance(self, args, name=None):\n        \"\"\"\n        This function saves feature importance to a csv file.\n        Attribute: \n            args - global arguments\n            name (str) - filename (suffix '.csv' not included)\n        Return: \n            model_importance.csv or name.csv\n        \"\"\"\n        path_name = args.output_path\n        if name is None:\n            model_name = 'model_importance.csv'\n        else:\n            model_name = name + '.csv'\n        self.importance.to_csv(path_name + model_name, index=False)\n        print_description(\"Feature Importance Saved\")\n    \n    def in_sample_performance(self):\n        \"\"\"\n        This function calculate the in-sample performance of self.model.\n        Attribute: None\n        Return: \n            self.performance - updated \n        \"\"\"\n        def calc_auc(y_pred, y_true):\n            return roc_auc_score(y_true, y_pred)\n        \n        def calc_auprc(y_pred, y_true):\n            return average_precision_score(y_true, y_pred)\n        \n        def calc_logloss(y_pred, y_true):\n            return log_loss(y_true, y_pred)\n        \n        y_pred = self.model.predict_proba(self.x2)[:,1]\n        y_true = self.y2\n        is_auc     = calc_auc(y_pred, y_true)\n        is_auprc   = calc_auprc(y_pred, y_true)\n        is_logloss = calc_logloss(y_pred, y_true)\n    \n        perf_df = pd.DataFrame(columns = self.performance.columns)\n        perf_df.loc[0,:] = ['validation', is_auc, is_auprc, is_logloss]\n        self.performance = pd.concat([self.performance, perf_df], axis=0, ignore_index=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:25.911593Z","iopub.execute_input":"2024-04-18T14:43:25.911947Z","iopub.status.idle":"2024-04-18T14:43:25.934941Z","shell.execute_reply.started":"2024-04-18T14:43:25.911912Z","shell.execute_reply":"2024-04-18T14:43:25.933400Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"class_fs_ir\"></a>\n<h2>Class IterativeReduction()</h2>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{}},{"cell_type":"code","source":"class IterativeReduction():\n    \"\"\"\n    This class conducts a iterative reduction feature selection. \n    \"\"\"\n    \n    def __init__(self, \n                 x_input, \n                 y_input, \n                 n_feature_list,\n                 hyperparameters, \n                 importance_type,\n                 args):\n        self.x_input = x_input\n        self.y_input = y_input\n        self.hp      = hyperparameters\n        self.importance_type = importance_type\n        self.n_feature_list  = n_feature_list\n        self.ir_results      = {}\n        \n    def iterative_reduction(self, start_n, end_n):\n        \"\"\"\n        This function executes one iteration in an iterative reduction.\n        Attribute: \n            - start_n (integer): the number of feature to reduct from\n            - end_n (integer): the number of feature to reduct to\n        Return: \n            self.ir_results - updated\n        \"\"\"\n        print('Feature reduction from %d to %d ...' %(start_n,end_n))\n        importance_old = self.ir_results['FL_'+str(start_n)]['importances']\n        selected_features = importance_old.head(end_n)['feature'].values.tolist()\n        shuffled_features = selected_features.copy()\n        random.shuffle(shuffled_features)\n        x_input_now = self.x_input[shuffled_features]\n\n        LGBModel = TrainLGBClassifier(x_input = x_input_now, \n                                      y_input = self.y_input,\n                                      hyperparameters = self.hp, \n                                      importance_type = self.importance_type,\n                                      args    = args,\n                                      verbose = 0)\n        LGBModel.split_data(args)\n        LGBModel.train_classifier(args)\n\n        n_now  = x_input_now.shape[1]\n        FL_now = 'FL_'+str(n_now)\n        self.ir_results[FL_now] = {}\n        self.ir_results[FL_now]['importances'] = LGBModel.importance\n        self.ir_results[FL_now]['features']    = LGBModel.importance['feature'] \n    \n    def Run_IR(self):\n        \"\"\"\n        This function executes the complete iterative reduction procedure.\n        Attribute: None\n        Return: \n            self.ir_results - updated\n        \"\"\"\n        \n        ##### Initialize the Iterative Reduction\n        LGBModel = TrainLGBClassifier(x_input = self.x_input, \n                                      y_input = self.y_input,\n                                      hyperparameters = self.hp, \n                                      importance_type = self.importance_type,\n                                      args    = args,\n                                      verbose = 0)\n        LGBModel.split_data(args)\n        LGBModel.train_classifier(args)\n        n_now  = self.x_input.shape[1]\n        FL_now = 'FL_'+str(n_now)\n        self.ir_results[FL_now] = {}\n        self.ir_results[FL_now]['importances'] = LGBModel.importance\n        self.ir_results[FL_now]['features']    = LGBModel.importance['feature']\n        \n        ##### Conduct the following iterations \n        for a in range(len(self.n_feature_list)-1):\n            self.iterative_reduction(self.n_feature_list[a], self.n_feature_list[a+1])\n    \n    def save_IR_results(self):\n        \"\"\"\n        This function saves the iterative reduction results in a pkl file.\n        Attribute: None\n        Return: \n            IR_results.pkl\n        \"\"\"\n        with open('IR_results.pkl', 'wb') as f:\n            pickle.dump(self.ir_results, f)\n        print_description(\"IR_results saved\")\n    \n    def load_IR_results(self, args):\n        \"\"\"\n        This function loads the iterative reduction results from a pkl file.\n        Attribute: None\n        Return: \n            self.ir_results - updated\n        \"\"\"\n        with open(args.IR_results_filename, 'rb') as f:\n            self.ir_results = pickle.load(f)\n        print_description(\"IR_results loaded\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:25.937137Z","iopub.execute_input":"2024-04-18T14:43:25.937552Z","iopub.status.idle":"2024-04-18T14:43:25.956492Z","shell.execute_reply.started":"2024-04-18T14:43:25.937516Z","shell.execute_reply":"2024-04-18T14:43:25.955189Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"class_fs_boruta\"></a>\n<h2>Class Boruta()</h2>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{}},{"cell_type":"code","source":"class Boruta():\n    \"\"\"\n    This class conducts a Boruta feature selection. \n    \"\"\"\n    \n    def __init__(self, \n                 x_input, \n                 y_input, \n                 n_iterations,\n                 hyperparameters, \n                 importance_type,\n                 args):\n        self.x_input         = x_input\n        self.y_input         = y_input\n        self.n_iterations    = n_iterations\n        self.hp              = hyperparameters\n        self.importance_type = importance_type\n        self.hits_df         = None\n        self.boruta_results  = {}\n        self.n_hits_list     = None\n    \n    def verify_n_hits_list(self,n_hits_list):\n        \"\"\"\n        This function checks if the list n_hits_list is valid.\n        Attribute: \n            n_hits_list(a list of integers) - the list to verify\n        Return: \n            True / False\n        \"\"\"\n        check = test_nonrepeating_ascending_list(n_hits_list,\n                                                 0,\n                                                 self.n_iterations)\n        if check:\n            print_description(\"n_hits_list is valid\")\n        else:\n            print_description(\"n_hits_list is not valid\")\n        \n    def get_n_hits(self):\n        \"\"\"\n        This function obtains the number of hits for each feature.\n        Attribute: None\n        Return: \n            self.hits_df - updated\n        \"\"\"\n        x_Data = self.x_input.copy()\n        hits = np.zeros(len(x_Data.columns))\n        \n        for _iter in range(self.n_iterations):\n            print(\"Iteration {}/{}\".format(_iter+1,self.n_iterations))\n            np.random.seed(_iter)\n            x_shadow = x_Data.apply(np.random.permutation)\n            x_shadow.columns = ['shadow_'+ feature for feature in x_Data.columns]\n            x_boruta = pd.concat([x_Data,x_shadow],axis=1,ignore_index=True)\n            y_boruta = self.y_input\n            \n            ##### Shuffle the modeling dataset (x_boruta and y_boruta)\n            combined_boruta = pd.concat([x_boruta, y_boruta], axis=1)\n            shuffled_combined_boruta = combined_boruta.sample(frac=1, random_state=_iter)\n            shuffled_x_boruta = shuffled_combined_boruta.iloc[:,:len(x_boruta.columns)]\n            shuffled_y_boruta = shuffled_combined_boruta[args.target_feature]\n            \n            ##### Build the boruta model and extarct the feature importance info\n            LGBModel = TrainLGBClassifier(x_input = shuffled_x_boruta,\n                                          y_input = shuffled_y_boruta,\n                                          hyperparameters = self.hp, \n                                          importance_type = self.importance_type,\n                                          args    = args,\n                                          verbose = 0)\n            LGBModel.split_data(args)\n            LGBModel.train_classifier(args)\n            \n            feature_imp_x      = LGBModel.model.feature_importances_[:len(x_Data.columns)]\n            feature_imp_shadow = LGBModel.model.feature_importances_[len(x_Data.columns):]\n            hits+=(feature_imp_x>feature_imp_shadow.max())\n\n        Hits_df = pd.DataFrame(columns=['Feature','Hits'])\n        Hits_df.iloc[:,0] = x_Data.columns\n        Hits_df.iloc[:,1] = hits\n        \n        self.hits_df = Hits_df\n        \n    def get_feature_lists(self, n_hits_list):\n        \"\"\"\n        This function obtains a number of feature lists based on \\\n        n_hits_list. For example, if n_hits_list[0]=1, a feature list\\\n        is created to includes all features with n_hits>=1.\n        Attribute: \n            n_hits_list(a list of integers) - the list to verify\n        Return: \n            self.boruta_results - updated\n        \"\"\"\n        self.n_hits_list = n_hits_list\n        Hits_df = self.hits_df\n        for i in range(len(n_hits_list)):\n            feature_keep = Hits_df[Hits_df['Hits']>=n_hits_list[i]]['Feature']\n            n_now  = len(feature_keep)\n            FL_now = 'FL_'+str(n_now)\n            self.boruta_results[FL_now] = feature_keep\n\n    def save_Boruta_results(self):\n        \"\"\"\n        This function saves the Boruta results in a pkl file.\n        Attribute: None\n        Return: \n            Boruta_results.pkl\n        \"\"\"\n        with open('Boruta_results.pkl', 'wb') as f:\n            pickle.dump(self.boruta_results, f)\n        print_description(\"Boruta_results saved\")\n        \n    def load_Boruta_results(self, args):\n        \"\"\"\n        This function loads Boruta results from a pkl file.\n        Attribute: None\n        Return: \n            self.boruta_results - updated\n        \"\"\"\n        with open(args.Boruta_results_filename, 'rb') as f:\n            self.boruta_results = pickle.load(f)\n        print_description(\"Boruta_results loaded\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:25.961842Z","iopub.execute_input":"2024-04-18T14:43:25.962570Z","iopub.status.idle":"2024-04-18T14:43:25.982892Z","shell.execute_reply.started":"2024-04-18T14:43:25.962535Z","shell.execute_reply":"2024-04-18T14:43:25.981815Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"prepare_data\"></a>\n<b><h1>5. Prepare Data</h1>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{"execution":{"iopub.status.busy":"2024-04-14T16:21:28.356511Z","iopub.execute_input":"2024-04-14T16:21:28.357046Z","iopub.status.idle":"2024-04-14T16:21:28.365686Z","shell.execute_reply.started":"2024-04-14T16:21:28.357009Z","shell.execute_reply":"2024-04-14T16:21:28.364145Z"}}},{"cell_type":"code","source":"def prepare_data(create_model_data):\n    \"\"\"\n    This function prepares the modeling data.\n    Attribute:\n        create_model_data (boolean): \n            if True, use several functions in Class X_Data_Treatment()\\\n            and Class Dataset(); \\\n            if False, use Dataset.load_modeling_data_from_csv()\n    Return:\n        review_dataset (DataFrame)\n    \"\"\"\n    review_dataset = Dataset()\n    if create_model_data:\n        review_df = pd.read_csv(args.input_file_name).head(1000)\n        review_df = review_df.rename(columns={'review':'movie_review'}) \n        review_df['sentiment_label']  = review_df['sentiment']\n        review_df['sentiment_number'] = (review_df['sentiment'] == 'positive').astype(int)\n        review_df.drop((['sentiment','sentiment_label']),axis=1,inplace=True)\n\n        review_dataset.add_raw_data_from_df(review_df)\n        review_dataset.get_x_y_data(args,verbose=1)\n        review_dataset._fill_na(strategy = 'none')\n        review_dataset._create_tfidf_features(args)\n        review_dataset.adding_tfidf_features()\n        review_dataset.save_modeling_data()\n    else:\n        review_dataset.load_modeling_data_from_csv(args)\n    return review_dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:25.984702Z","iopub.execute_input":"2024-04-18T14:43:25.985046Z","iopub.status.idle":"2024-04-18T14:43:25.997708Z","shell.execute_reply.started":"2024-04-18T14:43:25.985019Z","shell.execute_reply":"2024-04-18T14:43:25.996612Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"review_dataset = prepare_data(create_model_data=False)\nreview_dataset._get_descriptive_statistics(review_dataset.X_Data)\nreview_dataset.data_analysis.descriptive_statistics","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:26.000488Z","iopub.execute_input":"2024-04-18T14:43:26.001666Z","iopub.status.idle":"2024-04-18T14:43:46.869273Z","shell.execute_reply.started":"2024-04-18T14:43:26.001598Z","shell.execute_reply":"2024-04-18T14:43:46.864675Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Data_Analysis object is created\n----------------------------------------------------------------------------------------------------\nDataset object is created\n----------------------------------------------------------------------------------------------------\nLoaded modeling data from csv\n----------------------------------------------------------------------------------------------------\nDescriptive statistics are created\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"         feature  n_miss  n_non_missing      n  p_miss     Type  N_unique  \\\n0         rv_000       0          49582  49582     0.0  float64       416   \n1          rv_10       0          49582  49582     0.0  float64      6554   \n2         rv_100       0          49582  49582     0.0  float64       833   \n3          rv_11       0          49582  49582     0.0  float64       486   \n4          rv_12       0          49582  49582     0.0  float64       584   \n...          ...     ...            ...    ...     ...      ...       ...   \n1995    rv_young       0          49582  49582     0.0  float64      5246   \n1996  rv_younger       0          49582  49582     0.0  float64       903   \n1997    rv_youth       0          49582  49582     0.0  float64       598   \n1998     rv_zero       0          49582  49582     0.0  float64       575   \n1999    rv_zombi       0          49582  49582     0.0  float64       729   \n\n          Mean               sample  \n0     0.001179  0.0,0.0,0.0,0.0,0.0  \n1     0.012516  0.0,0.0,0.0,0.0,0.0  \n2     0.002084  0.0,0.0,0.0,0.0,0.0  \n3     0.001473  0.0,0.0,0.0,0.0,0.0  \n4     0.001585  0.0,0.0,0.0,0.0,0.0  \n...        ...                  ...  \n1995  0.009738  0.0,0.0,0.0,0.0,0.0  \n1996  0.002144  0.0,0.0,0.0,0.0,0.0  \n1997  0.001567  0.0,0.0,0.0,0.0,0.0  \n1998  0.001629  0.0,0.0,0.0,0.0,0.0  \n1999  0.003782  0.0,0.0,0.0,0.0,0.0  \n\n[2000 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>n_miss</th>\n      <th>n_non_missing</th>\n      <th>n</th>\n      <th>p_miss</th>\n      <th>Type</th>\n      <th>N_unique</th>\n      <th>Mean</th>\n      <th>sample</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>rv_000</td>\n      <td>0</td>\n      <td>49582</td>\n      <td>49582</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>416</td>\n      <td>0.001179</td>\n      <td>0.0,0.0,0.0,0.0,0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>rv_10</td>\n      <td>0</td>\n      <td>49582</td>\n      <td>49582</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>6554</td>\n      <td>0.012516</td>\n      <td>0.0,0.0,0.0,0.0,0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>rv_100</td>\n      <td>0</td>\n      <td>49582</td>\n      <td>49582</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>833</td>\n      <td>0.002084</td>\n      <td>0.0,0.0,0.0,0.0,0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>rv_11</td>\n      <td>0</td>\n      <td>49582</td>\n      <td>49582</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>486</td>\n      <td>0.001473</td>\n      <td>0.0,0.0,0.0,0.0,0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>rv_12</td>\n      <td>0</td>\n      <td>49582</td>\n      <td>49582</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>584</td>\n      <td>0.001585</td>\n      <td>0.0,0.0,0.0,0.0,0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>rv_young</td>\n      <td>0</td>\n      <td>49582</td>\n      <td>49582</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>5246</td>\n      <td>0.009738</td>\n      <td>0.0,0.0,0.0,0.0,0.0</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>rv_younger</td>\n      <td>0</td>\n      <td>49582</td>\n      <td>49582</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>903</td>\n      <td>0.002144</td>\n      <td>0.0,0.0,0.0,0.0,0.0</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>rv_youth</td>\n      <td>0</td>\n      <td>49582</td>\n      <td>49582</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>598</td>\n      <td>0.001567</td>\n      <td>0.0,0.0,0.0,0.0,0.0</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>rv_zero</td>\n      <td>0</td>\n      <td>49582</td>\n      <td>49582</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>575</td>\n      <td>0.001629</td>\n      <td>0.0,0.0,0.0,0.0,0.0</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>rv_zombi</td>\n      <td>0</td>\n      <td>49582</td>\n      <td>49582</td>\n      <td>0.0</td>\n      <td>float64</td>\n      <td>729</td>\n      <td>0.003782</td>\n      <td>0.0,0.0,0.0,0.0,0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id=\"feature_selection\"></a>\n<b><h1>6. Feature Selection</h1>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"run_iterative_reduction\"></a>\n<b><h2>6.1 Iterative Reduction Approach</h2>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:46:21.138085Z","iopub.execute_input":"2024-04-16T20:46:21.138799Z","iopub.status.idle":"2024-04-16T20:46:21.153688Z","shell.execute_reply.started":"2024-04-16T20:46:21.138764Z","shell.execute_reply":"2024-04-16T20:46:21.152845Z"}}},{"cell_type":"code","source":"ReRun = True\nir = IterativeReduction(x_input = review_dataset.X_Data, \n                        y_input = review_dataset.Y_Data, \n                        n_feature_list = [2000,1800,1600,1400,1200,1000,\n                                          800,600,400,200],\n                        hyperparameters = args.params_LGB, \n                        importance_type = 'gain',\n                        args    = args)\nif ReRun:\n    ir.Run_IR()\n    ir.save_IR_results()\nelse:\n    ir.load_IR_results(args)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:43:46.875162Z","iopub.execute_input":"2024-04-18T14:43:46.877147Z","iopub.status.idle":"2024-04-18T14:54:08.539272Z","shell.execute_reply.started":"2024-04-18T14:43:46.877108Z","shell.execute_reply":"2024-04-18T14:54:08.537516Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[723]\ttraining's binary_logloss: 0.160656\ttraining's auc: 0.992658\tvalid_1's binary_logloss: 0.297905\tvalid_1's auc: 0.94615\nTraining is done\n----------------------------------------------------------------------------------------------------\nFeature reduction from 2000 to 1800 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[971]\ttraining's binary_logloss: 0.13125\ttraining's auc: 0.996576\tvalid_1's binary_logloss: 0.296992\tvalid_1's auc: 0.946015\nTraining is done\n----------------------------------------------------------------------------------------------------\nFeature reduction from 1800 to 1600 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[861]\ttraining's binary_logloss: 0.143542\ttraining's auc: 0.995094\tvalid_1's binary_logloss: 0.29713\tvalid_1's auc: 0.94612\nTraining is done\n----------------------------------------------------------------------------------------------------\nFeature reduction from 1600 to 1400 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1041]\ttraining's binary_logloss: 0.124211\ttraining's auc: 0.99717\tvalid_1's binary_logloss: 0.296559\tvalid_1's auc: 0.946207\nTraining is done\n----------------------------------------------------------------------------------------------------\nFeature reduction from 1400 to 1200 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[888]\ttraining's binary_logloss: 0.140586\ttraining's auc: 0.995349\tvalid_1's binary_logloss: 0.29754\tvalid_1's auc: 0.945932\nTraining is done\n----------------------------------------------------------------------------------------------------\nFeature reduction from 1200 to 1000 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[848]\ttraining's binary_logloss: 0.146651\ttraining's auc: 0.994447\tvalid_1's binary_logloss: 0.297477\tvalid_1's auc: 0.946017\nTraining is done\n----------------------------------------------------------------------------------------------------\nFeature reduction from 1000 to 800 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[880]\ttraining's binary_logloss: 0.144878\ttraining's auc: 0.994479\tvalid_1's binary_logloss: 0.298861\tvalid_1's auc: 0.945348\nTraining is done\n----------------------------------------------------------------------------------------------------\nFeature reduction from 800 to 600 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[664]\ttraining's binary_logloss: 0.175692\ttraining's auc: 0.989156\tvalid_1's binary_logloss: 0.303854\tvalid_1's auc: 0.943653\nTraining is done\n----------------------------------------------------------------------------------------------------\nFeature reduction from 600 to 400 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[560]\ttraining's binary_logloss: 0.199821\ttraining's auc: 0.982973\tvalid_1's binary_logloss: 0.308763\tvalid_1's auc: 0.94187\nTraining is done\n----------------------------------------------------------------------------------------------------\nFeature reduction from 400 to 200 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[404]\ttraining's binary_logloss: 0.251189\ttraining's auc: 0.96729\tvalid_1's binary_logloss: 0.332709\tvalid_1's auc: 0.932082\nTraining is done\n----------------------------------------------------------------------------------------------------\nIR_results saved\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"run_boruta\"></a>\n<b><h2>6.2 Boruta Approach</h2>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:53:51.076301Z","iopub.execute_input":"2024-04-16T20:53:51.076721Z","iopub.status.idle":"2024-04-16T20:53:51.083562Z","shell.execute_reply.started":"2024-04-16T20:53:51.076690Z","shell.execute_reply":"2024-04-16T20:53:51.082541Z"}}},{"cell_type":"code","source":"boruta = Boruta(x_input = review_dataset.X_Data, \n                y_input = review_dataset.Y_Data, \n                n_iterations = 50,\n                hyperparameters = args.params_LGB, \n                importance_type = 'gain',\n                args            = args)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:54:08.543397Z","iopub.execute_input":"2024-04-18T14:54:08.543816Z","iopub.status.idle":"2024-04-18T14:54:08.550802Z","shell.execute_reply.started":"2024-04-18T14:54:08.543779Z","shell.execute_reply":"2024-04-18T14:54:08.549409Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"n_hits_list = [1,2,3,4,5,6,7,8,9,10,20,30,40,50]\nboruta.verify_n_hits_list(n_hits_list)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:54:08.552245Z","iopub.execute_input":"2024-04-18T14:54:08.552636Z","iopub.status.idle":"2024-04-18T14:54:08.564131Z","shell.execute_reply.started":"2024-04-18T14:54:08.552603Z","shell.execute_reply":"2024-04-18T14:54:08.563264Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"n_hits_list is valid\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"ReRun = True\nif ReRun:\n    boruta.get_n_hits()\n    boruta.get_feature_lists(n_hits_list)\n    boruta.save_Boruta_results()\nelse:\n    boruta.load_Boruta_results(args)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T14:54:08.565358Z","iopub.execute_input":"2024-04-18T14:54:08.565675Z","iopub.status.idle":"2024-04-18T17:17:22.571842Z","shell.execute_reply.started":"2024-04-18T14:54:08.565649Z","shell.execute_reply":"2024-04-18T17:17:22.570262Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Iteration 1/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[921]\ttraining's binary_logloss: 0.12659\ttraining's auc: 0.997954\tvalid_1's binary_logloss: 0.299799\tvalid_1's auc: 0.944881\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 2/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[754]\ttraining's binary_logloss: 0.149118\ttraining's auc: 0.995547\tvalid_1's binary_logloss: 0.301894\tvalid_1's auc: 0.944189\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 3/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[617]\ttraining's binary_logloss: 0.171471\ttraining's auc: 0.992333\tvalid_1's binary_logloss: 0.301672\tvalid_1's auc: 0.944991\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 4/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[634]\ttraining's binary_logloss: 0.169605\ttraining's auc: 0.992743\tvalid_1's binary_logloss: 0.298664\tvalid_1's auc: 0.946287\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 5/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[773]\ttraining's binary_logloss: 0.147435\ttraining's auc: 0.996079\tvalid_1's binary_logloss: 0.295968\tvalid_1's auc: 0.94692\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 6/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[702]\ttraining's binary_logloss: 0.153673\ttraining's auc: 0.994921\tvalid_1's binary_logloss: 0.312693\tvalid_1's auc: 0.940269\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 7/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[616]\ttraining's binary_logloss: 0.170846\ttraining's auc: 0.992042\tvalid_1's binary_logloss: 0.308248\tvalid_1's auc: 0.942447\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 8/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[670]\ttraining's binary_logloss: 0.15992\ttraining's auc: 0.994162\tvalid_1's binary_logloss: 0.305317\tvalid_1's auc: 0.94352\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 9/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[645]\ttraining's binary_logloss: 0.165577\ttraining's auc: 0.99324\tvalid_1's binary_logloss: 0.304958\tvalid_1's auc: 0.943688\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 10/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[491]\ttraining's binary_logloss: 0.198497\ttraining's auc: 0.986754\tvalid_1's binary_logloss: 0.305001\tvalid_1's auc: 0.944742\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 11/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[791]\ttraining's binary_logloss: 0.141519\ttraining's auc: 0.996465\tvalid_1's binary_logloss: 0.30688\tvalid_1's auc: 0.942233\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 12/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[606]\ttraining's binary_logloss: 0.172604\ttraining's auc: 0.99206\tvalid_1's binary_logloss: 0.305621\tvalid_1's auc: 0.943467\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 13/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[768]\ttraining's binary_logloss: 0.145659\ttraining's auc: 0.996266\tvalid_1's binary_logloss: 0.300484\tvalid_1's auc: 0.945157\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 14/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[740]\ttraining's binary_logloss: 0.150754\ttraining's auc: 0.995492\tvalid_1's binary_logloss: 0.299336\tvalid_1's auc: 0.945623\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 15/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[623]\ttraining's binary_logloss: 0.169446\ttraining's auc: 0.992608\tvalid_1's binary_logloss: 0.306803\tvalid_1's auc: 0.942828\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 16/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[693]\ttraining's binary_logloss: 0.158916\ttraining's auc: 0.994262\tvalid_1's binary_logloss: 0.301065\tvalid_1's auc: 0.945121\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 17/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[572]\ttraining's binary_logloss: 0.179933\ttraining's auc: 0.990813\tvalid_1's binary_logloss: 0.301978\tvalid_1's auc: 0.945276\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 18/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[688]\ttraining's binary_logloss: 0.156714\ttraining's auc: 0.994625\tvalid_1's binary_logloss: 0.305423\tvalid_1's auc: 0.94315\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 19/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[811]\ttraining's binary_logloss: 0.139286\ttraining's auc: 0.996886\tvalid_1's binary_logloss: 0.30173\tvalid_1's auc: 0.944657\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 20/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[623]\ttraining's binary_logloss: 0.170666\ttraining's auc: 0.992406\tvalid_1's binary_logloss: 0.302102\tvalid_1's auc: 0.944792\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 21/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[735]\ttraining's binary_logloss: 0.151047\ttraining's auc: 0.995288\tvalid_1's binary_logloss: 0.305263\tvalid_1's auc: 0.943133\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 22/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[933]\ttraining's binary_logloss: 0.125398\ttraining's auc: 0.998064\tvalid_1's binary_logloss: 0.301783\tvalid_1's auc: 0.944319\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 23/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[754]\ttraining's binary_logloss: 0.148949\ttraining's auc: 0.995872\tvalid_1's binary_logloss: 0.300381\tvalid_1's auc: 0.945055\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 24/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[638]\ttraining's binary_logloss: 0.169177\ttraining's auc: 0.99279\tvalid_1's binary_logloss: 0.301065\tvalid_1's auc: 0.945526\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 25/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[750]\ttraining's binary_logloss: 0.148503\ttraining's auc: 0.995759\tvalid_1's binary_logloss: 0.305904\tvalid_1's auc: 0.942708\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 26/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[796]\ttraining's binary_logloss: 0.14316\ttraining's auc: 0.996408\tvalid_1's binary_logloss: 0.300834\tvalid_1's auc: 0.944773\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 27/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[653]\ttraining's binary_logloss: 0.163965\ttraining's auc: 0.993506\tvalid_1's binary_logloss: 0.307047\tvalid_1's auc: 0.942585\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 28/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[654]\ttraining's binary_logloss: 0.164094\ttraining's auc: 0.993465\tvalid_1's binary_logloss: 0.305101\tvalid_1's auc: 0.943676\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 29/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[813]\ttraining's binary_logloss: 0.139969\ttraining's auc: 0.996814\tvalid_1's binary_logloss: 0.301527\tvalid_1's auc: 0.944549\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 30/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[785]\ttraining's binary_logloss: 0.145235\ttraining's auc: 0.996191\tvalid_1's binary_logloss: 0.298541\tvalid_1's auc: 0.945783\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 31/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[661]\ttraining's binary_logloss: 0.165092\ttraining's auc: 0.993312\tvalid_1's binary_logloss: 0.300512\tvalid_1's auc: 0.945683\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 32/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[674]\ttraining's binary_logloss: 0.160099\ttraining's auc: 0.993962\tvalid_1's binary_logloss: 0.30387\tvalid_1's auc: 0.94398\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 33/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[858]\ttraining's binary_logloss: 0.134211\ttraining's auc: 0.997428\tvalid_1's binary_logloss: 0.297697\tvalid_1's auc: 0.946049\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 34/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[713]\ttraining's binary_logloss: 0.153467\ttraining's auc: 0.994866\tvalid_1's binary_logloss: 0.30602\tvalid_1's auc: 0.942822\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 35/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[616]\ttraining's binary_logloss: 0.171623\ttraining's auc: 0.992139\tvalid_1's binary_logloss: 0.303844\tvalid_1's auc: 0.943997\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 36/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[675]\ttraining's binary_logloss: 0.162605\ttraining's auc: 0.993751\tvalid_1's binary_logloss: 0.29912\tvalid_1's auc: 0.945997\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 37/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[610]\ttraining's binary_logloss: 0.173316\ttraining's auc: 0.991642\tvalid_1's binary_logloss: 0.303754\tvalid_1's auc: 0.944452\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 38/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[604]\ttraining's binary_logloss: 0.171994\ttraining's auc: 0.992142\tvalid_1's binary_logloss: 0.306622\tvalid_1's auc: 0.943093\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 39/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[737]\ttraining's binary_logloss: 0.150896\ttraining's auc: 0.995421\tvalid_1's binary_logloss: 0.303008\tvalid_1's auc: 0.944145\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 40/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[647]\ttraining's binary_logloss: 0.166632\ttraining's auc: 0.992923\tvalid_1's binary_logloss: 0.302474\tvalid_1's auc: 0.944486\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 41/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[606]\ttraining's binary_logloss: 0.172534\ttraining's auc: 0.991987\tvalid_1's binary_logloss: 0.304794\tvalid_1's auc: 0.944057\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 42/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[988]\ttraining's binary_logloss: 0.118654\ttraining's auc: 0.998508\tvalid_1's binary_logloss: 0.303526\tvalid_1's auc: 0.943346\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 43/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[701]\ttraining's binary_logloss: 0.155751\ttraining's auc: 0.994524\tvalid_1's binary_logloss: 0.308464\tvalid_1's auc: 0.941987\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 44/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[627]\ttraining's binary_logloss: 0.170425\ttraining's auc: 0.992492\tvalid_1's binary_logloss: 0.299709\tvalid_1's auc: 0.945972\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 45/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[601]\ttraining's binary_logloss: 0.175455\ttraining's auc: 0.99162\tvalid_1's binary_logloss: 0.300279\tvalid_1's auc: 0.945916\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 46/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[705]\ttraining's binary_logloss: 0.156467\ttraining's auc: 0.994843\tvalid_1's binary_logloss: 0.302136\tvalid_1's auc: 0.94466\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 47/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[718]\ttraining's binary_logloss: 0.153973\ttraining's auc: 0.995022\tvalid_1's binary_logloss: 0.301906\tvalid_1's auc: 0.944779\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 48/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[661]\ttraining's binary_logloss: 0.162125\ttraining's auc: 0.99363\tvalid_1's binary_logloss: 0.307961\tvalid_1's auc: 0.942136\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 49/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[663]\ttraining's binary_logloss: 0.163117\ttraining's auc: 0.993266\tvalid_1's binary_logloss: 0.305183\tvalid_1's auc: 0.943464\nTraining is done\n----------------------------------------------------------------------------------------------------\nIteration 50/50\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[639]\ttraining's binary_logloss: 0.167318\ttraining's auc: 0.992947\tvalid_1's binary_logloss: 0.304266\tvalid_1's auc: 0.94399\nTraining is done\n----------------------------------------------------------------------------------------------------\nBoruta_results saved\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### The number of hits for 10 randomly selected features","metadata":{}},{"cell_type":"code","source":"boruta.hits_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:17:22.574626Z","iopub.execute_input":"2024-04-18T17:17:22.574969Z","iopub.status.idle":"2024-04-18T17:17:22.590723Z","shell.execute_reply.started":"2024-04-18T17:17:22.574940Z","shell.execute_reply":"2024-04-18T17:17:22.589489Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"         Feature  Hits\n732   rv_freedom  34.0\n757       rv_gem  50.0\n190       rv_ben   0.0\n1081   rv_mainli   0.0\n225    rv_bottom   0.0\n1414     rv_read   1.0\n659     rv_fault   0.0\n611      rv_evid   0.0\n1041   rv_likabl   0.0\n713     rv_forev   0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Hits</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>732</th>\n      <td>rv_freedom</td>\n      <td>34.0</td>\n    </tr>\n    <tr>\n      <th>757</th>\n      <td>rv_gem</td>\n      <td>50.0</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>rv_ben</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1081</th>\n      <td>rv_mainli</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>225</th>\n      <td>rv_bottom</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1414</th>\n      <td>rv_read</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>659</th>\n      <td>rv_fault</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>611</th>\n      <td>rv_evid</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1041</th>\n      <td>rv_likabl</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>713</th>\n      <td>rv_forev</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id=\"model_performance\"></a>\n<b><h2>6.3 Model Performance</h2>\n<a href=\"#outlines\"><b>Up to outlines</b>","metadata":{}},{"cell_type":"code","source":"##### Specify the metrics used for model selection.\nmetric = 'auc'","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:17:22.592292Z","iopub.execute_input":"2024-04-18T17:17:22.592672Z","iopub.status.idle":"2024-04-18T17:17:22.606465Z","shell.execute_reply.started":"2024-04-18T17:17:22.592644Z","shell.execute_reply":"2024-04-18T17:17:22.605396Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"##### Prepare the feature list from the iterative reduction procedure\nfeature_lists_ir = {}\nfor key in ir.ir_results:\n    feature_lists_ir[key] = ir.ir_results[key]['features']\nfeature_lists_ir = {'ir_'+ key: value for key, value \\\n                    in feature_lists_ir.items()}","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:17:22.607604Z","iopub.execute_input":"2024-04-18T17:17:22.607926Z","iopub.status.idle":"2024-04-18T17:17:22.618165Z","shell.execute_reply.started":"2024-04-18T17:17:22.607900Z","shell.execute_reply":"2024-04-18T17:17:22.617152Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"##### Prepare the feature list from Boruta procedure\nfeature_lists_boruta = boruta.boruta_results\nfeature_lists_boruta = {'boruta_'+ key: value for key, value \\\n                        in feature_lists_boruta.items()}","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:17:22.619703Z","iopub.execute_input":"2024-04-18T17:17:22.620057Z","iopub.status.idle":"2024-04-18T17:17:22.630027Z","shell.execute_reply.started":"2024-04-18T17:17:22.620028Z","shell.execute_reply":"2024-04-18T17:17:22.628897Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def get_perf_df_from_fl_dict(fl_dict, x_input, y_input,\n                             hp, importance_type, metric, args):\n    \"\"\"\n    This function obtains the model performance of a model, given \\\n    modeling data and a feature list. \n    Attribute:\n        fl_dict (dict) - a dict that stores feature lists \n        x_input (DataFrame) - X_Data for modeling\n        y_input (Series) - Y_Data for modeling\n        hp (dict) - a dict that stores hyperparameters\n        importance_type (str) - 'gain' or 'split'\n        metric (str) - 'auc' or 'logloss' (must be in args.metric)\n        args - global arguments\n    Return:\n        perf_df (DataFrame) - a DataFrame with performance information\n    \"\"\"\n    \n    def get_perf_from_fl(fl, x_input, y_input,\n                         hp, importance_type, metric, args):\n        LGBModel = TrainLGBClassifier(x_input = x_input[fl],\n                                      y_input = y_input,\n                                      hyperparameters = hp,\n                                      importance_type = importance_type,\n                                      args    = args,\n                                      verbose = 0)\n        LGBModel.split_data(args)\n        LGBModel.train_classifier(args)\n        return LGBModel.best_score['valid_1'][metric]\n    \n    fl_list = list(fl_dict.keys())\n    perf_df = pd.DataFrame(columns=['fl_name',\n                                    'n_features',\n                                    metric])\n    _perf_df = perf_df.copy()\n    for _fl_name in fl_list:\n        _fl = fl_dict[_fl_name]\n        _perf = get_perf_from_fl(fl = _fl,\n                                 x_input = review_dataset.X_Data, \n                                 y_input = review_dataset.Y_Data,\n                                 hp = args.params_LGB, \n                                 importance_type = 'gain', \n                                 metric = 'auc', \n                                 args = args)\n        \n        _perf_df.loc[0,] = [_fl_name,len(_fl),_perf]\n        perf_df = pd.concat([perf_df,_perf_df], axis=0)\n    return perf_df\n                             ","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:17:22.631838Z","iopub.execute_input":"2024-04-18T17:17:22.632471Z","iopub.status.idle":"2024-04-18T17:17:22.649437Z","shell.execute_reply.started":"2024-04-18T17:17:22.632439Z","shell.execute_reply":"2024-04-18T17:17:22.648045Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"perf_ir = get_perf_df_from_fl_dict(fl_dict = feature_lists_ir, \n                                   x_input = review_dataset.X_Data,\n                                   y_input = review_dataset.Y_Data,\n                                   hp      = args.params_LGB,\n                                   importance_type = 'gain', \n                                   metric  = metric, \n                                   args    = args)\nperf_ir['Method'] = 'Iterative Reduction'","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:17:22.650799Z","iopub.execute_input":"2024-04-18T17:17:22.651273Z","iopub.status.idle":"2024-04-18T17:27:39.688845Z","shell.execute_reply.started":"2024-04-18T17:17:22.651224Z","shell.execute_reply":"2024-04-18T17:27:39.687941Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[791]\ttraining's binary_logloss: 0.152084\ttraining's auc: 0.993946\tvalid_1's binary_logloss: 0.298293\tvalid_1's auc: 0.945818\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1030]\ttraining's binary_logloss: 0.125063\ttraining's auc: 0.997162\tvalid_1's binary_logloss: 0.296364\tvalid_1's auc: 0.946291\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[924]\ttraining's binary_logloss: 0.136226\ttraining's auc: 0.995939\tvalid_1's binary_logloss: 0.295588\tvalid_1's auc: 0.94661\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[809]\ttraining's binary_logloss: 0.149806\ttraining's auc: 0.994237\tvalid_1's binary_logloss: 0.297041\tvalid_1's auc: 0.946308\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[656]\ttraining's binary_logloss: 0.170881\ttraining's auc: 0.990821\tvalid_1's binary_logloss: 0.29849\tvalid_1's auc: 0.946118\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[863]\ttraining's binary_logloss: 0.144844\ttraining's auc: 0.994744\tvalid_1's binary_logloss: 0.298808\tvalid_1's auc: 0.9455\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[646]\ttraining's binary_logloss: 0.174708\ttraining's auc: 0.989726\tvalid_1's binary_logloss: 0.30119\tvalid_1's auc: 0.944929\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[620]\ttraining's binary_logloss: 0.182144\ttraining's auc: 0.987774\tvalid_1's binary_logloss: 0.304938\tvalid_1's auc: 0.943393\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[606]\ttraining's binary_logloss: 0.193383\ttraining's auc: 0.984494\tvalid_1's binary_logloss: 0.309358\tvalid_1's auc: 0.941493\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[437]\ttraining's binary_logloss: 0.246004\ttraining's auc: 0.968784\tvalid_1's binary_logloss: 0.333083\tvalid_1's auc: 0.931754\nTraining is done\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"perf_boruta = get_perf_df_from_fl_dict(fl_dict = feature_lists_boruta,\n                                       x_input = review_dataset.X_Data,\n                                       y_input = review_dataset.Y_Data,\n                                       hp      = args.params_LGB,\n                                       importance_type = 'gain',\n                                       metric  = metric, \n                                       args    = args)\nperf_boruta['Method'] = 'Boruta'","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:27:39.690124Z","iopub.execute_input":"2024-04-18T17:27:39.691153Z","iopub.status.idle":"2024-04-18T17:33:38.263773Z","shell.execute_reply.started":"2024-04-18T17:27:39.691117Z","shell.execute_reply":"2024-04-18T17:33:38.262463Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[742]\ttraining's binary_logloss: 0.17158\ttraining's auc: 0.989244\tvalid_1's binary_logloss: 0.29719\tvalid_1's auc: 0.946182\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[767]\ttraining's binary_logloss: 0.171637\ttraining's auc: 0.988985\tvalid_1's binary_logloss: 0.299966\tvalid_1's auc: 0.944916\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[709]\ttraining's binary_logloss: 0.180623\ttraining's auc: 0.986996\tvalid_1's binary_logloss: 0.302032\tvalid_1's auc: 0.94416\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[595]\ttraining's binary_logloss: 0.196931\ttraining's auc: 0.983385\tvalid_1's binary_logloss: 0.303852\tvalid_1's auc: 0.943782\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[703]\ttraining's binary_logloss: 0.183426\ttraining's auc: 0.986382\tvalid_1's binary_logloss: 0.303629\tvalid_1's auc: 0.943603\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[659]\ttraining's binary_logloss: 0.190138\ttraining's auc: 0.984789\tvalid_1's binary_logloss: 0.304405\tvalid_1's auc: 0.943348\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[750]\ttraining's binary_logloss: 0.180146\ttraining's auc: 0.986974\tvalid_1's binary_logloss: 0.304749\tvalid_1's auc: 0.943032\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[548]\ttraining's binary_logloss: 0.207192\ttraining's auc: 0.980669\tvalid_1's binary_logloss: 0.30741\tvalid_1's auc: 0.942426\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[586]\ttraining's binary_logloss: 0.201702\ttraining's auc: 0.982004\tvalid_1's binary_logloss: 0.307066\tvalid_1's auc: 0.942385\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[618]\ttraining's binary_logloss: 0.198005\ttraining's auc: 0.982754\tvalid_1's binary_logloss: 0.307534\tvalid_1's auc: 0.942125\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[597]\ttraining's binary_logloss: 0.207034\ttraining's auc: 0.980174\tvalid_1's binary_logloss: 0.310394\tvalid_1's auc: 0.940981\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[513]\ttraining's binary_logloss: 0.22516\ttraining's auc: 0.974981\tvalid_1's binary_logloss: 0.316098\tvalid_1's auc: 0.938944\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[405]\ttraining's binary_logloss: 0.250643\ttraining's auc: 0.967255\tvalid_1's binary_logloss: 0.321596\tvalid_1's auc: 0.937109\nTraining is done\n----------------------------------------------------------------------------------------------------\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[384]\ttraining's binary_logloss: 0.268371\ttraining's auc: 0.96079\tvalid_1's binary_logloss: 0.336413\tvalid_1's auc: 0.930539\nTraining is done\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"perf = pd.concat([perf_ir,perf_boruta],axis=0,ignore_index=True)\nperf","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:33:38.265640Z","iopub.execute_input":"2024-04-18T17:33:38.266119Z","iopub.status.idle":"2024-04-18T17:33:38.288861Z","shell.execute_reply.started":"2024-04-18T17:33:38.266085Z","shell.execute_reply":"2024-04-18T17:33:38.287348Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"          fl_name n_features       auc               Method\n0      ir_FL_2000       2000  0.945818  Iterative Reduction\n1      ir_FL_1800       1800  0.946291  Iterative Reduction\n2      ir_FL_1600       1600   0.94661  Iterative Reduction\n3      ir_FL_1400       1400  0.946308  Iterative Reduction\n4      ir_FL_1200       1200  0.946118  Iterative Reduction\n5      ir_FL_1000       1000    0.9455  Iterative Reduction\n6       ir_FL_800        800  0.944929  Iterative Reduction\n7       ir_FL_600        600  0.943393  Iterative Reduction\n8       ir_FL_400        400  0.941493  Iterative Reduction\n9       ir_FL_200        200  0.931754  Iterative Reduction\n10  boruta_FL_538        538  0.946182               Boruta\n11  boruta_FL_462        462  0.944916               Boruta\n12  boruta_FL_425        425   0.94416               Boruta\n13  boruta_FL_401        401  0.943782               Boruta\n14  boruta_FL_386        386  0.943603               Boruta\n15  boruta_FL_375        375  0.943348               Boruta\n16  boruta_FL_363        363  0.943032               Boruta\n17  boruta_FL_356        356  0.942426               Boruta\n18  boruta_FL_351        351  0.942385               Boruta\n19  boruta_FL_344        344  0.942125               Boruta\n20  boruta_FL_297        297  0.940981               Boruta\n21  boruta_FL_254        254  0.938944               Boruta\n22  boruta_FL_215        215  0.937109               Boruta\n23  boruta_FL_154        154  0.930539               Boruta","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fl_name</th>\n      <th>n_features</th>\n      <th>auc</th>\n      <th>Method</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ir_FL_2000</td>\n      <td>2000</td>\n      <td>0.945818</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ir_FL_1800</td>\n      <td>1800</td>\n      <td>0.946291</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ir_FL_1600</td>\n      <td>1600</td>\n      <td>0.94661</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ir_FL_1400</td>\n      <td>1400</td>\n      <td>0.946308</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ir_FL_1200</td>\n      <td>1200</td>\n      <td>0.946118</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ir_FL_1000</td>\n      <td>1000</td>\n      <td>0.9455</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ir_FL_800</td>\n      <td>800</td>\n      <td>0.944929</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ir_FL_600</td>\n      <td>600</td>\n      <td>0.943393</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ir_FL_400</td>\n      <td>400</td>\n      <td>0.941493</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ir_FL_200</td>\n      <td>200</td>\n      <td>0.931754</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>boruta_FL_538</td>\n      <td>538</td>\n      <td>0.946182</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>boruta_FL_462</td>\n      <td>462</td>\n      <td>0.944916</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>boruta_FL_425</td>\n      <td>425</td>\n      <td>0.94416</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>boruta_FL_401</td>\n      <td>401</td>\n      <td>0.943782</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>boruta_FL_386</td>\n      <td>386</td>\n      <td>0.943603</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>boruta_FL_375</td>\n      <td>375</td>\n      <td>0.943348</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>boruta_FL_363</td>\n      <td>363</td>\n      <td>0.943032</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>boruta_FL_356</td>\n      <td>356</td>\n      <td>0.942426</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>boruta_FL_351</td>\n      <td>351</td>\n      <td>0.942385</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>boruta_FL_344</td>\n      <td>344</td>\n      <td>0.942125</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>boruta_FL_297</td>\n      <td>297</td>\n      <td>0.940981</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>boruta_FL_254</td>\n      <td>254</td>\n      <td>0.938944</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>boruta_FL_215</td>\n      <td>215</td>\n      <td>0.937109</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>boruta_FL_154</td>\n      <td>154</td>\n      <td>0.930539</td>\n      <td>Boruta</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sns.scatterplot(data=perf,x='n_features',y=metric,hue='Method')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T17:33:38.290850Z","iopub.execute_input":"2024-04-18T17:33:38.291251Z","iopub.status.idle":"2024-04-18T17:33:38.749238Z","shell.execute_reply.started":"2024-04-18T17:33:38.291219Z","shell.execute_reply":"2024-04-18T17:33:38.748371Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]}]}