{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data Manipulating and Visualization\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats.mstats import winsorize\nimport random\n\n\n# Operating System\nimport os\nfrom datetime import datetime\n\n# Machine Learning Algorithms\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Performance metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import accuracy_score\n\n# Hyperparameter\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer, Categorical\n\n## Mathematics and Statistics\nimport scipy.stats as stats\nfrom scipy.stats import uniform\nfrom scipy.stats import loguniform\n\n# NLP related \nimport string\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem.porter import *\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-06T15:42:03.607912Z","iopub.execute_input":"2023-10-06T15:42:03.608257Z","iopub.status.idle":"2023-10-06T15:42:09.479345Z","shell.execute_reply.started":"2023-10-06T15:42:03.608229Z","shell.execute_reply":"2023-10-06T15:42:09.477897Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load modeling data\n### [The creation of the the modeling data is discussed in this notebook](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/imdb_data.ipynb)","metadata":{}},{"cell_type":"code","source":"Modeling_Date = pd.read_csv('/kaggle/input/imdb-date/Modeling_Date.csv',low_memory=False)\nX_Data = Modeling_Date.drop(['movie_review','sentiment_label','sentiment_number'], axis=1)\nY_Data = Modeling_Date.sentiment_number","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:42:09.481212Z","iopub.execute_input":"2023-10-06T15:42:09.481534Z","iopub.status.idle":"2023-10-06T15:42:45.565808Z","shell.execute_reply.started":"2023-10-06T15:42:09.481508Z","shell.execute_reply":"2023-10-06T15:42:45.564513Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### First, build a model with all the 2000 features. ","metadata":{}},{"cell_type":"code","source":"x1,x2,y1,y2 = train_test_split(X_Data,Y_Data,test_size = 0.3,random_state = 42)\n\nparams_LGB= {'boosting_type'    : 'gbdt',\n             'objective'        : 'binary',\n             'colsample_bytree' : 0.8,\n             'learning_rate'    : 0.05,\n             'min_child_samples': 10,\n             'min_child_weight' : 5,\n             'max_depth'        : -1,\n             'min_split_gain'   : 0,\n             'num_leaves'       : 31,\n             'subsample_for_bin': 50000,\n             'n_estimators'     : 5000,\n             'subsample_freq'   : 1\n}\n\nLGB_2000 = lgb.LGBMClassifier(**params_LGB,importance_type='gain')\nLGB_2000.fit(X = x1, y = y1,\n             eval_metric=['auc','logloss'], eval_set=[(x1,y1),(x2,y2)],\n             callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n\nimportance_df = pd.DataFrame(list(x1)).rename(columns={0:'Features'})\nimportance_df['importance'] = LGB_2000.feature_importances_\nimportance_df = importance_df.sort_values(by=['importance'],ascending=False).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:42:45.567493Z","iopub.execute_input":"2023-10-06T15:42:45.567902Z","iopub.status.idle":"2023-10-06T15:45:07.830102Z","shell.execute_reply.started":"2023-10-06T15:42:45.567870Z","shell.execute_reply":"2023-10-06T15:45:07.828868Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[838]\ttraining's auc: 0.996931\ttraining's binary_logloss: 0.127731\tvalid_1's auc: 0.944385\tvalid_1's binary_logloss: 0.301285\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### There are 1520 features with postive feature importance. ","metadata":{}},{"cell_type":"code","source":"importance_df[importance_df['importance']>0]","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:45:07.832624Z","iopub.execute_input":"2023-10-06T15:45:07.832985Z","iopub.status.idle":"2023-10-06T15:45:07.852374Z","shell.execute_reply.started":"2023-10-06T15:45:07.832955Z","shell.execute_reply":"2023-10-06T15:45:07.850920Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     Features    importance\n0         bad  26179.362509\n1       worst  20654.644251\n2        wast  17914.736233\n3       great  11337.439661\n4          aw   9731.041632\n...       ...           ...\n1515     luck      2.224650\n1516  univers      2.213020\n1517     fred      2.198200\n1518      dub      2.010700\n1519     wayn      1.459320\n\n[1520 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Features</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bad</td>\n      <td>26179.362509</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>worst</td>\n      <td>20654.644251</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wast</td>\n      <td>17914.736233</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>great</td>\n      <td>11337.439661</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aw</td>\n      <td>9731.041632</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1515</th>\n      <td>luck</td>\n      <td>2.224650</td>\n    </tr>\n    <tr>\n      <th>1516</th>\n      <td>univers</td>\n      <td>2.213020</td>\n    </tr>\n    <tr>\n      <th>1517</th>\n      <td>fred</td>\n      <td>2.198200</td>\n    </tr>\n    <tr>\n      <th>1518</th>\n      <td>dub</td>\n      <td>2.010700</td>\n    </tr>\n    <tr>\n      <th>1519</th>\n      <td>wayn</td>\n      <td>1.459320</td>\n    </tr>\n  </tbody>\n</table>\n<p>1520 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### To reduce the running time, I am keeping only the top 500 features among these 2000 features in the model, based on their importance. Then conduct the feature selection analysis (below) from these 500 features. ","metadata":{}},{"cell_type":"code","source":"##### Keep the top 500 features\nkeep_features = importance_df.head(500)['Features']","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:45:07.854095Z","iopub.execute_input":"2023-10-06T15:45:07.854633Z","iopub.status.idle":"2023-10-06T15:45:07.861231Z","shell.execute_reply.started":"2023-10-06T15:45:07.854588Z","shell.execute_reply":"2023-10-06T15:45:07.859693Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_Data_top500 = X_Data[keep_features]","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:45:07.864644Z","iopub.execute_input":"2023-10-06T15:45:07.864992Z","iopub.status.idle":"2023-10-06T15:45:07.944939Z","shell.execute_reply.started":"2023-10-06T15:45:07.864957Z","shell.execute_reply":"2023-10-06T15:45:07.943420Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 2. Create Random Noise Features","metadata":{}},{"cell_type":"code","source":"def create_random_noises(n_random_noise,df):\n    new_df = df.copy()\n    n_rows = len(df)\n    noise_features = []\n    \n    ##### Adding random noises that follow Normal(0,1) distribution\n    for i in range(n_random_noise):\n        new_df['norm_'+str(i+1)] = np.random.normal(0,1,n_rows)\n        noise_features = noise_features + ['norm_'+str(i+1)]\n    \n    ##### Adding random noises that follow Normal(0,1) distribution\n    for i in range(n_random_noise):\n        new_df['unif_'+str(i+1)] = np.random.uniform(0,1,n_rows)\n        noise_features = noise_features + ['unif_'+str(i+1)]\n    ##### Adding random noises that follow Bernoulli(0.5) distribution\n    for i in range(n_random_noise):\n        new_df['ber_'+str(i+1)] = np.random.binomial(1,0.5,n_rows)\n        noise_features = noise_features + ['ber_'+str(i+1)] \n        \n    return new_df,noise_features","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:45:07.946869Z","iopub.execute_input":"2023-10-06T15:45:07.947398Z","iopub.status.idle":"2023-10-06T15:45:07.960415Z","shell.execute_reply.started":"2023-10-06T15:45:07.947363Z","shell.execute_reply":"2023-10-06T15:45:07.959256Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### This is how the function works.\n- #### There are three distributions: Normal(0,1), Uniform(0,1), and Bernoulli(0.5). These are fixed. \n- #### For exampel, if n_random_noise = 3, the function creates 3 random noise features for each of the distributions. \n- #### These features are added to the original dataframe, with all the original columns unchanged. ","metadata":{}},{"cell_type":"code","source":"temp = pd.DataFrame( {'A': list(range(10)),\n        'B': list(range(10,20)),\n        'C': list(range(20,30))})\ncreate_random_noises(3,temp)","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:45:07.962070Z","iopub.execute_input":"2023-10-06T15:45:07.962384Z","iopub.status.idle":"2023-10-06T15:45:07.985182Z","shell.execute_reply.started":"2023-10-06T15:45:07.962359Z","shell.execute_reply":"2023-10-06T15:45:07.983678Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(   A   B   C    norm_1    norm_2    norm_3    unif_1    unif_2    unif_3  \\\n 0  0  10  20 -0.626849 -0.991432 -0.662590  0.975370  0.029285  0.604849   \n 1  1  11  21  0.153650  0.178153  0.418251  0.177928  0.637209  0.066638   \n 2  2  12  22  0.206623  1.916912 -0.282340  0.029553  0.269940  0.069398   \n 3  3  13  23 -0.437213 -0.440159 -0.449994  0.054007  0.273678  0.062441   \n 4  4  14  24 -0.838972  0.160270  0.032290  0.920138  0.085556  0.784156   \n 5  5  15  25  0.198322  0.979394 -0.624693  0.622266  0.672422  0.555796   \n 6  6  16  26  1.464103 -0.489683  0.255679  0.803987  0.439981  0.174949   \n 7  7  17  27  1.168045  0.450790  0.434817  0.583797  0.263613  0.567969   \n 8  8  18  28 -1.035023  0.404480  0.057958  0.895280  0.023530  0.328578   \n 9  9  19  29  0.116945  2.024884 -0.503333  0.183130  0.174395  0.133494   \n \n    ber_1  ber_2  ber_3  \n 0      1      0      1  \n 1      0      1      0  \n 2      1      1      1  \n 3      1      0      0  \n 4      0      1      0  \n 5      0      1      1  \n 6      1      0      0  \n 7      0      0      1  \n 8      1      1      0  \n 9      1      0      1  ,\n ['norm_1',\n  'norm_2',\n  'norm_3',\n  'unif_1',\n  'unif_2',\n  'unif_3',\n  'ber_1',\n  'ber_2',\n  'ber_3'])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Apply this function to X_Data (the dataframe for predictive features)","metadata":{}},{"cell_type":"code","source":"X_Data_with_noise,noise_feature_names = create_random_noises(5,X_Data_top500)","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:45:07.986419Z","iopub.execute_input":"2023-10-06T15:45:07.986717Z","iopub.status.idle":"2023-10-06T15:45:08.108928Z","shell.execute_reply.started":"2023-10-06T15:45:07.986693Z","shell.execute_reply":"2023-10-06T15:45:08.107589Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_Data_with_noise[noise_feature_names].describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:45:08.114220Z","iopub.execute_input":"2023-10-06T15:45:08.114582Z","iopub.status.idle":"2023-10-06T15:45:08.217418Z","shell.execute_reply.started":"2023-10-06T15:45:08.114554Z","shell.execute_reply":"2023-10-06T15:45:08.216289Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"             norm_1        norm_2        norm_3        norm_4        norm_5  \\\ncount  49582.000000  49582.000000  49582.000000  49582.000000  49582.000000   \nmean       0.001828      0.001218     -0.007725     -0.001759     -0.009726   \nstd        0.996881      1.000142      1.000060      0.994839      0.998677   \nmin       -4.053176     -4.593791     -4.530292     -4.140811     -4.245464   \n25%       -0.670963     -0.675347     -0.680455     -0.668804     -0.685493   \n50%        0.001923     -0.000599     -0.007004     -0.004916     -0.009682   \n75%        0.671993      0.677425      0.671397      0.674417      0.660919   \nmax        3.762298      3.959252      4.132506      3.765039      4.230507   \n\n             unif_1        unif_2        unif_3        unif_4        unif_5  \\\ncount  49582.000000  49582.000000  49582.000000  49582.000000  49582.000000   \nmean       0.500396      0.499292      0.500662      0.502463      0.500052   \nstd        0.289390      0.289421      0.289109      0.288868      0.288815   \nmin        0.000021      0.000012      0.000036      0.000009      0.000038   \n25%        0.248966      0.245926      0.249252      0.252811      0.250181   \n50%        0.502668      0.499558      0.499783      0.502308      0.501425   \n75%        0.751648      0.750786      0.752307      0.753162      0.749737   \nmax        0.999989      0.999993      0.999971      0.999975      0.999985   \n\n              ber_1         ber_2         ber_3         ber_4         ber_5  \ncount  49582.000000  49582.000000  49582.000000  49582.000000  49582.000000  \nmean       0.500766      0.499839      0.500262      0.501936      0.496975  \nstd        0.500004      0.500005      0.500005      0.500001      0.499996  \nmin        0.000000      0.000000      0.000000      0.000000      0.000000  \n25%        0.000000      0.000000      0.000000      0.000000      0.000000  \n50%        1.000000      0.000000      1.000000      1.000000      0.000000  \n75%        1.000000      1.000000      1.000000      1.000000      1.000000  \nmax        1.000000      1.000000      1.000000      1.000000      1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>norm_1</th>\n      <th>norm_2</th>\n      <th>norm_3</th>\n      <th>norm_4</th>\n      <th>norm_5</th>\n      <th>unif_1</th>\n      <th>unif_2</th>\n      <th>unif_3</th>\n      <th>unif_4</th>\n      <th>unif_5</th>\n      <th>ber_1</th>\n      <th>ber_2</th>\n      <th>ber_3</th>\n      <th>ber_4</th>\n      <th>ber_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n      <td>49582.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.001828</td>\n      <td>0.001218</td>\n      <td>-0.007725</td>\n      <td>-0.001759</td>\n      <td>-0.009726</td>\n      <td>0.500396</td>\n      <td>0.499292</td>\n      <td>0.500662</td>\n      <td>0.502463</td>\n      <td>0.500052</td>\n      <td>0.500766</td>\n      <td>0.499839</td>\n      <td>0.500262</td>\n      <td>0.501936</td>\n      <td>0.496975</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.996881</td>\n      <td>1.000142</td>\n      <td>1.000060</td>\n      <td>0.994839</td>\n      <td>0.998677</td>\n      <td>0.289390</td>\n      <td>0.289421</td>\n      <td>0.289109</td>\n      <td>0.288868</td>\n      <td>0.288815</td>\n      <td>0.500004</td>\n      <td>0.500005</td>\n      <td>0.500005</td>\n      <td>0.500001</td>\n      <td>0.499996</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-4.053176</td>\n      <td>-4.593791</td>\n      <td>-4.530292</td>\n      <td>-4.140811</td>\n      <td>-4.245464</td>\n      <td>0.000021</td>\n      <td>0.000012</td>\n      <td>0.000036</td>\n      <td>0.000009</td>\n      <td>0.000038</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.670963</td>\n      <td>-0.675347</td>\n      <td>-0.680455</td>\n      <td>-0.668804</td>\n      <td>-0.685493</td>\n      <td>0.248966</td>\n      <td>0.245926</td>\n      <td>0.249252</td>\n      <td>0.252811</td>\n      <td>0.250181</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.001923</td>\n      <td>-0.000599</td>\n      <td>-0.007004</td>\n      <td>-0.004916</td>\n      <td>-0.009682</td>\n      <td>0.502668</td>\n      <td>0.499558</td>\n      <td>0.499783</td>\n      <td>0.502308</td>\n      <td>0.501425</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.671993</td>\n      <td>0.677425</td>\n      <td>0.671397</td>\n      <td>0.674417</td>\n      <td>0.660919</td>\n      <td>0.751648</td>\n      <td>0.750786</td>\n      <td>0.752307</td>\n      <td>0.753162</td>\n      <td>0.749737</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.762298</td>\n      <td>3.959252</td>\n      <td>4.132506</td>\n      <td>3.765039</td>\n      <td>4.230507</td>\n      <td>0.999989</td>\n      <td>0.999993</td>\n      <td>0.999971</td>\n      <td>0.999975</td>\n      <td>0.999985</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 3. Random Noise Test\n### The random noise features do not have any intuitions, and therefore should not be selected. The goal of the random noise test is to evaluate a feature selection method, according to whether the method is able to identify these random noise feauture, and not to select them.\n### According to the results below, \n- #### The Iterative Reduction approach is not able to identify random noise features that follow N(0,1) and Uniform[0,1] distributions.\n- #### The Boruta approach works better at identifying the random noise features. ","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Iterative Reduction\n### See introduction about the Iterative Reduction [in this notebook](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/imdb_featureselection.ipynb). \n### The following codes perform the IR approach.","metadata":{}},{"cell_type":"code","source":"def Run_IR(params_LGB,importance_type,x1,x2,y1,y2,n_feature_list):\n    def get_importance_from_LGBM(model,x_data,importance_type):\n        ### Get the feature importance list, merge with the columns names, and sorted by the importance score. \n        importance_df = pd.DataFrame(list(x_data)).rename(columns={0:'Features'})\n        importance_df[importance_type] = model.feature_importances_\n        importance_df = importance_df.sort_values(by=[importance_type],ascending=False)\n        return importance_df\n\n    def iterative_reduction(start_n,end_n,importance_dict):\n        print('Feature reduction from %d to %d ...' %(start_n,end_n))\n        importance_old = importance_dict['importance_'+str(start_n)]\n        Selected_Features = importance_old.head(end_n)['Features'].values.tolist()\n        Shuffled_Features = Selected_Features.copy()\n        random.shuffle(Shuffled_Features)\n        x1_new = x1[Shuffled_Features]\n        x2_new = x2[Shuffled_Features]\n        \n        LGB_now = lgb.LGBMClassifier(**params_LGB,importance_type=importance_type)\n        LGB_now.fit(X = x1_new, y = y1,\n                    eval_metric=['logloss'], eval_set=[(x1_new,y1),(x2_new,y2)],\n                    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n        importance_now = get_importance_from_LGBM(LGB_now,x1_new,importance_type)\n        \n        updated_importance_dict = importance_dict.copy()\n        updated_importance_dict['importance_'+str(end_n)] = importance_now\n        return(updated_importance_dict)\n\n    ##### Create a dict to store the importance list\n    importance_dict   = {}\n    \n    ##### First iteration (including all the features considered)\n    LGB_515 = lgb.LGBMClassifier(**params_LGB,importance_type=importance_type)\n    LGB_515.fit(X = x1, y = y1,\n                 eval_metric=['auc','logloss'], eval_set=[(x1,y1),(x2,y2)],\n                 callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n    importance_515 = get_importance_from_LGBM(LGB_515,x1,importance_type)\n    importance_dict['importance_515'] = importance_515\n    \n    ##### Run Iteration Reduction\n    for a in range(len(n_feature_list)-1):\n        importance_dict = iterative_reduction(n_feature_list[a], n_feature_list[a+1],importance_dict)\n    \n    ##### Create a dict to store the feature list\n    feature_list_dict = {}\n    for i in range(len(n_feature_list)):\n        temp = importance_dict['importance_'+str(n_feature_list[i])]\n        feature_list_dict['fl_ir_'+str(n_feature_list[i])] = temp['Features']\n    \n    return feature_list_dict","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:45:08.219459Z","iopub.execute_input":"2023-10-06T15:45:08.219812Z","iopub.status.idle":"2023-10-06T15:45:08.234087Z","shell.execute_reply.started":"2023-10-06T15:45:08.219761Z","shell.execute_reply":"2023-10-06T15:45:08.232874Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"n_feature_list   =  [515]+[500-i*20 for i in range(20)]\nx1,x2,y1,y2 = train_test_split(X_Data_with_noise,Y_Data,test_size = 0.3,random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:45:08.235659Z","iopub.execute_input":"2023-10-06T15:45:08.236008Z","iopub.status.idle":"2023-10-06T15:45:08.457061Z","shell.execute_reply.started":"2023-10-06T15:45:08.235979Z","shell.execute_reply":"2023-10-06T15:45:08.455582Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"IR_fl = Run_IR(params_LGB,'gain',x1,x2,y1,y2,n_feature_list)","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:45:08.459321Z","iopub.execute_input":"2023-10-06T15:45:08.459870Z","iopub.status.idle":"2023-10-06T15:55:53.899576Z","shell.execute_reply.started":"2023-10-06T15:45:08.459823Z","shell.execute_reply":"2023-10-06T15:55:53.898284Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[626]\ttraining's auc: 0.991925\ttraining's binary_logloss: 0.162885\tvalid_1's auc: 0.940761\tvalid_1's binary_logloss: 0.310783\nFeature reduction from 515 to 500 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[848]\ttraining's binary_logloss: 0.13375\tvalid_1's binary_logloss: 0.312329\nFeature reduction from 500 to 480 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[755]\ttraining's binary_logloss: 0.145717\tvalid_1's binary_logloss: 0.311656\nFeature reduction from 480 to 460 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[684]\ttraining's binary_logloss: 0.15587\tvalid_1's binary_logloss: 0.312514\nFeature reduction from 460 to 440 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[678]\ttraining's binary_logloss: 0.157631\tvalid_1's binary_logloss: 0.315834\nFeature reduction from 440 to 420 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[539]\ttraining's binary_logloss: 0.180654\tvalid_1's binary_logloss: 0.314939\nFeature reduction from 420 to 400 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[626]\ttraining's binary_logloss: 0.16796\tvalid_1's binary_logloss: 0.315076\nFeature reduction from 400 to 380 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[568]\ttraining's binary_logloss: 0.178466\tvalid_1's binary_logloss: 0.315802\nFeature reduction from 380 to 360 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[552]\ttraining's binary_logloss: 0.182549\tvalid_1's binary_logloss: 0.317704\nFeature reduction from 360 to 340 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[517]\ttraining's binary_logloss: 0.189476\tvalid_1's binary_logloss: 0.31966\nFeature reduction from 340 to 320 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[709]\ttraining's binary_logloss: 0.162402\tvalid_1's binary_logloss: 0.320687\nFeature reduction from 320 to 300 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[528]\ttraining's binary_logloss: 0.192061\tvalid_1's binary_logloss: 0.323616\nFeature reduction from 300 to 280 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[498]\ttraining's binary_logloss: 0.199173\tvalid_1's binary_logloss: 0.327187\nFeature reduction from 280 to 260 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[509]\ttraining's binary_logloss: 0.200259\tvalid_1's binary_logloss: 0.33097\nFeature reduction from 260 to 240 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[508]\ttraining's binary_logloss: 0.202721\tvalid_1's binary_logloss: 0.33298\nFeature reduction from 240 to 220 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[487]\ttraining's binary_logloss: 0.21035\tvalid_1's binary_logloss: 0.337199\nFeature reduction from 220 to 200 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[380]\ttraining's binary_logloss: 0.234307\tvalid_1's binary_logloss: 0.340562\nFeature reduction from 200 to 180 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[399]\ttraining's binary_logloss: 0.235747\tvalid_1's binary_logloss: 0.342801\nFeature reduction from 180 to 160 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[447]\ttraining's binary_logloss: 0.233206\tvalid_1's binary_logloss: 0.345363\nFeature reduction from 160 to 140 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[380]\ttraining's binary_logloss: 0.253054\tvalid_1's binary_logloss: 0.354041\nFeature reduction from 140 to 120 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[353]\ttraining's binary_logloss: 0.266745\tvalid_1's binary_logloss: 0.361746\n","output_type":"stream"}]},{"cell_type":"code","source":"fl_names = ['fl_ir_'+ str(i) for i in n_feature_list]\nrank_ir_df = pd.DataFrame(columns=['feature_list','n_features']+noise_feature_names)\nfor fl in range(len(fl_names)):\n    fl_now = list(IR_fl[fl_names[fl]])\n    rank_ir_df.loc[fl,'feature_list'] = fl_names[fl]\n    rank_ir_df.loc[fl,'n_features']   = len(fl_now)\n    for noise_name in noise_feature_names:\n        if noise_name in fl_now: \n            rank_ir_df.loc[fl,noise_name] = fl_now.index(noise_name)+1\n        else:\n            rank_ir_df.loc[fl,noise_name] = np.nan\n\nrank_ir_df","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:55:53.901461Z","iopub.execute_input":"2023-10-06T15:55:53.901784Z","iopub.status.idle":"2023-10-06T15:55:53.968161Z","shell.execute_reply.started":"2023-10-06T15:55:53.901758Z","shell.execute_reply":"2023-10-06T15:55:53.966888Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   feature_list n_features norm_1 norm_2 norm_3 norm_4 norm_5 unif_1 unif_2  \\\n0     fl_ir_515        515    107    101    142    106    112    135     99   \n1     fl_ir_500        500     80     64     87     59     74     84     63   \n2     fl_ir_480        480     94     72     85     64     86     96     75   \n3     fl_ir_460        460     98     97     95     88    105    111     93   \n4     fl_ir_440        440    104     92    103     75    101    116     74   \n5     fl_ir_420        420    117    108    138    100    121    153    114   \n6     fl_ir_400        400     94     80    108     86    103    114     92   \n7     fl_ir_380        380    103     96    133     90    112    157    102   \n8     fl_ir_360        360     98    122    112    110    104    120     94   \n9     fl_ir_340        340    108     96    121    116    117    151    102   \n10    fl_ir_320        320     64     62     80     63     75     73     61   \n11    fl_ir_300        300    103    100    126    107    105    131     98   \n12    fl_ir_280        280     97    111    114     91    109    128     99   \n13    fl_ir_260        260    110    111    105     92    100    108     93   \n14    fl_ir_240        240     92     95    114     88     97    106    105   \n15    fl_ir_220        220     86    111    113     83     75    102    100   \n16    fl_ir_200        200    144    125    142    121    139    177    173   \n17    fl_ir_180        180    107    124    115    103    104     96    111   \n18    fl_ir_160        160     82     93     80     79     90    103     77   \n19    fl_ir_140        140    103    109    116     87     80    102     79   \n20    fl_ir_120        120    108    109    120     75     87    105    100   \n\n   unif_3 unif_4 unif_5 ber_1 ber_2 ber_3 ber_4 ber_5  \n0      93     86    111   511   509   512   514   515  \n1      62     56     65   NaN   NaN   NaN   NaN   NaN  \n2      89     65     82   NaN   NaN   NaN   NaN   NaN  \n3      77     73     90   NaN   NaN   NaN   NaN   NaN  \n4      78     60     95   NaN   NaN   NaN   NaN   NaN  \n5     106     99    112   NaN   NaN   NaN   NaN   NaN  \n6      88     78     97   NaN   NaN   NaN   NaN   NaN  \n7      98     69     97   NaN   NaN   NaN   NaN   NaN  \n8      87     80     97   NaN   NaN   NaN   NaN   NaN  \n9      89     97    110   NaN   NaN   NaN   NaN   NaN  \n10     49     56     60   NaN   NaN   NaN   NaN   NaN  \n11     89     77     97   NaN   NaN   NaN   NaN   NaN  \n12     88     86    101   NaN   NaN   NaN   NaN   NaN  \n13     84     74    102   NaN   NaN   NaN   NaN   NaN  \n14     86     84     75   NaN   NaN   NaN   NaN   NaN  \n15     95     72     93   NaN   NaN   NaN   NaN   NaN  \n16    115    103    138   NaN   NaN   NaN   NaN   NaN  \n17     93     88    122   NaN   NaN   NaN   NaN   NaN  \n18     70     58     91   NaN   NaN   NaN   NaN   NaN  \n19     95    105     77   NaN   NaN   NaN   NaN   NaN  \n20     92     85    118   NaN   NaN   NaN   NaN   NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_list</th>\n      <th>n_features</th>\n      <th>norm_1</th>\n      <th>norm_2</th>\n      <th>norm_3</th>\n      <th>norm_4</th>\n      <th>norm_5</th>\n      <th>unif_1</th>\n      <th>unif_2</th>\n      <th>unif_3</th>\n      <th>unif_4</th>\n      <th>unif_5</th>\n      <th>ber_1</th>\n      <th>ber_2</th>\n      <th>ber_3</th>\n      <th>ber_4</th>\n      <th>ber_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fl_ir_515</td>\n      <td>515</td>\n      <td>107</td>\n      <td>101</td>\n      <td>142</td>\n      <td>106</td>\n      <td>112</td>\n      <td>135</td>\n      <td>99</td>\n      <td>93</td>\n      <td>86</td>\n      <td>111</td>\n      <td>511</td>\n      <td>509</td>\n      <td>512</td>\n      <td>514</td>\n      <td>515</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fl_ir_500</td>\n      <td>500</td>\n      <td>80</td>\n      <td>64</td>\n      <td>87</td>\n      <td>59</td>\n      <td>74</td>\n      <td>84</td>\n      <td>63</td>\n      <td>62</td>\n      <td>56</td>\n      <td>65</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fl_ir_480</td>\n      <td>480</td>\n      <td>94</td>\n      <td>72</td>\n      <td>85</td>\n      <td>64</td>\n      <td>86</td>\n      <td>96</td>\n      <td>75</td>\n      <td>89</td>\n      <td>65</td>\n      <td>82</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fl_ir_460</td>\n      <td>460</td>\n      <td>98</td>\n      <td>97</td>\n      <td>95</td>\n      <td>88</td>\n      <td>105</td>\n      <td>111</td>\n      <td>93</td>\n      <td>77</td>\n      <td>73</td>\n      <td>90</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fl_ir_440</td>\n      <td>440</td>\n      <td>104</td>\n      <td>92</td>\n      <td>103</td>\n      <td>75</td>\n      <td>101</td>\n      <td>116</td>\n      <td>74</td>\n      <td>78</td>\n      <td>60</td>\n      <td>95</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fl_ir_420</td>\n      <td>420</td>\n      <td>117</td>\n      <td>108</td>\n      <td>138</td>\n      <td>100</td>\n      <td>121</td>\n      <td>153</td>\n      <td>114</td>\n      <td>106</td>\n      <td>99</td>\n      <td>112</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>fl_ir_400</td>\n      <td>400</td>\n      <td>94</td>\n      <td>80</td>\n      <td>108</td>\n      <td>86</td>\n      <td>103</td>\n      <td>114</td>\n      <td>92</td>\n      <td>88</td>\n      <td>78</td>\n      <td>97</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>fl_ir_380</td>\n      <td>380</td>\n      <td>103</td>\n      <td>96</td>\n      <td>133</td>\n      <td>90</td>\n      <td>112</td>\n      <td>157</td>\n      <td>102</td>\n      <td>98</td>\n      <td>69</td>\n      <td>97</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fl_ir_360</td>\n      <td>360</td>\n      <td>98</td>\n      <td>122</td>\n      <td>112</td>\n      <td>110</td>\n      <td>104</td>\n      <td>120</td>\n      <td>94</td>\n      <td>87</td>\n      <td>80</td>\n      <td>97</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fl_ir_340</td>\n      <td>340</td>\n      <td>108</td>\n      <td>96</td>\n      <td>121</td>\n      <td>116</td>\n      <td>117</td>\n      <td>151</td>\n      <td>102</td>\n      <td>89</td>\n      <td>97</td>\n      <td>110</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>fl_ir_320</td>\n      <td>320</td>\n      <td>64</td>\n      <td>62</td>\n      <td>80</td>\n      <td>63</td>\n      <td>75</td>\n      <td>73</td>\n      <td>61</td>\n      <td>49</td>\n      <td>56</td>\n      <td>60</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>fl_ir_300</td>\n      <td>300</td>\n      <td>103</td>\n      <td>100</td>\n      <td>126</td>\n      <td>107</td>\n      <td>105</td>\n      <td>131</td>\n      <td>98</td>\n      <td>89</td>\n      <td>77</td>\n      <td>97</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>fl_ir_280</td>\n      <td>280</td>\n      <td>97</td>\n      <td>111</td>\n      <td>114</td>\n      <td>91</td>\n      <td>109</td>\n      <td>128</td>\n      <td>99</td>\n      <td>88</td>\n      <td>86</td>\n      <td>101</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>fl_ir_260</td>\n      <td>260</td>\n      <td>110</td>\n      <td>111</td>\n      <td>105</td>\n      <td>92</td>\n      <td>100</td>\n      <td>108</td>\n      <td>93</td>\n      <td>84</td>\n      <td>74</td>\n      <td>102</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>fl_ir_240</td>\n      <td>240</td>\n      <td>92</td>\n      <td>95</td>\n      <td>114</td>\n      <td>88</td>\n      <td>97</td>\n      <td>106</td>\n      <td>105</td>\n      <td>86</td>\n      <td>84</td>\n      <td>75</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>fl_ir_220</td>\n      <td>220</td>\n      <td>86</td>\n      <td>111</td>\n      <td>113</td>\n      <td>83</td>\n      <td>75</td>\n      <td>102</td>\n      <td>100</td>\n      <td>95</td>\n      <td>72</td>\n      <td>93</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>fl_ir_200</td>\n      <td>200</td>\n      <td>144</td>\n      <td>125</td>\n      <td>142</td>\n      <td>121</td>\n      <td>139</td>\n      <td>177</td>\n      <td>173</td>\n      <td>115</td>\n      <td>103</td>\n      <td>138</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>fl_ir_180</td>\n      <td>180</td>\n      <td>107</td>\n      <td>124</td>\n      <td>115</td>\n      <td>103</td>\n      <td>104</td>\n      <td>96</td>\n      <td>111</td>\n      <td>93</td>\n      <td>88</td>\n      <td>122</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>fl_ir_160</td>\n      <td>160</td>\n      <td>82</td>\n      <td>93</td>\n      <td>80</td>\n      <td>79</td>\n      <td>90</td>\n      <td>103</td>\n      <td>77</td>\n      <td>70</td>\n      <td>58</td>\n      <td>91</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>fl_ir_140</td>\n      <td>140</td>\n      <td>103</td>\n      <td>109</td>\n      <td>116</td>\n      <td>87</td>\n      <td>80</td>\n      <td>102</td>\n      <td>79</td>\n      <td>95</td>\n      <td>105</td>\n      <td>77</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>fl_ir_120</td>\n      <td>120</td>\n      <td>108</td>\n      <td>109</td>\n      <td>120</td>\n      <td>75</td>\n      <td>87</td>\n      <td>105</td>\n      <td>100</td>\n      <td>92</td>\n      <td>85</td>\n      <td>118</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Observations \n#### - Among the 15 random noise features, all of the Normal noises and the Uniform noises are relatively important. \n#### - Through the IR procedure, their feature importance ranks relatively high, within the top 100. Therefore, the IR approach cannot successfully remove them.","metadata":{}},{"cell_type":"markdown","source":"## 3.2 Boruta\n### See introduction about the Boruta [in this notebook](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/imdb_featureselection.ipynb). \n### The following codes perform the Boruta approach.","metadata":{}},{"cell_type":"code","source":"def Run_Boruta(params_LGB,importance_type,X_Data,Y_Data,TH_values,N_iteration):\n    hits            = np.zeros(len(X_Data.columns))\n    early_stopping_rounds = 50\n    for iter_ in range(N_iteration):\n        np.random.seed(iter_)\n        X_shadow = X_Data.apply(np.random.permutation)\n        X_shadow.columns = ['shadow'+ feature for feature in X_Data.columns]\n        X_boruta = pd.concat([X_Data,X_shadow],axis=1,ignore_index=True)\n        x_train,x_valid,y_train,y_valid = train_test_split(X_boruta,\n                                                            Y_Data,\n                                                            test_size = 0.3, \n                                                            random_state = 100)\n        LGB_model = lgb.LGBMClassifier(**params_LGB,importance_type=importance_type)\n        LGB_model.fit(X = x_train, y = y_train,\n                     eval_metric=['auc','logloss'], eval_set=[(x_train,y_train),(x_valid,y_valid)],\n                     callbacks=[lgb.early_stopping(early_stopping_rounds), lgb.log_evaluation(0)])\n        feature_imp_X      = LGB_model.feature_importances_[:len(X_Data.columns)]\n        feature_imp_shadow = LGB_model.feature_importances_[len(X_Data.columns):]\n        hits+=(feature_imp_X>feature_imp_shadow.max())\n    \n    Hits_df = pd.DataFrame(columns=['Feature','Hits'])\n    Hits_df.iloc[:,0] = X_Data.columns\n    Hits_df.iloc[:,1] = hits\n    \n    feature_list_dict = {}\n    for i in range(len(TH_values)):\n        feature_keep = Hits_df[Hits_df['Hits']>=TH_values[i]]['Feature']\n        feature_list_dict['fl_boruta_'+str(TH_values[i])] = feature_keep\n    return feature_list_dict","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:55:53.969870Z","iopub.execute_input":"2023-10-06T15:55:53.970491Z","iopub.status.idle":"2023-10-06T15:55:53.983394Z","shell.execute_reply.started":"2023-10-06T15:55:53.970444Z","shell.execute_reply":"2023-10-06T15:55:53.982087Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"TH_values=[1,2,3,4,5,6,7,8,9,10,20,30,40,50]\nBoruta_fl = Run_Boruta(params_LGB,'gain',X_Data_with_noise,Y_Data,TH_values,50)","metadata":{"execution":{"iopub.status.busy":"2023-10-06T15:55:53.985462Z","iopub.execute_input":"2023-10-06T15:55:53.985988Z","iopub.status.idle":"2023-10-06T17:02:17.255446Z","shell.execute_reply.started":"2023-10-06T15:55:53.985942Z","shell.execute_reply":"2023-10-06T17:02:17.254219Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[831]\ttraining's auc: 0.997729\ttraining's binary_logloss: 0.127295\tvalid_1's auc: 0.942126\tvalid_1's binary_logloss: 0.306996\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[601]\ttraining's auc: 0.993025\ttraining's binary_logloss: 0.162839\tvalid_1's auc: 0.942198\tvalid_1's binary_logloss: 0.307753\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[633]\ttraining's auc: 0.994177\ttraining's binary_logloss: 0.156448\tvalid_1's auc: 0.942195\tvalid_1's binary_logloss: 0.307544\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[744]\ttraining's auc: 0.9966\ttraining's binary_logloss: 0.138872\tvalid_1's auc: 0.942006\tvalid_1's binary_logloss: 0.307496\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[561]\ttraining's auc: 0.991727\ttraining's binary_logloss: 0.170365\tvalid_1's auc: 0.941774\tvalid_1's binary_logloss: 0.308926\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[487]\ttraining's auc: 0.988906\ttraining's binary_logloss: 0.184137\tvalid_1's auc: 0.941449\tvalid_1's binary_logloss: 0.310478\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[684]\ttraining's auc: 0.995365\ttraining's binary_logloss: 0.148382\tvalid_1's auc: 0.942632\tvalid_1's binary_logloss: 0.306254\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[760]\ttraining's auc: 0.996703\ttraining's binary_logloss: 0.137272\tvalid_1's auc: 0.942092\tvalid_1's binary_logloss: 0.307403\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[696]\ttraining's auc: 0.995682\ttraining's binary_logloss: 0.146478\tvalid_1's auc: 0.941875\tvalid_1's binary_logloss: 0.307996\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[566]\ttraining's auc: 0.992035\ttraining's binary_logloss: 0.168881\tvalid_1's auc: 0.941942\tvalid_1's binary_logloss: 0.308584\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[672]\ttraining's auc: 0.995027\ttraining's binary_logloss: 0.150673\tvalid_1's auc: 0.94218\tvalid_1's binary_logloss: 0.307347\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[677]\ttraining's auc: 0.99513\ttraining's binary_logloss: 0.149758\tvalid_1's auc: 0.941921\tvalid_1's binary_logloss: 0.307859\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[725]\ttraining's auc: 0.996118\ttraining's binary_logloss: 0.142471\tvalid_1's auc: 0.94221\tvalid_1's binary_logloss: 0.306985\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[750]\ttraining's auc: 0.996655\ttraining's binary_logloss: 0.138318\tvalid_1's auc: 0.942342\tvalid_1's binary_logloss: 0.306678\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[633]\ttraining's auc: 0.994179\ttraining's binary_logloss: 0.156566\tvalid_1's auc: 0.94224\tvalid_1's binary_logloss: 0.30744\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[819]\ttraining's auc: 0.997628\ttraining's binary_logloss: 0.128651\tvalid_1's auc: 0.942329\tvalid_1's binary_logloss: 0.306391\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[612]\ttraining's auc: 0.993358\ttraining's binary_logloss: 0.160984\tvalid_1's auc: 0.941763\tvalid_1's binary_logloss: 0.308796\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[573]\ttraining's auc: 0.992339\ttraining's binary_logloss: 0.167243\tvalid_1's auc: 0.942036\tvalid_1's binary_logloss: 0.308357\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[614]\ttraining's auc: 0.993664\ttraining's binary_logloss: 0.159588\tvalid_1's auc: 0.941903\tvalid_1's binary_logloss: 0.308333\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[522]\ttraining's auc: 0.99021\ttraining's binary_logloss: 0.177645\tvalid_1's auc: 0.941686\tvalid_1's binary_logloss: 0.309564\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[685]\ttraining's auc: 0.995422\ttraining's binary_logloss: 0.148093\tvalid_1's auc: 0.942026\tvalid_1's binary_logloss: 0.307701\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[648]\ttraining's auc: 0.994559\ttraining's binary_logloss: 0.154035\tvalid_1's auc: 0.942154\tvalid_1's binary_logloss: 0.307576\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[632]\ttraining's auc: 0.994078\ttraining's binary_logloss: 0.156927\tvalid_1's auc: 0.942043\tvalid_1's binary_logloss: 0.307832\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[552]\ttraining's auc: 0.991761\ttraining's binary_logloss: 0.170448\tvalid_1's auc: 0.942207\tvalid_1's binary_logloss: 0.307997\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[651]\ttraining's auc: 0.9945\ttraining's binary_logloss: 0.154035\tvalid_1's auc: 0.942105\tvalid_1's binary_logloss: 0.307602\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[747]\ttraining's auc: 0.996423\ttraining's binary_logloss: 0.139592\tvalid_1's auc: 0.941735\tvalid_1's binary_logloss: 0.308153\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[635]\ttraining's auc: 0.994168\ttraining's binary_logloss: 0.156647\tvalid_1's auc: 0.942177\tvalid_1's binary_logloss: 0.307459\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[789]\ttraining's auc: 0.997212\ttraining's binary_logloss: 0.13269\tvalid_1's auc: 0.942364\tvalid_1's binary_logloss: 0.306542\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[664]\ttraining's auc: 0.994903\ttraining's binary_logloss: 0.151761\tvalid_1's auc: 0.942581\tvalid_1's binary_logloss: 0.30651\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[563]\ttraining's auc: 0.991947\ttraining's binary_logloss: 0.169267\tvalid_1's auc: 0.942621\tvalid_1's binary_logloss: 0.306915\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[726]\ttraining's auc: 0.996176\ttraining's binary_logloss: 0.142587\tvalid_1's auc: 0.941642\tvalid_1's binary_logloss: 0.308505\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[567]\ttraining's auc: 0.992081\ttraining's binary_logloss: 0.168473\tvalid_1's auc: 0.942043\tvalid_1's binary_logloss: 0.308388\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[661]\ttraining's auc: 0.994761\ttraining's binary_logloss: 0.152133\tvalid_1's auc: 0.942447\tvalid_1's binary_logloss: 0.306705\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[602]\ttraining's auc: 0.993184\ttraining's binary_logloss: 0.162284\tvalid_1's auc: 0.942554\tvalid_1's binary_logloss: 0.306775\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[636]\ttraining's auc: 0.994068\ttraining's binary_logloss: 0.157139\tvalid_1's auc: 0.942384\tvalid_1's binary_logloss: 0.307036\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[548]\ttraining's auc: 0.991369\ttraining's binary_logloss: 0.172294\tvalid_1's auc: 0.941817\tvalid_1's binary_logloss: 0.309114\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[716]\ttraining's auc: 0.995957\ttraining's binary_logloss: 0.143585\tvalid_1's auc: 0.942489\tvalid_1's binary_logloss: 0.306364\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[770]\ttraining's auc: 0.996897\ttraining's binary_logloss: 0.135707\tvalid_1's auc: 0.941872\tvalid_1's binary_logloss: 0.307786\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[654]\ttraining's auc: 0.994683\ttraining's binary_logloss: 0.153426\tvalid_1's auc: 0.942337\tvalid_1's binary_logloss: 0.307088\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[619]\ttraining's auc: 0.993557\ttraining's binary_logloss: 0.159613\tvalid_1's auc: 0.94192\tvalid_1's binary_logloss: 0.308326\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[525]\ttraining's auc: 0.990489\ttraining's binary_logloss: 0.176566\tvalid_1's auc: 0.941715\tvalid_1's binary_logloss: 0.309499\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[554]\ttraining's auc: 0.991523\ttraining's binary_logloss: 0.171282\tvalid_1's auc: 0.941976\tvalid_1's binary_logloss: 0.308594\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[601]\ttraining's auc: 0.993301\ttraining's binary_logloss: 0.161861\tvalid_1's auc: 0.942055\tvalid_1's binary_logloss: 0.308097\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[606]\ttraining's auc: 0.993286\ttraining's binary_logloss: 0.161673\tvalid_1's auc: 0.942158\tvalid_1's binary_logloss: 0.307726\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[811]\ttraining's auc: 0.997468\ttraining's binary_logloss: 0.130178\tvalid_1's auc: 0.942026\tvalid_1's binary_logloss: 0.307296\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[512]\ttraining's auc: 0.990018\ttraining's binary_logloss: 0.179024\tvalid_1's auc: 0.941711\tvalid_1's binary_logloss: 0.309601\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[602]\ttraining's auc: 0.993334\ttraining's binary_logloss: 0.161671\tvalid_1's auc: 0.942115\tvalid_1's binary_logloss: 0.307934\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[683]\ttraining's auc: 0.995386\ttraining's binary_logloss: 0.148068\tvalid_1's auc: 0.942102\tvalid_1's binary_logloss: 0.307487\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[721]\ttraining's auc: 0.996136\ttraining's binary_logloss: 0.142371\tvalid_1's auc: 0.942069\tvalid_1's binary_logloss: 0.307457\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[927]\ttraining's auc: 0.998688\ttraining's binary_logloss: 0.115408\tvalid_1's auc: 0.942244\tvalid_1's binary_logloss: 0.306742\n","output_type":"stream"}]},{"cell_type":"code","source":"fl_names = ['fl_boruta_'+ str(i) for i in TH_values]\nnoise_boruta_df = pd.DataFrame(columns=['feature_list','n_features']+noise_feature_names)\nfor fl in range(len(fl_names)):\n    fl_now = list(Boruta_fl[fl_names[fl]])\n    noise_boruta_df.loc[fl,'feature_list'] = fl_names[fl]\n    noise_boruta_df.loc[fl,'n_features']   = len(fl_now)\n    for noise_name in noise_feature_names:\n        if noise_name in fl_now: \n            noise_boruta_df.loc[fl,noise_name] = 1\n        else:\n            noise_boruta_df.loc[fl,noise_name] = 0\nnoise_boruta_df","metadata":{"execution":{"iopub.status.busy":"2023-10-06T17:45:52.937250Z","iopub.execute_input":"2023-10-06T17:45:52.938116Z","iopub.status.idle":"2023-10-06T17:45:52.999202Z","shell.execute_reply.started":"2023-10-06T17:45:52.938076Z","shell.execute_reply":"2023-10-06T17:45:52.998081Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"    feature_list n_features norm_1 norm_2 norm_3 norm_4 norm_5 unif_1 unif_2  \\\n0    fl_boruta_1        187      0      1      0      1      0      0      1   \n1    fl_boruta_2        179      0      1      0      1      0      0      1   \n2    fl_boruta_3        178      0      0      0      1      0      0      1   \n3    fl_boruta_4        166      0      0      0      1      0      0      1   \n4    fl_boruta_5        162      0      0      0      1      0      0      1   \n5    fl_boruta_6        159      0      0      0      1      0      0      1   \n6    fl_boruta_7        156      0      0      0      1      0      0      1   \n7    fl_boruta_8        155      0      0      0      1      0      0      1   \n8    fl_boruta_9        149      0      0      0      1      0      0      0   \n9   fl_boruta_10        147      0      0      0      1      0      0      0   \n10  fl_boruta_20        125      0      0      0      0      0      0      0   \n11  fl_boruta_30        112      0      0      0      0      0      0      0   \n12  fl_boruta_40        104      0      0      0      0      0      0      0   \n13  fl_boruta_50         93      0      0      0      0      0      0      0   \n\n   unif_3 unif_4 unif_5 ber_1 ber_2 ber_3 ber_4 ber_5  \n0       1      1      1     0     0     0     0     0  \n1       1      0      0     0     0     0     0     0  \n2       1      0      0     0     0     0     0     0  \n3       1      0      0     0     0     0     0     0  \n4       1      0      0     0     0     0     0     0  \n5       1      0      0     0     0     0     0     0  \n6       1      0      0     0     0     0     0     0  \n7       1      0      0     0     0     0     0     0  \n8       1      0      0     0     0     0     0     0  \n9       1      0      0     0     0     0     0     0  \n10      0      0      0     0     0     0     0     0  \n11      0      0      0     0     0     0     0     0  \n12      0      0      0     0     0     0     0     0  \n13      0      0      0     0     0     0     0     0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_list</th>\n      <th>n_features</th>\n      <th>norm_1</th>\n      <th>norm_2</th>\n      <th>norm_3</th>\n      <th>norm_4</th>\n      <th>norm_5</th>\n      <th>unif_1</th>\n      <th>unif_2</th>\n      <th>unif_3</th>\n      <th>unif_4</th>\n      <th>unif_5</th>\n      <th>ber_1</th>\n      <th>ber_2</th>\n      <th>ber_3</th>\n      <th>ber_4</th>\n      <th>ber_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fl_boruta_1</td>\n      <td>187</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fl_boruta_2</td>\n      <td>179</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fl_boruta_3</td>\n      <td>178</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fl_boruta_4</td>\n      <td>166</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fl_boruta_5</td>\n      <td>162</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fl_boruta_6</td>\n      <td>159</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>fl_boruta_7</td>\n      <td>156</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>fl_boruta_8</td>\n      <td>155</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fl_boruta_9</td>\n      <td>149</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fl_boruta_10</td>\n      <td>147</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>fl_boruta_20</td>\n      <td>125</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>fl_boruta_30</td>\n      <td>112</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>fl_boruta_40</td>\n      <td>104</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>fl_boruta_50</td>\n      <td>93</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Observations \n#### - Among the 15 random noise features, most of them only obtain very few hits (i.e., being more predictive than all the shadow features). \n#### - They can be easily removed, even when the threshold for the number of hits is set very low. \n#### - Three random features obtain 10 or more hits, with one being normal noise and two being uniform noise.","metadata":{}}]}