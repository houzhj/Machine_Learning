{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data Manipulating and Visualization\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats.mstats import winsorize\nimport random\n\n\n# Operating System\nimport os\nfrom datetime import datetime\n\n# Machine Learning Algorithms\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Performance metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import accuracy_score\n\n# Hyperparameter\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer, Categorical\n\n## Mathematics and Statistics\nimport scipy.stats as stats\nfrom scipy.stats import uniform\nfrom scipy.stats import loguniform\n\n# NLP related \nimport string\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem.porter import *\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-04T17:11:49.405280Z","iopub.execute_input":"2023-10-04T17:11:49.405664Z","iopub.status.idle":"2023-10-04T17:11:51.775823Z","shell.execute_reply.started":"2023-10-04T17:11:49.405633Z","shell.execute_reply":"2023-10-04T17:11:51.773960Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load modeling data\n### [The creation of the the modeling data is discussed in this notebook](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/imdb_data.ipynb)\n","metadata":{}},{"cell_type":"code","source":"Modeling_Date = pd.read_csv('/kaggle/input/imdb-date/Modeling_Date.csv',low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T17:11:51.779002Z","iopub.execute_input":"2023-10-04T17:11:51.780218Z","iopub.status.idle":"2023-10-04T17:12:25.221831Z","shell.execute_reply.started":"2023-10-04T17:11:51.780165Z","shell.execute_reply":"2023-10-04T17:12:25.220214Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"X_Data = Modeling_Date.drop(['movie_review','sentiment_label','sentiment_number'], axis=1)\nY_Data = Modeling_Date.sentiment_number","metadata":{"execution":{"iopub.status.busy":"2023-10-04T17:12:25.223712Z","iopub.execute_input":"2023-10-04T17:12:25.224211Z","iopub.status.idle":"2023-10-04T17:12:25.531057Z","shell.execute_reply.started":"2023-10-04T17:12:25.224166Z","shell.execute_reply":"2023-10-04T17:12:25.529858Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 2. Feature Selection\n### This study conducted and compared two feature selection approaches: the (1)\"Iterative Reduction (IR)\" approach, and the (2) Boruta approach. \n* #### Algorithms: \nThere are many machine learning algorithms to choose from, and here I have chosen the Light Gradient Boosting Model. The function [lightgbm.LGBMClassifier()](https://lightgbm.readthedocs.io/en/stable/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier) is used. The early stopping method is applied (the original data is randomly split into a new training set and a validation set). \n\n* #### Feature Importance Type: \nThese two methods are closely related to feature importance, so different feature importance metrics may yield different results. Several common types of importance types are considered. In this notebook, we only consider the [gain](https://eli5.readthedocs.io/en/latest/libraries/lightgbm.html) importance.\n\n* #### Hyperparameter: \nDuring the feature selection process, various models with different features will be estimated and compared. In this process, I will not tune hyperparameters but will use pre-specified hyperparameters. Hyperparameter tuning will be performed after the final feature list is determined.\n\n* #### Performance Metrics: \nLogloss is considered. \n\n## **(1) Iterative Reduction (IR)**\n### In this approach, the models are evolving as ‘less important’ features are removed. And the feature list (hence the feature important list) is updated whenever a new model is trained. The steps of the IR approach are described as the following:\n* #### Assuming we start from 2000 features, a model with 2000 features is built, call it Model_2000, and the feature important list of this model is List_2000 is calculated.\n* #### The Model_1900 is built using the top 1900 features in List_2000, and a new feature list List_1900 is calculated. \n* #### The Model_1800 is built using the top 1800 features in List_1900, and a new feature list List_1800 is calculated. \n* #### So on and so forth. In the end we obtain several models with different feature list, containing decreasing number of features. In general the final feature list is selected from the model with the best performance among these models. \n\n","metadata":{}},{"cell_type":"markdown","source":"## **(2) Boruta**\n### [This article](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a) provides a description of the general idea of Boruta\n### The steps of the Boruta approach are described as the following:\n- #### Step 1: Assuming we start from 2000 features. Create a shadow feature for each feature (the shadow feature for X1 is a random shuffle of the initial X1). \n- #### Step 2: Build a “Boruta” model using all the 2000 original features and the 2000 shadow features created in Step 1. Note that in this step, the Boruta model can use different algorithms. We considered light gradient boosting, extreme gradient boosting, and random forest. \n- #### Step 3: Calculate the feature importance of each feature in the “Boruta” model built in Step 2. We considered several types of feature importance. Then calculate the number of “Hit” for each original features. A feature gets a “Hit” if its importance is higher than the most importance shadow feature. \n- #### Step 4: Repeat Step 1-3 many (such as 100) times. \n- #### Step 5: Finally, select a threshold of the number of Hits “h”, and keep all the features with number of “Hits”>h. \n","metadata":{}},{"cell_type":"markdown","source":"## **Settings**\n### Hyperparameters for LGB\n","metadata":{}},{"cell_type":"code","source":"params_LGB= {'boosting_type'    : 'gbdt',\n             'objective'        : 'binary',\n             'colsample_bytree' : 0.2,\n             'learning_rate'    : 0.05,\n             'min_child_samples': 10,\n             'min_child_weight' : 5,\n             'max_depth'        : -1,\n             'min_split_gain'   : 0,\n             'num_leaves'       : 31,\n             'subsample_for_bin': 50000,\n             'subsample_freq'   : 1,\n             'n_estimators'     : 5000\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-04T17:12:25.533480Z","iopub.execute_input":"2023-10-04T17:12:25.534202Z","iopub.status.idle":"2023-10-04T17:12:25.550044Z","shell.execute_reply.started":"2023-10-04T17:12:25.534158Z","shell.execute_reply":"2023-10-04T17:12:25.548595Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Iterative Reduction Approch\n","metadata":{}},{"cell_type":"markdown","source":"### First, run a LGB model using all the features, and look at the feature importance of this model.","metadata":{}},{"cell_type":"code","source":"x1,x2,y1,y2 = train_test_split(X_Data,Y_Data,test_size = 0.3,random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T17:12:25.552288Z","iopub.execute_input":"2023-10-04T17:12:25.552884Z","iopub.status.idle":"2023-10-04T17:12:26.775812Z","shell.execute_reply.started":"2023-10-04T17:12:25.552843Z","shell.execute_reply":"2023-10-04T17:12:26.774425Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"LGB_2000 = lgb.LGBMClassifier(**params_LGB,importance_type='gain')\nLGB_2000.fit(X = x1, y = y1,\n             eval_metric=['logloss'], eval_set=[(x1,y1),(x2,y2)],\n             callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\nimportance_df = pd.DataFrame(list(x1)).rename(columns={0:'Features'})\nimportance_df['importance'] = LGB_2000.feature_importances_\nimportance_df = importance_df.sort_values(by=['importance'],ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T17:12:26.777521Z","iopub.execute_input":"2023-10-04T17:12:26.777828Z","iopub.status.idle":"2023-10-04T17:13:54.773894Z","shell.execute_reply.started":"2023-10-04T17:12:26.777802Z","shell.execute_reply":"2023-10-04T17:13:54.772465Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1055]\ttraining's binary_logloss: 0.122556\tvalid_1's binary_logloss: 0.295978\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### In fact, there are quite a few features with a gain/split importance of 0. It typically means that the feature did not contribute to the model's predictive performance during the training process. \n- #### A feature with zero \"gain\"/\"split\" importance means it was not selected as the splitting feature at any node during the construction of the ensemble of decision trees. \n- #### If a feature's split importance is 0, its gain importance is also 0. \n\n### As shown below, there are 261 features with positive gain/split importance scores. \n- #### So, in the first iteration, we reduce the number from 2000 to 261. \n- #### Then continued to gradually decrease the number of features.","metadata":{}},{"cell_type":"code","source":"importance_df[importance_df['importance']>0]","metadata":{"execution":{"iopub.status.busy":"2023-10-04T17:13:54.775621Z","iopub.execute_input":"2023-10-04T17:13:54.776018Z","iopub.status.idle":"2023-10-04T17:13:54.797395Z","shell.execute_reply.started":"2023-10-04T17:13:54.775978Z","shell.execute_reply":"2023-10-04T17:13:54.796075Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       Features    importance\n160         bad  18180.554622\n1978      worst  12578.323444\n1923       wast  12302.281613\n797       great   9692.643652\n151          aw   9274.292484\n...         ...           ...\n1553  sentiment      1.681140\n438       cross      1.514130\n1074      lynch      1.461890\n1158       moor      1.215910\n1711    subplot      1.174220\n\n[1692 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Features</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>160</th>\n      <td>bad</td>\n      <td>18180.554622</td>\n    </tr>\n    <tr>\n      <th>1978</th>\n      <td>worst</td>\n      <td>12578.323444</td>\n    </tr>\n    <tr>\n      <th>1923</th>\n      <td>wast</td>\n      <td>12302.281613</td>\n    </tr>\n    <tr>\n      <th>797</th>\n      <td>great</td>\n      <td>9692.643652</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>aw</td>\n      <td>9274.292484</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1553</th>\n      <td>sentiment</td>\n      <td>1.681140</td>\n    </tr>\n    <tr>\n      <th>438</th>\n      <td>cross</td>\n      <td>1.514130</td>\n    </tr>\n    <tr>\n      <th>1074</th>\n      <td>lynch</td>\n      <td>1.461890</td>\n    </tr>\n    <tr>\n      <th>1158</th>\n      <td>moor</td>\n      <td>1.215910</td>\n    </tr>\n    <tr>\n      <th>1711</th>\n      <td>subplot</td>\n      <td>1.174220</td>\n    </tr>\n  </tbody>\n</table>\n<p>1692 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### The following codes perform the IR approach","metadata":{}},{"cell_type":"code","source":"def Run_IR(params_LGB,importance_type,x1,x2,y1,y2,n_feature_list):\n    def get_importance_from_LGBM(model,x_data,importance_type):\n        ### Get the feature importance list, merge with the columns names, and sorted by the importance score. \n        importance_df = pd.DataFrame(list(x_data)).rename(columns={0:'Features'})\n        importance_df[importance_type] = model.feature_importances_\n        importance_df = importance_df.sort_values(by=[importance_type],ascending=False)\n        return importance_df\n\n    def iterative_reduction(start_n,end_n,importance_dict):\n        print('Feature reduction from %d to %d ...' %(start_n,end_n))\n        importance_old = importance_dict['importance_'+str(start_n)]\n        Selected_Features = importance_old.head(end_n)['Features'].values.tolist()\n        Shuffled_Features = Selected_Features.copy()\n        random.shuffle(Shuffled_Features)\n        x1_new = x1[Shuffled_Features]\n        x2_new = x2[Shuffled_Features]\n        \n        LGB_now = lgb.LGBMClassifier(**params_LGB,importance_type=importance_type)\n        LGB_now.fit(X = x1_new, y = y1,\n                    eval_metric=['logloss'], eval_set=[(x1_new,y1),(x2_new,y2)],\n                    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n        importance_now = get_importance_from_LGBM(LGB_now,x1_new,importance_type)\n        \n        updated_importance_dict = importance_dict.copy()\n        updated_importance_dict['importance_'+str(end_n)] = importance_now\n        return(updated_importance_dict)\n\n    ##### Create a dict to store the importance list\n    importance_dict   = {}\n    \n    ##### First iteration (including all the features considered)\n    LGB_2000 = lgb.LGBMClassifier(**params_LGB,importance_type=importance_type)\n    LGB_2000.fit(X = x1, y = y1,\n                 eval_metric=['auc','logloss'], eval_set=[(x1,y1),(x2,y2)],\n                 callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n    importance_2000 = get_importance_from_LGBM(LGB_2000,x1,importance_type)\n    importance_dict['importance_2000'] = importance_2000\n    \n    ##### Run Iteration Reduction\n    for a in range(len(n_feature_list)-1):\n        importance_dict = iterative_reduction(n_feature_list[a], n_feature_list[a+1],importance_dict)\n    \n    ##### Create a dict to store the feature list\n    feature_list_dict = {}\n    for i in range(len(n_feature_list)):\n        temp = importance_dict['importance_'+str(n_feature_list[i])]\n        feature_list_dict['fl_ir_'+str(n_feature_list[i])] = temp['Features']\n    \n    return feature_list_dict","metadata":{"execution":{"iopub.status.busy":"2023-10-04T17:13:54.799102Z","iopub.execute_input":"2023-10-04T17:13:54.799508Z","iopub.status.idle":"2023-10-04T17:13:54.814089Z","shell.execute_reply.started":"2023-10-04T17:13:54.799476Z","shell.execute_reply":"2023-10-04T17:13:54.812449Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### IR with **Gain** importance","metadata":{}},{"cell_type":"code","source":"n_feature_list   = [2000,261,240,220,200,180,160,140,120,100,80,60]\nIR_fl = Run_IR(params_LGB,'gain',x1,x2,y1,y2,n_feature_list)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T17:13:54.815744Z","iopub.execute_input":"2023-10-04T17:13:54.816114Z","iopub.status.idle":"2023-10-04T17:17:20.135523Z","shell.execute_reply.started":"2023-10-04T17:13:54.816084Z","shell.execute_reply":"2023-10-04T17:17:20.133529Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[723]\ttraining's auc: 0.992658\ttraining's binary_logloss: 0.160656\tvalid_1's auc: 0.94615\tvalid_1's binary_logloss: 0.297905\nFeature reduction from 2000 to 261 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[541]\ttraining's binary_logloss: 0.218562\tvalid_1's binary_logloss: 0.323533\nFeature reduction from 261 to 240 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[457]\ttraining's binary_logloss: 0.234313\tvalid_1's binary_logloss: 0.328048\nFeature reduction from 240 to 220 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[498]\ttraining's binary_logloss: 0.23121\tvalid_1's binary_logloss: 0.32974\nFeature reduction from 220 to 200 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[471]\ttraining's binary_logloss: 0.239987\tvalid_1's binary_logloss: 0.33384\nFeature reduction from 200 to 180 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[493]\ttraining's binary_logloss: 0.241907\tvalid_1's binary_logloss: 0.336837\nFeature reduction from 180 to 160 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[567]\ttraining's binary_logloss: 0.239314\tvalid_1's binary_logloss: 0.341092\nFeature reduction from 160 to 140 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[481]\ttraining's binary_logloss: 0.260732\tvalid_1's binary_logloss: 0.348392\nFeature reduction from 140 to 120 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[468]\ttraining's binary_logloss: 0.272065\tvalid_1's binary_logloss: 0.35556\nFeature reduction from 120 to 100 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[513]\ttraining's binary_logloss: 0.279635\tvalid_1's binary_logloss: 0.365242\nFeature reduction from 100 to 80 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[518]\ttraining's binary_logloss: 0.293287\tvalid_1's binary_logloss: 0.379202\nFeature reduction from 80 to 60 ...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[469]\ttraining's binary_logloss: 0.324738\tvalid_1's binary_logloss: 0.399445\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2.2 Boruta\n### The following codes perform the Boruta approach","metadata":{}},{"cell_type":"code","source":"def Run_Boruta(params_LGB,importance_type,X_Data,Y_Data,TH_values,N_iteration):\n    hits            = np.zeros(len(X_Data.columns))\n    early_stopping_rounds = 50\n    for iter_ in range(N_iteration):\n        np.random.seed(iter_)\n        X_shadow = X_Data.apply(np.random.permutation)\n        X_shadow.columns = ['shadow'+ feature for feature in X_Data.columns]\n        X_boruta = pd.concat([X_Data,X_shadow],axis=1,ignore_index=True)\n        x_train,x_valid,y_train,y_valid = train_test_split(X_boruta,\n                                                            Y_Data,\n                                                            test_size = 0.3, \n                                                            random_state = 100)\n        LGB_model = lgb.LGBMClassifier(**params_LGB,importance_type=importance_type)\n        LGB_model.fit(X = x_train, y = y_train,\n                     eval_metric=['auc','logloss'], eval_set=[(x_train,y_train),(x_valid,y_valid)],\n                     callbacks=[lgb.early_stopping(early_stopping_rounds), lgb.log_evaluation(0)])\n        feature_imp_X      = LGB_model.feature_importances_[:len(X_Data.columns)]\n        feature_imp_shadow = LGB_model.feature_importances_[len(X_Data.columns):]\n        hits+=(feature_imp_X>feature_imp_shadow.max())\n    \n    Hits_df = pd.DataFrame(columns=['Feature','Hits'])\n    Hits_df.iloc[:,0] = X_Data.columns\n    Hits_df.iloc[:,1] = hits\n    \n    feature_list_dict = {}\n    for i in range(len(TH_values)):\n        feature_keep = Hits_df[Hits_df['Hits']>=TH_values[i]]['Feature']\n        feature_list_dict['fl_boruta_'+str(TH_values[i])] = feature_keep\n    return feature_list_dict","metadata":{"execution":{"iopub.status.busy":"2023-10-04T17:17:20.139211Z","iopub.execute_input":"2023-10-04T17:17:20.139577Z","iopub.status.idle":"2023-10-04T17:17:20.151371Z","shell.execute_reply.started":"2023-10-04T17:17:20.139550Z","shell.execute_reply":"2023-10-04T17:17:20.150062Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"TH_values=[1,2,3,4,5,6,7,8,9,10,20,30,40,50]\nBoruta_fl = Run_Boruta(params_LGB,'gain',X_Data,Y_Data,TH_values,50)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T17:17:20.153360Z","iopub.execute_input":"2023-10-04T17:17:20.153694Z","iopub.status.idle":"2023-10-04T19:01:49.862502Z","shell.execute_reply.started":"2023-10-04T17:17:20.153660Z","shell.execute_reply":"2023-10-04T19:01:49.860448Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[805]\ttraining's auc: 0.996539\ttraining's binary_logloss: 0.141374\tvalid_1's auc: 0.944223\tvalid_1's binary_logloss: 0.302562\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[751]\ttraining's auc: 0.995663\ttraining's binary_logloss: 0.14913\tvalid_1's auc: 0.944019\tvalid_1's binary_logloss: 0.303285\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[638]\ttraining's auc: 0.992712\ttraining's binary_logloss: 0.168194\tvalid_1's auc: 0.944334\tvalid_1's binary_logloss: 0.303607\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[636]\ttraining's auc: 0.992875\ttraining's binary_logloss: 0.167836\tvalid_1's auc: 0.944523\tvalid_1's binary_logloss: 0.303137\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[629]\ttraining's auc: 0.992645\ttraining's binary_logloss: 0.168959\tvalid_1's auc: 0.94422\tvalid_1's binary_logloss: 0.304057\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[717]\ttraining's auc: 0.994973\ttraining's binary_logloss: 0.154398\tvalid_1's auc: 0.944358\tvalid_1's binary_logloss: 0.302743\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[629]\ttraining's auc: 0.992613\ttraining's binary_logloss: 0.169031\tvalid_1's auc: 0.943966\tvalid_1's binary_logloss: 0.304527\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[679]\ttraining's auc: 0.994058\ttraining's binary_logloss: 0.160306\tvalid_1's auc: 0.944194\tvalid_1's binary_logloss: 0.303264\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[757]\ttraining's auc: 0.995602\ttraining's binary_logloss: 0.148658\tvalid_1's auc: 0.945093\tvalid_1's binary_logloss: 0.300682\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[730]\ttraining's auc: 0.994895\ttraining's binary_logloss: 0.153449\tvalid_1's auc: 0.944442\tvalid_1's binary_logloss: 0.302471\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[695]\ttraining's auc: 0.9943\ttraining's binary_logloss: 0.158144\tvalid_1's auc: 0.944238\tvalid_1's binary_logloss: 0.303396\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[637]\ttraining's auc: 0.992805\ttraining's binary_logloss: 0.167799\tvalid_1's auc: 0.944127\tvalid_1's binary_logloss: 0.304186\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[713]\ttraining's auc: 0.99507\ttraining's binary_logloss: 0.154027\tvalid_1's auc: 0.94443\tvalid_1's binary_logloss: 0.302489\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[664]\ttraining's auc: 0.99365\ttraining's binary_logloss: 0.163083\tvalid_1's auc: 0.944319\tvalid_1's binary_logloss: 0.30333\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[661]\ttraining's auc: 0.993575\ttraining's binary_logloss: 0.163117\tvalid_1's auc: 0.944489\tvalid_1's binary_logloss: 0.302918\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[786]\ttraining's auc: 0.996267\ttraining's binary_logloss: 0.143539\tvalid_1's auc: 0.944569\tvalid_1's binary_logloss: 0.301717\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[697]\ttraining's auc: 0.994417\ttraining's binary_logloss: 0.157697\tvalid_1's auc: 0.944102\tvalid_1's binary_logloss: 0.303531\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[750]\ttraining's auc: 0.995672\ttraining's binary_logloss: 0.148873\tvalid_1's auc: 0.944531\tvalid_1's binary_logloss: 0.302027\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[758]\ttraining's auc: 0.995847\ttraining's binary_logloss: 0.147746\tvalid_1's auc: 0.944468\tvalid_1's binary_logloss: 0.302129\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[742]\ttraining's auc: 0.995447\ttraining's binary_logloss: 0.151004\tvalid_1's auc: 0.944853\tvalid_1's binary_logloss: 0.301479\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[711]\ttraining's auc: 0.994816\ttraining's binary_logloss: 0.15531\tvalid_1's auc: 0.944347\tvalid_1's binary_logloss: 0.302852\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[754]\ttraining's auc: 0.995756\ttraining's binary_logloss: 0.148537\tvalid_1's auc: 0.94396\tvalid_1's binary_logloss: 0.303399\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[793]\ttraining's auc: 0.99644\ttraining's binary_logloss: 0.142694\tvalid_1's auc: 0.94449\tvalid_1's binary_logloss: 0.301841\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[612]\ttraining's auc: 0.992028\ttraining's binary_logloss: 0.172261\tvalid_1's auc: 0.944287\tvalid_1's binary_logloss: 0.30398\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[662]\ttraining's auc: 0.993566\ttraining's binary_logloss: 0.163292\tvalid_1's auc: 0.944207\tvalid_1's binary_logloss: 0.303565\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[638]\ttraining's auc: 0.993019\ttraining's binary_logloss: 0.16699\tvalid_1's auc: 0.944651\tvalid_1's binary_logloss: 0.302982\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[609]\ttraining's auc: 0.992011\ttraining's binary_logloss: 0.172618\tvalid_1's auc: 0.944084\tvalid_1's binary_logloss: 0.304467\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[613]\ttraining's auc: 0.992\ttraining's binary_logloss: 0.172297\tvalid_1's auc: 0.944497\tvalid_1's binary_logloss: 0.303349\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[725]\ttraining's auc: 0.995147\ttraining's binary_logloss: 0.153249\tvalid_1's auc: 0.944655\tvalid_1's binary_logloss: 0.302035\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[781]\ttraining's auc: 0.996363\ttraining's binary_logloss: 0.144123\tvalid_1's auc: 0.944131\tvalid_1's binary_logloss: 0.30291\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[686]\ttraining's auc: 0.994089\ttraining's binary_logloss: 0.159391\tvalid_1's auc: 0.944358\tvalid_1's binary_logloss: 0.30298\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[640]\ttraining's auc: 0.993052\ttraining's binary_logloss: 0.166687\tvalid_1's auc: 0.944634\tvalid_1's binary_logloss: 0.302862\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[593]\ttraining's auc: 0.9913\ttraining's binary_logloss: 0.176034\tvalid_1's auc: 0.943937\tvalid_1's binary_logloss: 0.30507\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[634]\ttraining's auc: 0.992746\ttraining's binary_logloss: 0.16804\tvalid_1's auc: 0.944444\tvalid_1's binary_logloss: 0.30333\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[825]\ttraining's auc: 0.996828\ttraining's binary_logloss: 0.13903\tvalid_1's auc: 0.944493\tvalid_1's binary_logloss: 0.301766\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[637]\ttraining's auc: 0.992704\ttraining's binary_logloss: 0.168164\tvalid_1's auc: 0.94469\tvalid_1's binary_logloss: 0.302681\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[761]\ttraining's auc: 0.995828\ttraining's binary_logloss: 0.147544\tvalid_1's auc: 0.944418\tvalid_1's binary_logloss: 0.302335\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[806]\ttraining's auc: 0.996641\ttraining's binary_logloss: 0.140934\tvalid_1's auc: 0.944236\tvalid_1's binary_logloss: 0.302354\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[617]\ttraining's auc: 0.992099\ttraining's binary_logloss: 0.171632\tvalid_1's auc: 0.944328\tvalid_1's binary_logloss: 0.303858\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[706]\ttraining's auc: 0.994642\ttraining's binary_logloss: 0.156255\tvalid_1's auc: 0.944732\tvalid_1's binary_logloss: 0.301809\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[815]\ttraining's auc: 0.996657\ttraining's binary_logloss: 0.140265\tvalid_1's auc: 0.944424\tvalid_1's binary_logloss: 0.302013\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[721]\ttraining's auc: 0.99497\ttraining's binary_logloss: 0.15379\tvalid_1's auc: 0.944411\tvalid_1's binary_logloss: 0.302524\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[670]\ttraining's auc: 0.993823\ttraining's binary_logloss: 0.161647\tvalid_1's auc: 0.943825\tvalid_1's binary_logloss: 0.304295\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[752]\ttraining's auc: 0.995642\ttraining's binary_logloss: 0.148775\tvalid_1's auc: 0.944152\tvalid_1's binary_logloss: 0.302919\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[727]\ttraining's auc: 0.995112\ttraining's binary_logloss: 0.152925\tvalid_1's auc: 0.944475\tvalid_1's binary_logloss: 0.302449\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[684]\ttraining's auc: 0.994159\ttraining's binary_logloss: 0.159617\tvalid_1's auc: 0.944439\tvalid_1's binary_logloss: 0.302922\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[803]\ttraining's auc: 0.996648\ttraining's binary_logloss: 0.141324\tvalid_1's auc: 0.944158\tvalid_1's binary_logloss: 0.302597\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[672]\ttraining's auc: 0.993992\ttraining's binary_logloss: 0.161134\tvalid_1's auc: 0.944232\tvalid_1's binary_logloss: 0.303472\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[652]\ttraining's auc: 0.993182\ttraining's binary_logloss: 0.165523\tvalid_1's auc: 0.944174\tvalid_1's binary_logloss: 0.30374\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[668]\ttraining's auc: 0.993702\ttraining's binary_logloss: 0.162287\tvalid_1's auc: 0.943412\tvalid_1's binary_logloss: 0.305438\n","output_type":"stream"}]},{"cell_type":"code","source":"All_fl = IR_fl.copy()\nAll_fl.update(Boruta_fl)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:01:49.866095Z","iopub.execute_input":"2023-10-04T19:01:49.866721Z","iopub.status.idle":"2023-10-04T19:01:49.879773Z","shell.execute_reply.started":"2023-10-04T19:01:49.866666Z","shell.execute_reply":"2023-10-04T19:01:49.877904Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def fl_performance(params_LGB,fl_dict,x1,x2,y1,y2):\n    def get_perf(fl,x1,x2,y1,y2):\n        x1_now = x1[fl]\n        x2_now = x2[fl]\n\n        LGBM = lgb.LGBMClassifier(**params_LGB)\n        LGBM.fit(X = x1_now, y = y1,\n                 eval_metric=['logloss'], eval_set=[(x1_now,y1),(x2_now,y2)],\n                 callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n\n        metric = LGBM.best_score_['valid_1']['binary_logloss']\n        \n        return metric\n\n    fl_names = list(fl_dict.keys())\n    performance_df = pd.DataFrame(columns=['fl_name','n_features','perf_1','perf_2','perf_3'])\n    for i in range(len(fl_names)):\n        fl_now = All_fl[fl_names[i]]\n\n        fl_shuffle_1 = fl_now.sample(frac=1)\n        fl_shuffle_2 = fl_now.sample(frac=1)\n        fl_shuffle_3 = fl_now.sample(frac=1)\n\n        performance_df.loc[i,'fl_name']    = fl_names[i]\n        performance_df.loc[i,'n_features'] = len(fl_now)\n        performance_df.loc[i,'perf_1']     = get_perf(fl_shuffle_1,x1,x2,y1,y2)\n        performance_df.loc[i,'perf_2']     = get_perf(fl_shuffle_2,x1,x2,y1,y2)\n        performance_df.loc[i,'perf_3']     = get_perf(fl_shuffle_3,x1,x2,y1,y2)\n\n    performance_df['perf_mean'] = performance_df[['perf_1', 'perf_2', 'perf_3']].mean(axis=1)\n    performance_df['perf_var']  = performance_df[['perf_1', 'perf_2', 'perf_3']].var(axis=1)\n    \n    return performance_df","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:01:49.882207Z","iopub.execute_input":"2023-10-04T19:01:49.882775Z","iopub.status.idle":"2023-10-04T19:01:49.900487Z","shell.execute_reply.started":"2023-10-04T19:01:49.882731Z","shell.execute_reply":"2023-10-04T19:01:49.898561Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"performance_df = fl_performance(params_LGB,All_fl,x1,x2,y1,y2)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:01:49.902383Z","iopub.execute_input":"2023-10-04T19:01:49.902784Z","iopub.status.idle":"2023-10-04T19:27:56.301245Z","shell.execute_reply.started":"2023-10-04T19:01:49.902752Z","shell.execute_reply":"2023-10-04T19:27:56.300235Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1109]\ttraining's binary_logloss: 0.117643\tvalid_1's binary_logloss: 0.295955\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1115]\ttraining's binary_logloss: 0.117326\tvalid_1's binary_logloss: 0.297443\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1071]\ttraining's binary_logloss: 0.121075\tvalid_1's binary_logloss: 0.297315\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[529]\ttraining's binary_logloss: 0.220486\tvalid_1's binary_logloss: 0.32264\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[536]\ttraining's binary_logloss: 0.218687\tvalid_1's binary_logloss: 0.322879\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[487]\ttraining's binary_logloss: 0.226631\tvalid_1's binary_logloss: 0.323033\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[489]\ttraining's binary_logloss: 0.228696\tvalid_1's binary_logloss: 0.327913\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[511]\ttraining's binary_logloss: 0.225999\tvalid_1's binary_logloss: 0.327544\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[487]\ttraining's binary_logloss: 0.229161\tvalid_1's binary_logloss: 0.326213\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[503]\ttraining's binary_logloss: 0.230864\tvalid_1's binary_logloss: 0.331404\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[480]\ttraining's binary_logloss: 0.234265\tvalid_1's binary_logloss: 0.330509\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[494]\ttraining's binary_logloss: 0.231929\tvalid_1's binary_logloss: 0.330611\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[466]\ttraining's binary_logloss: 0.240898\tvalid_1's binary_logloss: 0.333126\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[450]\ttraining's binary_logloss: 0.243142\tvalid_1's binary_logloss: 0.332924\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[482]\ttraining's binary_logloss: 0.238639\tvalid_1's binary_logloss: 0.33386\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[484]\ttraining's binary_logloss: 0.24347\tvalid_1's binary_logloss: 0.336977\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[510]\ttraining's binary_logloss: 0.240181\tvalid_1's binary_logloss: 0.336341\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[442]\ttraining's binary_logloss: 0.250331\tvalid_1's binary_logloss: 0.337044\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[488]\ttraining's binary_logloss: 0.251999\tvalid_1's binary_logloss: 0.340971\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[520]\ttraining's binary_logloss: 0.247027\tvalid_1's binary_logloss: 0.341378\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[527]\ttraining's binary_logloss: 0.245175\tvalid_1's binary_logloss: 0.340417\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[520]\ttraining's binary_logloss: 0.255321\tvalid_1's binary_logloss: 0.348836\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[519]\ttraining's binary_logloss: 0.255295\tvalid_1's binary_logloss: 0.347421\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[500]\ttraining's binary_logloss: 0.258123\tvalid_1's binary_logloss: 0.348113\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[458]\ttraining's binary_logloss: 0.273171\tvalid_1's binary_logloss: 0.355276\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[438]\ttraining's binary_logloss: 0.277052\tvalid_1's binary_logloss: 0.354121\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[473]\ttraining's binary_logloss: 0.271503\tvalid_1's binary_logloss: 0.354788\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[453]\ttraining's binary_logloss: 0.286929\tvalid_1's binary_logloss: 0.365318\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[444]\ttraining's binary_logloss: 0.286135\tvalid_1's binary_logloss: 0.363604\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[460]\ttraining's binary_logloss: 0.285119\tvalid_1's binary_logloss: 0.364586\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[470]\ttraining's binary_logloss: 0.299597\tvalid_1's binary_logloss: 0.378191\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[449]\ttraining's binary_logloss: 0.301407\tvalid_1's binary_logloss: 0.379188\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[492]\ttraining's binary_logloss: 0.296651\tvalid_1's binary_logloss: 0.378343\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[410]\ttraining's binary_logloss: 0.329807\tvalid_1's binary_logloss: 0.399251\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[448]\ttraining's binary_logloss: 0.325632\tvalid_1's binary_logloss: 0.399517\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[429]\ttraining's binary_logloss: 0.32852\tvalid_1's binary_logloss: 0.400102\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[733]\ttraining's binary_logloss: 0.180997\tvalid_1's binary_logloss: 0.305355\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[784]\ttraining's binary_logloss: 0.175254\tvalid_1's binary_logloss: 0.305718\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[777]\ttraining's binary_logloss: 0.176187\tvalid_1's binary_logloss: 0.304979\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[784]\ttraining's binary_logloss: 0.17672\tvalid_1's binary_logloss: 0.30623\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[708]\ttraining's binary_logloss: 0.185919\tvalid_1's binary_logloss: 0.306793\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[706]\ttraining's binary_logloss: 0.18583\tvalid_1's binary_logloss: 0.307147\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[734]\ttraining's binary_logloss: 0.183848\tvalid_1's binary_logloss: 0.30832\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[733]\ttraining's binary_logloss: 0.184708\tvalid_1's binary_logloss: 0.30786\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[764]\ttraining's binary_logloss: 0.180469\tvalid_1's binary_logloss: 0.307446\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[643]\ttraining's binary_logloss: 0.196314\tvalid_1's binary_logloss: 0.309371\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[762]\ttraining's binary_logloss: 0.182515\tvalid_1's binary_logloss: 0.308631\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[710]\ttraining's binary_logloss: 0.188393\tvalid_1's binary_logloss: 0.307981\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[683]\ttraining's binary_logloss: 0.19274\tvalid_1's binary_logloss: 0.30787\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[687]\ttraining's binary_logloss: 0.192692\tvalid_1's binary_logloss: 0.308494\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[671]\ttraining's binary_logloss: 0.194075\tvalid_1's binary_logloss: 0.309168\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[675]\ttraining's binary_logloss: 0.194358\tvalid_1's binary_logloss: 0.309722\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[707]\ttraining's binary_logloss: 0.191138\tvalid_1's binary_logloss: 0.308376\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[698]\ttraining's binary_logloss: 0.191393\tvalid_1's binary_logloss: 0.309675\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[663]\ttraining's binary_logloss: 0.196545\tvalid_1's binary_logloss: 0.309407\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[716]\ttraining's binary_logloss: 0.190438\tvalid_1's binary_logloss: 0.309837\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[689]\ttraining's binary_logloss: 0.193403\tvalid_1's binary_logloss: 0.3105\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[676]\ttraining's binary_logloss: 0.196065\tvalid_1's binary_logloss: 0.30998\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[678]\ttraining's binary_logloss: 0.195736\tvalid_1's binary_logloss: 0.310638\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[680]\ttraining's binary_logloss: 0.19555\tvalid_1's binary_logloss: 0.310681\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[662]\ttraining's binary_logloss: 0.198047\tvalid_1's binary_logloss: 0.31029\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[639]\ttraining's binary_logloss: 0.201155\tvalid_1's binary_logloss: 0.310677\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[654]\ttraining's binary_logloss: 0.199113\tvalid_1's binary_logloss: 0.310536\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[548]\ttraining's binary_logloss: 0.21368\tvalid_1's binary_logloss: 0.310546\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[645]\ttraining's binary_logloss: 0.200635\tvalid_1's binary_logloss: 0.31064\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[670]\ttraining's binary_logloss: 0.197601\tvalid_1's binary_logloss: 0.310381\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[645]\ttraining's binary_logloss: 0.203387\tvalid_1's binary_logloss: 0.311307\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[683]\ttraining's binary_logloss: 0.19833\tvalid_1's binary_logloss: 0.311642\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[816]\ttraining's binary_logloss: 0.18453\tvalid_1's binary_logloss: 0.312385\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[669]\ttraining's binary_logloss: 0.20524\tvalid_1's binary_logloss: 0.315581\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[683]\ttraining's binary_logloss: 0.203651\tvalid_1's binary_logloss: 0.315004\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[610]\ttraining's binary_logloss: 0.211989\tvalid_1's binary_logloss: 0.313955\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[618]\ttraining's binary_logloss: 0.217354\tvalid_1's binary_logloss: 0.317927\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[524]\ttraining's binary_logloss: 0.228856\tvalid_1's binary_logloss: 0.319116\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[641]\ttraining's binary_logloss: 0.214528\tvalid_1's binary_logloss: 0.318679\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[552]\ttraining's binary_logloss: 0.236829\tvalid_1's binary_logloss: 0.327434\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[509]\ttraining's binary_logloss: 0.242556\tvalid_1's binary_logloss: 0.328025\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[472]\ttraining's binary_logloss: 0.247555\tvalid_1's binary_logloss: 0.329087\n","output_type":"stream"}]},{"cell_type":"code","source":"plot_df = performance_df.copy()\nplot_df['Feature Selection Method'] = plot_df['fl_name'].apply(lambda x:'Iterative Reduction' if 'ir' in x else 'Boruta')\nplot_df","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:27:56.302882Z","iopub.execute_input":"2023-10-04T19:27:56.303463Z","iopub.status.idle":"2023-10-04T19:27:56.326175Z","shell.execute_reply.started":"2023-10-04T19:27:56.303430Z","shell.execute_reply":"2023-10-04T19:27:56.324921Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"         fl_name n_features    perf_1    perf_2    perf_3 perf_mean  perf_var  \\\n0     fl_ir_2000       2000  0.295955  0.297443  0.297315  0.296904  0.000001   \n1      fl_ir_261        261   0.32264  0.322879  0.323033  0.322851       0.0   \n2      fl_ir_240        240  0.327913  0.327544  0.326213  0.327223  0.000001   \n3      fl_ir_220        220  0.331404  0.330509  0.330611  0.330841       0.0   \n4      fl_ir_200        200  0.333126  0.332924   0.33386  0.333303       0.0   \n5      fl_ir_180        180  0.336977  0.336341  0.337044  0.336787       0.0   \n6      fl_ir_160        160  0.340971  0.341378  0.340417  0.340922       0.0   \n7      fl_ir_140        140  0.348836  0.347421  0.348113  0.348123  0.000001   \n8      fl_ir_120        120  0.355276  0.354121  0.354788  0.354728       0.0   \n9      fl_ir_100        100  0.365318  0.363604  0.364586  0.364503  0.000001   \n10      fl_ir_80         80  0.378191  0.379188  0.378343  0.378574       0.0   \n11      fl_ir_60         60  0.399251  0.399517  0.400102  0.399624       0.0   \n12   fl_boruta_1        397  0.305355  0.305718  0.304979  0.305351       0.0   \n13   fl_boruta_2        373   0.30623  0.306793  0.307147  0.306723       0.0   \n14   fl_boruta_3        355   0.30832   0.30786  0.307446  0.307876       0.0   \n15   fl_boruta_4        344  0.309371  0.308631  0.307981  0.308661       0.0   \n16   fl_boruta_5        337   0.30787  0.308494  0.309168  0.308511       0.0   \n17   fl_boruta_6        329  0.309722  0.308376  0.309675  0.309258  0.000001   \n18   fl_boruta_7        322  0.309407  0.309837    0.3105  0.309915       0.0   \n19   fl_boruta_8        315   0.30998  0.310638  0.310681  0.310433       0.0   \n20   fl_boruta_9        311   0.31029  0.310677  0.310536  0.310501       0.0   \n21  fl_boruta_10        307  0.310546   0.31064  0.310381  0.310522       0.0   \n22  fl_boruta_20        284  0.311307  0.311642  0.312385  0.311778       0.0   \n23  fl_boruta_30        259  0.315581  0.315004  0.313955  0.314847  0.000001   \n24  fl_boruta_40        236  0.317927  0.319116  0.318679  0.318574       0.0   \n25  fl_boruta_50        185  0.327434  0.328025  0.329087  0.328182  0.000001   \n\n   Feature Selection Method  \n0       Iterative Reduction  \n1       Iterative Reduction  \n2       Iterative Reduction  \n3       Iterative Reduction  \n4       Iterative Reduction  \n5       Iterative Reduction  \n6       Iterative Reduction  \n7       Iterative Reduction  \n8       Iterative Reduction  \n9       Iterative Reduction  \n10      Iterative Reduction  \n11      Iterative Reduction  \n12                   Boruta  \n13                   Boruta  \n14                   Boruta  \n15                   Boruta  \n16                   Boruta  \n17                   Boruta  \n18                   Boruta  \n19                   Boruta  \n20                   Boruta  \n21                   Boruta  \n22                   Boruta  \n23                   Boruta  \n24                   Boruta  \n25                   Boruta  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fl_name</th>\n      <th>n_features</th>\n      <th>perf_1</th>\n      <th>perf_2</th>\n      <th>perf_3</th>\n      <th>perf_mean</th>\n      <th>perf_var</th>\n      <th>Feature Selection Method</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fl_ir_2000</td>\n      <td>2000</td>\n      <td>0.295955</td>\n      <td>0.297443</td>\n      <td>0.297315</td>\n      <td>0.296904</td>\n      <td>0.000001</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fl_ir_261</td>\n      <td>261</td>\n      <td>0.32264</td>\n      <td>0.322879</td>\n      <td>0.323033</td>\n      <td>0.322851</td>\n      <td>0.0</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fl_ir_240</td>\n      <td>240</td>\n      <td>0.327913</td>\n      <td>0.327544</td>\n      <td>0.326213</td>\n      <td>0.327223</td>\n      <td>0.000001</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fl_ir_220</td>\n      <td>220</td>\n      <td>0.331404</td>\n      <td>0.330509</td>\n      <td>0.330611</td>\n      <td>0.330841</td>\n      <td>0.0</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fl_ir_200</td>\n      <td>200</td>\n      <td>0.333126</td>\n      <td>0.332924</td>\n      <td>0.33386</td>\n      <td>0.333303</td>\n      <td>0.0</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fl_ir_180</td>\n      <td>180</td>\n      <td>0.336977</td>\n      <td>0.336341</td>\n      <td>0.337044</td>\n      <td>0.336787</td>\n      <td>0.0</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>fl_ir_160</td>\n      <td>160</td>\n      <td>0.340971</td>\n      <td>0.341378</td>\n      <td>0.340417</td>\n      <td>0.340922</td>\n      <td>0.0</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>fl_ir_140</td>\n      <td>140</td>\n      <td>0.348836</td>\n      <td>0.347421</td>\n      <td>0.348113</td>\n      <td>0.348123</td>\n      <td>0.000001</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fl_ir_120</td>\n      <td>120</td>\n      <td>0.355276</td>\n      <td>0.354121</td>\n      <td>0.354788</td>\n      <td>0.354728</td>\n      <td>0.0</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fl_ir_100</td>\n      <td>100</td>\n      <td>0.365318</td>\n      <td>0.363604</td>\n      <td>0.364586</td>\n      <td>0.364503</td>\n      <td>0.000001</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>fl_ir_80</td>\n      <td>80</td>\n      <td>0.378191</td>\n      <td>0.379188</td>\n      <td>0.378343</td>\n      <td>0.378574</td>\n      <td>0.0</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>fl_ir_60</td>\n      <td>60</td>\n      <td>0.399251</td>\n      <td>0.399517</td>\n      <td>0.400102</td>\n      <td>0.399624</td>\n      <td>0.0</td>\n      <td>Iterative Reduction</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>fl_boruta_1</td>\n      <td>397</td>\n      <td>0.305355</td>\n      <td>0.305718</td>\n      <td>0.304979</td>\n      <td>0.305351</td>\n      <td>0.0</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>fl_boruta_2</td>\n      <td>373</td>\n      <td>0.30623</td>\n      <td>0.306793</td>\n      <td>0.307147</td>\n      <td>0.306723</td>\n      <td>0.0</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>fl_boruta_3</td>\n      <td>355</td>\n      <td>0.30832</td>\n      <td>0.30786</td>\n      <td>0.307446</td>\n      <td>0.307876</td>\n      <td>0.0</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>fl_boruta_4</td>\n      <td>344</td>\n      <td>0.309371</td>\n      <td>0.308631</td>\n      <td>0.307981</td>\n      <td>0.308661</td>\n      <td>0.0</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>fl_boruta_5</td>\n      <td>337</td>\n      <td>0.30787</td>\n      <td>0.308494</td>\n      <td>0.309168</td>\n      <td>0.308511</td>\n      <td>0.0</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>fl_boruta_6</td>\n      <td>329</td>\n      <td>0.309722</td>\n      <td>0.308376</td>\n      <td>0.309675</td>\n      <td>0.309258</td>\n      <td>0.000001</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>fl_boruta_7</td>\n      <td>322</td>\n      <td>0.309407</td>\n      <td>0.309837</td>\n      <td>0.3105</td>\n      <td>0.309915</td>\n      <td>0.0</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>fl_boruta_8</td>\n      <td>315</td>\n      <td>0.30998</td>\n      <td>0.310638</td>\n      <td>0.310681</td>\n      <td>0.310433</td>\n      <td>0.0</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>fl_boruta_9</td>\n      <td>311</td>\n      <td>0.31029</td>\n      <td>0.310677</td>\n      <td>0.310536</td>\n      <td>0.310501</td>\n      <td>0.0</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>fl_boruta_10</td>\n      <td>307</td>\n      <td>0.310546</td>\n      <td>0.31064</td>\n      <td>0.310381</td>\n      <td>0.310522</td>\n      <td>0.0</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>fl_boruta_20</td>\n      <td>284</td>\n      <td>0.311307</td>\n      <td>0.311642</td>\n      <td>0.312385</td>\n      <td>0.311778</td>\n      <td>0.0</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>fl_boruta_30</td>\n      <td>259</td>\n      <td>0.315581</td>\n      <td>0.315004</td>\n      <td>0.313955</td>\n      <td>0.314847</td>\n      <td>0.000001</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>fl_boruta_40</td>\n      <td>236</td>\n      <td>0.317927</td>\n      <td>0.319116</td>\n      <td>0.318679</td>\n      <td>0.318574</td>\n      <td>0.0</td>\n      <td>Boruta</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>fl_boruta_50</td>\n      <td>185</td>\n      <td>0.327434</td>\n      <td>0.328025</td>\n      <td>0.329087</td>\n      <td>0.328182</td>\n      <td>0.000001</td>\n      <td>Boruta</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sns.scatterplot(data=plot_df,x='n_features',y='perf_mean',hue='Feature Selection Method')\nplt.xlim(0, 500)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:43:17.013037Z","iopub.execute_input":"2023-10-04T19:43:17.013611Z","iopub.status.idle":"2023-10-04T19:43:17.353846Z","shell.execute_reply.started":"2023-10-04T19:43:17.013573Z","shell.execute_reply":"2023-10-04T19:43:17.352427Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
